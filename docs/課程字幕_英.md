# 1
## 1-1 Introduction
**Duration: 5 minutes**

Hi, my name is Mehmet Ozkaya and Welcome to my course, Deploying .Net Microservices with K8s,

Azure Kubernetes Services (AKS) and Automating with Azure DevOps.

I am a software architect and currently focusing on microservices and automation deployments.

In this course, we're going **to** learn how to Deploying .Net Microservices into Kubernetes, and moving

deployments to the cloud Azure kubernetes services (AKS) with using Azure Container Registry (ACR). And last section is

We will learn how to Automating Deployments with Azure DevOps and GitHub. You can see here, the steps

of our core structure, We're going to containerize our microservices on docker environment, and push

push these images to the DockerHub and deploy microservices on Kubernetes. As the same setup,

We are going to shifting to the cloud for deploying Azure Kubernetes Services (AKS) with pushing images to

Azure Container Registry (ACR).

Also we will cover additional topics that; Docker compose microservices, K8s components,

Zero-downtime deployments, Using azure resources like ACR, AKS

and Automate whole deployment process with writing custom pipelines with Azure DevOps and Azure Pipelines.

So See the overall picture. You can see that we will have 3 microservices which we are going to develop

and deploy together.

The first one is the Shopping.Client MVC Application

First of all, we are going to develop Shopping MVC Client Application For Consuming Api Resource

which will be the Shopping.Client Asp.Net MVC Web Project.

But we will start with developing this project as a standalone Web application which includes

own data inside it.

And we will add container support with DockerFile, push docker images to Docker hub , and see the deployment options

deployment options like "Azure Web App for Container" resources for 1 web application. Shopping.API Application

After that we are going to develop Shopping.API Microservice with MongoDb and Compose All Docker Containers.

Compose All Docker Containers.

This API project will have Products data and performs CRUD operations with exposing API methods

for consuming from Shopping Client project.

We will containerize API application with creating dockerfile and push images to Azure Container Registry.

Azure Container Registry.

And of course, we have a Mongo DB, our API project will manage product records stored in a no-sql mongodb database

as described in the picture.

we will pull mongodb docker image from docker hub and create connection with our API project.

At the end of the section, we will have 3 microservices whichs are Shopping.Client - Shopping.API -

MongoDb microservices.

As you can see that, we have created docker images, compose docker containers and tested them

in our local computers.

Deploy these docker container images on local kubernetes clusters, and push our image to ACR,

shifting deployment to the cloud Azure kubernetes services (AKS), update microservices with zero-downtime deployments.

zero-downtime deployments. And the last step, we are focusing on automation deployments with creating ci/cd pipelines

pipelines on azure devops tool.

So we will develop seperate microservices deployment pipeline yamls with using Azure Pipelines.

When we push code to Github, microservices pipeline triggers, build docker images and push the ACR

ACR and deploy to Azure Kubernetes services with zero-downtime deployments and automating

the whole process.

In the last section, we will give assignment for deploying multi-container microservices applications

with automating ci/cd pipelines.

We had developed run-aspnetcore-microservices reference application before this course.

You can see the overall picture of the reference microservices application.

microservices application.

Now, I am expecting to you, developing pipelines yamls files for deploy microservices to AKS

and update microservices with zero-downtime deployments.

By the end of this course, you'll learn how to deploy your multi-container microservices applications

with automating all deployment process separately. Before beginning the course,

You should be familiar with C#, ASP.NET Core and Docker.

And Docker. This course will have

will have good theoretical information but also will be 90% of hands-on deployment and development activities.

All microservices and deployment steps will developed step by step and together.

I hope you'll join me on this journey and develop this project with me. 

## 1-2 Prerequisites and Source Code
**Duration: 3 minutes**
In this video we are going to talk about prerequisites. First of all you don't need to know anything

about Deploying Microservices.

We will cover these topics in the courses.

But this course assumes you have some basic knowledge of C#, .Net Web Applications and Docker Container knowledge.

And Microservices knowledge would be plus.

Nevertheless, I will develop all parts from scratch so you don't need to worry about that.

We will see all the definitions of Docker Container Process, Container Registries like DockerHub or Azure Container Registry (ACR),

Deploy to Azure App Service for Containers, K8s,

AKS and Azure pipelines for automation.

You can find full source code on github, I am sharing the link in the resource of this video.

You can fork the repository and if you have any findings of expectations please open an issue on github as well as

writing to me on Q&A of the course. I am quick responder on Q&A section.

Also we are going to create a new repository and step by step develop together from scratch. no worries.

No worries.

Bu the main repo will store in aspnetrun organization in github which I can refactor or add

add other features apart from the course.

Let's have a look at the tooling we'll use.

We will use Visual studio 2019 and Visual Studio Code for IDE developments.

I will use both of them because these tools have great features for functionality.

We will use Visual studio 2019 for developing and dockerize our microservice application.

And We will use Visual Studio Code for writing k8s and aks manifest files and deploying microservices

on k8s clusters. And writing your own pipelines

for ci/cd devops flows. VS Code has great extentions for it.

Docker Desktop and Docker Account for pushing images Docker Hub

We will have used the Docker account, so we will need to Docker Desktop and Docker Account for

you.

And we will also use Git on Local and Github Account for granting our devops pipelines triggering

when push the code.

And we will need to Free Azure Subscription for creating all azure resources like ACR,

Web app for Containers, AKS and so on..

Also, we need to Azure Devops Account for ci/cd devops pipelines. So you don't need to create

now.

I will assist you when required to these tools and subscriptions. All are free and

you can deploy your own microservices with using azure free subscriptions.
## 1-3 What is Docker and Container ?
**Duration: 1 minute**
In this video we are going to talk about What is Docker and What is Container. Docker is

an open platform for developing, shipping, and running applications. Docker provides to separate your applications

from your infrastructure so you can deliver software quickly. Advantages of Docker’s methodologies

for shipping, testing, and deploying code quickly,

you can significantly reduce the delay between writing code and running it in production.

Docker provides for automating the deployment of applications as portable, self-sufficient containers

that can run on the cloud or on-premises.

Docker containers can run anywhere, in your local computer to the cloud.

Docker image containers can run natively on Linux and Windows machines.

And a container is a standard unit of software that packages up code and all its dependencies

so the application runs quickly and reliably from one computing environment to another.

A Docker container image

is a lightweight, standalone, executable package of software that includes everything needed to run an application.
## 1-4 Docker Containers, Images, and Registries
**Duration: 2 minutes**
In this video we are going to talk about Docker containers, images, and registries. When using Docker,

a developer develops an application and packages application with its dependencies into a container image.

image.

An image is a static representation of the application with its configuration and dependencies.

In order to run the application, the application’s image is instantiated to create a container,

which will be running on the Docker host. Containers can be tested in a development local machines.

Developers should store images in a registry, which is a library of images and is needed when deploying to production orchestrators.

the production orchestrators.

Docker images are stores a public registry via Docker Hub; other vendors provide registries

for different collections of images, including Azure Container Registry.

Alternatively, enterprises can have a private registry on-premises for their own Docker images.

As you can see the images shows, how docker components related each other. Developer creates container

in local and push the images the Docker Registry.

Or its possible that developer download existing image from registry and create container

from image in local environment.

If we look at the more specific example of Application Containerization with Docker, First, we should

write dockerfile for our application, Build application with this docker file and creates the docker images.

docker images. And lastly run this images on any machine and creates running docker container from docker image.

We will use all steps with orchestrating whole microservices application with docker and

Kubernetes for the next sections.
# 2
## 2-1
In this section we are going to Building MVC Client Application For Consuming API Resource which will

be the Shopping.Client Project. But this time, we will start with developing simple MVC application

before integrate with API project.

Let's check our big picture and see what we are going to build one by one.

As you can see that, we are in here and start to developing Shopping MVC Web Application project.

Shopping MVC Web Application project.

First, We will develop this project as a standalone Web application which includes own data inside it.

it, and we will add container support.

with Dockerfile, push docker images to Docker hub and see the deployment options like "Azure Web App for Container"

resources for 1 web application.

So in this section, we are going to develop Shopping.Client MVC application which will be standalone

web application and includes own data.

Next sections we will consume the Shopping.API project.

After we’ve learned the basics, we can start with the coding part.
## 2-2
In this video, we are going to create a net core MVC web application for shopping client microservices.

Let's take an action.

Before we start, let me show you our GitHub repository, which we are going to develop and deploy step

by step together.

So as you can see that it is empty repository on GitHub.

You can also start with creating new repository for you on GitHub.

Now we will clone this repository.

After that we will start creating a new project.

Let me click the code section and copy the clone URL.

Open the Visual Studio, click the clone repository pasting in here and the pet is run slash DevOps.

Okay, let me clone this application and see what's happened.

So as you can see, that it is retrieving objects from the GitHub and we will use this repository during

the course.

So yes, our repository is cloned, but there is no any solution or Visual Studio project yet.

For that purpose, let me click the file new project.

We are going to create new project.

And I'm going to select the blank solution for now.

And the solution name will be the shopping.

And of course we should provide our folder structure which is related to our GitHub repository.

You can verify in here our repository name is run slash DevOps.

Okay.

We give the solution name as a shopping and we are going to create new solution and just hit the create

button.

Yes, we have now only one solution.

And if you see the git chains, you can see the Our Solution folder created under the GitHub repository

with creating new shopping folder.

So now I'm able to create new projects under this solution item.

So let me right click add new project.

This time we are going to create ASP.Net Core web application.

And the web application name should be the shopping dot client.

Okay.

Under the shopping folder, we have a shopping cart client project, which is the MVC project.

Let me click the create button.

And you should select the ASP.Net Core MVC model view controller, and we are not going to use Https

now and make sure that you are using the MVC template.

And the Net framework is ASP.Net Core 5.0.

Let's hit the Create button and creating our first microservice, which is the shopping client.

It takes some time.

Okay?

Yes.

After that, let's modify the launch settings in order to set port numbers correctly From our big picture

of our architecture.

Let me right click properties and click the debug section.

And we are going to select this shopping dot client.

And our port number will be the 5001.

Okay, I'm saving these lunch settings and finally we can run our application.

Let me select the running profile.

We would like to choose shopping client and run this profile.

This will be the our first microservice and we will start with the standalone application and after

that we will integrate with the API project.

But let's go step by step.

Yes, as you can see that the project is running and 5.1 and everything seems okay.

In the next video we are going to develop this application.
## 2-3 Adding Model Class into MVC Application
**Duration: 1 minute**
In this video, we are going to add modal class into our MVC application.

Let's take an action.

As you know that we are using the model view controller MVC template.

So that's why we have a models folder.

So I'm going to add new modal class, right click add new item.

And the name is the model class will be the product.

Okay, In the product class, we should provide some of the properties.

So I am just copy and paste these properties.

ID name, category description, image file and the price.

So these modal class will be used during the course with shopping client and the API project.

Let's continue with the controller part.
## 2-4 Developing Shopping.Client Microservices Data Model and Context Objects
**Duration: 2 minutes**
In this video we are going to develop shopping client, microservices, data model and context objects.

So let's take an action.

Before we start, I would like to mention about every microservice should have its own database, so

we should create data store for shopping client microservices for shopping client microservice.

I will use static list objects values in order to store product data.

This is the first step.

Our development.

We will create a standalone MVC client application with data folder and deploy this application onto

Azure.

After that, we will continue to develop with consuming products from the shopping API project and compose

all microservices on Docker compose and moving to Kubernetes environments.

So let me focus one standalone microservice for now.

In order to use product class as an entity object, we should create a context class which should store

our product information.

So I'm going to start with creating data folder right click add new folder, which name is data, and

I'm going to add new class under the data object new item.

And for the context.

Okay, Now for this time, we are using the product data inside of this static class.

That's why I'm making the static class and I'm providing the static method for storing the products.

Let me copy and paste in here.

The products should be come from the.

Moodle namespace.

Okay.

As you can see that we have a product context class and storing the products in this class we define

products, read only list object and setting some products entities when object is initializing.

By this way we can simulating that product objects store in product context object.

As you can see that we can implement the product context object like entity framework core in-memory

implementation in order to store our products related data into its own database.
## 2-5 Listing Products on Index Page of Shopping.Client Microservice
**Duration: 3 minutes**
Let's open the wives home and.

Before that, we should go to the home controller.

Let me go to the home controller.

We are going to listing products into the index page.

So that's why first I navigate the index.

But this index page operations handle from the home controller class.

So that's why we should change the our index method of the home controller.

So into the view object, I am returning the products in here.

Let me paste the parameter product.

Context.

And of course we should import the data folder and using the products hard coded list object in here

we are passing these products into that.

We've.

So we have now products on index page.

We should map these data into HTML list objects.

So for that purpose after that, let me modify the index HTML.

So I'm going to copy and paste because this is the basic operations and.

Basically I'm providing to some HTML table and listing the object with the for each keyword.

Of course I'm passing the product I enumerable list in here because we were passing in the home controller

products list.

So that's why I'm passing these products and can be iterating in the HTML table objects.

Okay, finally we can run our application.

Let me run our application.

And see what happened.

Yes, as you can see that we have finished two products MVC client application.

Next step we will dockerize this application.
## 2-6 Create Docker Container for Shopping.Client Microservice
**Duration: 5 minutes**
In this video, we are going to create Docker container for our shopping client microservices.

For creation Docker container.

We need to create Docker file for our application.

This is very easy to using Visual Studio tooling features.

Let's take an action now.

First, we are going to add Docker support on our Visual Studio solution.

So I'm going to right click the shopping client application and you will see the Add button and you

will see the Add Docker support.

Let me hit this Docker support.

Or selecting the target operating system will be the Linux just hitting the.

Okay.

So this operation creates a new Docker file for us.

As you can see, that Docker file is created and also it will perform this Docker file operation into

the Visual Studio.

Let's see the Docker file.

Let me open Docker file.

We can examine the Docker file.

The Docker file, which we will make the necessary settings for the Docker.

And automatically, as you can see, that generated.

The purpose of creating the Docker file is when we ask Docker to extract the image of our project,

it will search for a file which name is Docker file in the project.

So this will make our application work accordingly to the settings in our file into the docker.

So see the files.

As you can see that it is start with the from and it is has some additional keywords.

Worker expose copy and run commands.

Basically Docker file consists of two main parts.

The first part.

The first part is the building, the application, and the second part is the running the application.

This part is building and this part is the publishing and running the application.

If we look at the commands area in here, the from part where the base image is specified in whichever

library the from project is used.

In this project, we are retrieving ASP.Net five image.

Starting from that image, we are taking this the base image.

If you look at what command are using for us, we are using the.

Net five SDK.

And the work command.

It is a part where we specify the folder under which folder location.

Docker container will copy the files for our project and this will copy files for us the app folder

location.

And of course we have a copy.

Command copy is the command used by copy project files from local file system to Docker image.

In our project we will first copy and restore the C-sharp project file.

After that, copy all these files again and create our application by running the Dotnet publish command.

And also one of the main command is run.

It is used for the commands that we need to run while the containers are being prepared.

First, it is ensured that the build is taken and then it is published.

The operation.

And another point is the entry point.

Entry point is the command that we specify for the first command and parameters that will run when the

container is up.

While the container is running the example.

For example, in our case shopping cart dot HTML.

This will be executed with the net command.

Okay.

We see the docker file and the command explanations.

Now let me check output for logs.

Let me open the output.

As you can see that we have a container tools and in the container tools Docker desktop is installed.

It is trying to verify the desktop, but our docker is not running for now.

If my docker was running, you will see that this Docker file will be building on the on the docker

of your local computer and we will see the outputs of that places.

So we need firstly start with Docker now and after that we will see how to use this Docker file.

But let me start the Docker also.

You should start from the docker on your local computer.
## 2-7 Run Docker Container for Shopping.Client Microservice
**Duration: 6 minutes**
In this video, we are going to run Docker container for our shopping client microservices.

Last video we have created Docker file.

Now we are going to run container on our local and also debug the application from Docker container.

Let's take an action before we start.

Make sure that you have installed and started Docker desktop application for your local computer.

You should download Docker desktop from this page and start Docker on your local computer and you should

see the Docker icon on your computer is running about that.

Okay, Let me return to our quotes.

As you can see that we have created Docker file in the last video.

So now once you edit the Docker file, you will see the run profile change as a docker.

Docker.

As you can see that there is a new running profile which name is the came in here.

So that's automatically arranged by the Visual Studio.

If there is a Docker file which is adding the Docker run profile, if you click the run Docker, this

will create a container from your Docker file image and run this container in your local computer.

Also, it is giving ability to debug your container with the Visual Studio.

It is very good feature from the Visual Studio.

So let's see where Visual Studio store these running profiles.

You can click the properties folder and see the launch settings.

All running profiles turning in here.

And as you can see, that Docker running profile came in here which generating from the Visual Studio.

Once we adding the Docker support and the Docker file generated.

Okay, now we can run directly this Docker profile.

Just click to the profile and it will start with some of the Docker commands.

Basically, we just do abstract these Docker commands from us for now.

We will examine later, but you can think about that.

Basically it is creating a Docker image from this Docker file, pulling this base image from the Docker

up and running the application in our local.

As you can see that I was putting the breakpoint in the index so we can see the breakpoint in here and

we can debug and inspect some of the Visual Studio objects.

And after that, as you can see that this application is running from the Docker container.

And also we can debug the Docker container from the Visual Studio.

It is very great, powerful feature.

You can debug inside of the Docker container and also you can develop and you can start with the develop

and directly inside of the Docker computer.

We are ready about the shipment at any of the code because we can also debugging and also building our

documents.

So let me show you the Visual Studio part.

As you can see that we have a container window in here.

In the container window, we see that the running containers listing in here, you can stop or close

the container image with these two link operations.

Also, you can see the environment variables, ports, logs, tricks directly comes from the ASP.Net

Logs.

As you can see that these are the ASP.Net Startup logs and you can see the inspector files inside of

the Docker container.

So it is very powerful feature when we are using the objects with Visual Studio.

We seen that the Docker image created from the running profile and we have also debug these application

with putting these debugger debugging Docker container inside of the Docker images and also Visual Studio

attaching the container process and able to debug inside of the container.

And we are developing directly inside of Docker container and it is really about the shipment at any

code level changes.

And we saw that there is a very good window which is a container tabs.

You will see the images and the containers in here.

First of all, it is downloading these images from looking the Docker file.

Let me open the Docker file.

As you can see that the first base image is ASP.Net five image.

And you can see the in here we have downloaded image ASP.Net five.

And over that we have creating a new image which is the shopping client with dev, target image and

now shopping client is running on our local computer and you can see the logs and you can debug the

applications.

All is very integrated with the Visual Studio and very good tooling experience with Visual Studio.

But behind all chickens there is lots of Docker commands performing from the Visual Studio.

In the next video we will examine these Docker commands where and how the commands running from the

Visual Studio.
## 2-8 Docker Commands for Shopping.Client Microservice
**Duration: 15 minutes**
In this video, we are going to see Docker commands and run Docker commands for our shopping client

microservices.

Last video we have run debug and stopped Docker container with using Visual Studio tooling, but it

is good to know underlining commands which Visual Studio run behind of us.

Let's take an action.

First of all, let me show you another way to Docker build approach.

You can see the Docker file in here.

It is generated from the Visual Studio, and when I click the Docker file, right click the Docker file

and you can see the Build Docker image button.

Let me click the Build Docker image and follow the logs.

As you can see, that it is running some of the Docker commands.

For us.

And basically it is building an image from this dockerfile right?

As you can find the Docker build command in here and these steps are applying the Docker file commands

one by one.

So this is basically building the Docker file with running Docker build command.

You can find in here and it is getting necessary images.

If not exist, it is retrieving again.

But this time we have already downloaded before and that's why it is a little bit faster from the first

time.

Yes, the build is succeed.

And rebuild, all succeed, as you can see that.

So this is another way to building Docker images.

Also, we already see that run Docker image on Visual Studio and debug it.

Let me show you again.

We put the debugger on controller class and when you click the Docker running profile, it is building

and running image and you can also debug inside of the Docker image.

It is very powerful feature.

You can develop and containerize at the same time and easy to shipping your image.

Write your code and push your image to the your deployment cycle.

As you can see that we see in the our.

Open client container and images and debug is came here.

We can inspect our objects and once we hit the F5 we see the project is running.

And I would like to show you another command from the Visual Studio tooling.

When Visual Studio Docker image is created.

If we clean the project, let me right click and clean the solution.

These.

Let me stop debugging.

Okay.

Uh, this will clean.

Also the commits.

You can see the the remove command.

Right.

So these kind of operations, all of the operations are running from the Visual Studio, but behind

of that it is running the Docker commands.

Right.

So what behind of all these operations, all steps are running one particular Docker command.

So let's see underlying Docker commands and also we can run that commands.

So for that purpose, select the project folder and right click in here.

There is another good base practice and you can see the open in terminal.

I am opening terminal in the my project path.

This is the PowerShell.

Let me see.

It's loading.

Okay, Very good.

So we can start with the basic Docker commands.

Let me show you.

One of the local commands, which is the Docker image.

We can see the building images.

Yes, as you can see that we have a ASP.Net Core image and we have Dotnet SDK image and also we have

two shopping client image, one for development environment, one for latest tech.

Okay.

So actually the latest tech should not be in here.

Let me remove this tech and show you how it is creating.

I'm removing these latest tech image.

Okay, It is deleted.

Let me show you again the limits.

Okay.

Normally when you run the Docker profile, it is basically creating an image with the tag name is the

development environment, right?

So if we would like to shipping these images, these tags should be the latest or the version number.

So in order to perform this operation, make the Visual Studio a run profile to release.

And it is running profile also should be the docker.

And when I run this Docker profile with release mode, it is building the image for me, but this time

it will be created.

Latest Tech.

So that means we are using this release mode when we are shipping our application to the deployment

cycle.

So it is I think we did succeed.

And also our application is running in release mode.

Okay.

Let me close now and open my.

Terminal again.

Okay, so this time, let's check the image.

Now we can see the latest tech came from the release mode, right?

It is another way of the good features when we are using Visual Studio to do operations.

But all these operations handled by the Docker commands, we will see one by one.

So let me continue with the Docker commands.

So now we have two images, one for development environment, one for the latest tech could be shipment

for the deployment, ready for the deployment image.

And now we can also do the manually these building operations.

So let me remove these items and see again we can remove these Docker RMI.

I'm giving the image name.

It is still running on old.

It cannot be forced.

Let me first.

I think it is still running on behind of that.

It is important for us for now.

Let me remove another one.

Okay.

Let's see.

Docker images.

Okay.

I removed one image, but it doesn't import it.

We are creating a new name so you can check the docker running containers with the docker PS.

Yes, as you can see, that the shopping client is still running.

So that's why I think it is not allowing the removing the image.

So let me stop first running Image Docker.

Stop.

I'm giving the container ID.

Okay, it is stopped now.

There is no running container on my local docker.

So let me see the.

Waiting containers.

Also we can remove in here Docker PS all commands.

Okay now we have not running container and also we are not waiting any container ready for running.

So when I check the Docker images we can now able to remove.

Uh, our shopping light.

Right.

Okay.

Yes.

Now my docker image is deleted successfully.

Yes, as you can see that we have not any shopping related image, as we explained before, Docker images.

Uh, let me show you images creates we can create a container from the image.

So.

When we are building an image, we should run this image.

When we run this image, it is basically creating a container.

You can see the this architecture or this relation with from our explanations.

But now we can remove one by one from the container.

And also we are removing the the whole image from our local computer.

So what we can how we can provide this local building operations with Docker commands.

For that purpose, we should be in the same location with the Docker file.

So if we look at the our location, it is the same with our Docker file.

Docker file is inside of the our shopping client.

So I am already in the Docker file pet right?

So in this pet I can write Docker build.

And give the name of the image coping API and I'm putting the dot.

That means in this pet we have a dockerfile.

Please build that docker file for me.

Hit enter and see what's happened.

So as you can see that it is one by one reading our Docker file and performing these commands into the

docker of my local computer.

So it is I think it's a problem for me.

It's most probably with the copy operation.

Yes.

In some cases we should provide the Docker build path, but this time we cannot provide.

But you got the idea when we are running the Docker build command inside of the Docker file path, it

will basically try to building these image from the Docker file.

And also we can run these creating when we create the image, we can run this image with the Docker

run command.

So this time we are not able to build the image with the build command because the path is not correct.

It is arranging by the Visual Studio.

So that's why.

Let me run again.

Let me right click and build Docker image.

Okay, let's copy this line of code and see what is running Docker build.

And there is a okay, as you can see that it is providing the Docker file location, the exact location.

So that's why we can able to boot.

We can not able to boot with this command.

We should provide the Docker file location and as you can see that it is giving the name and performing

the label operations and so on.

I'm not proceed this command because it is already proceed for me from Visual Studio.

I'm now directly check the Docker image and see what's happened.

Let me see the image.

Yes, basically we just do creates image for me.

And let me see the running Docker containers.

Yes, there is no container now.

So in order to run this image with the container, so we should write Docker run command and I'm going

to provide the port number.

Let me this time give me the port number 8080 and I am providing the name parameter for my application.

Let me give my app.

And of course we should provide the image name.

The image name is popping.

Client.

So I am taking the image name from here and basically I'm running the Docker run command and specifying

the port.

Let's hit enter and see what's going on.

Yes, as you can see that the create a container for us and run this container for me.

When I hit the curb, I saw that my shopping client container is running with the 88 port.

So once I.

See the.

He hit the localhost 880.

You can see the our Docker image running in our container.

So basically what I would like to explain that all these operations handled by the Docker commands basically,

but Visual Studio has very great, powerful integration with the Docker.

So you can also use the these two link experience.

But it is good to know underlying Docker commands behind all us with following the output windows and

other stuff.

So let me explain the other basic commands.

You can run the close and you can write the docker and see the all commands.

Yes, the all commands explains very well in here.

So what we are going to mostly in our course we are using the Docker run and we also can be using the

start.

It is basically starting to stop containers and we already using the Docker stop for stopping the containers.

And mostly we are using the pool because we are pulling an image from the Docker hub registry.

And also we are going to push the Docker image to the Docker hub and also other container registry.

So as you can see that we have the Docker commands which we can manage our Docker containers.

So in the next video we are mostly talking about the Docker hub.
## 2-9 Docker Hub Container Registry for Shopping.Client Microservice
**Duration: 4 minutes**
In this video, we are going to see the GitHub container registry and register to Docker for pushing

our image to the crop.

So what is the registry?

It is an environment where the inmates are kept and distributed.

Just like a GitHub.

We can push the images we have to Docker registry or we can pull a Docker image that was previously

loaded into the local area.

In this way we can access an image that will meet your needs very quickly and change it if necessary

and resend it as a different version by giving a new tag name in Docker registry, the desired image

can be pulled locally with the Docker image, pull, image, name command, or you can container process

started from a relevant image by running the docker container run image name to build container.

So which companies provide the ready registry environments?

The first one is the Docker Hub Registry.

There is one limitation for private applications.

Public is unlimited.

And the second one is the Azure Container Registry.

Also, we will use this Azure container registry during the course.

Also there is a container registry and Google container registries.

So let's take an action, first of all.

There is another way to see in the Docker hub.

Let me show you in the application.

So this is the Hub.docker.com.

You can see this like a marketplaces.

First of all, you should log in the Docker hub.

If you have no user, please register and create a subscription for you.

It is free.

You can see the official repositories in here, for example, PostgreSQL, MySQL, Python, ASP.Net

Core and so on.

You can search with the official repositories.

For example, for Mongo official image, it is enough to use the carpool mongo command.

You can see the details in here, description sites in which ports it is exposing, also in which configurations

could be applying.

We will see with the next sections for that mongo image.

So once you have logging the Docker hub, you can search the official image like that and also you can

create your own repository.

So let me go my page and click the repositories.

You can see that I have no any repository yet, so let's create a new repository together.

We can give the name swapping app and you can provide the description.

We will use the public repository for now, but it is allowed only one private repository.

If you use the Azure Container Registry, it is allowed one more private repository to store in their

environment.

So I'm not going to apply any settings and let's just create a repository on the Docker hub.

So as you can see that we have created a shopping app repository and we will push our image into this

repository.

We will use this naming convention.

And as you can see, that the car store our images with the tag numbers into their own hub.

And we will also push our shopping client application into this repository with the next video.
## 2-10 Push Docker Hub Container Registry to Shopping.Client Microservice Docker Image
**Duration: 9 minutes**
In this video, we are going to push the GitHub container registry to shopping client microservice Docker

image.

Let's take an action.

First of all, let me open terminal on Visual Studio.

Let me.

Go back to Visual Studio.

I'm going to right click the project and click to Open in Terminal.

This page should be the same page with the cur file.

It is good to use the same page with the docker file when you performing Docker operations.

I would like to use inline terminal instead of open a new command window.

You can also use the command window for that for that operations because Docker is working in a command

line tool.

But I would like to use the PowerShell from the inside of the Visual Studio.

We will follow the all operations in the one window.

And, uh, it is good to use in on the same page with the Docker file and the development environment.

So after that, the first operation should be the login, the docker within command line, because once

we created the, the corrupt repository, we are providing the login before that.

So that's why it's the same way we can login the Docker.

The command is Docker login.

This will ask us to username and password.

I was providing before.

That's why it is direct login success for me.

You can also login your docker environment with providing the username and password.

So.

After login the docker.

Uh, we should take the Docker image before pushing the any image to the Docker hub.

The image should have the tag name and we should take the image.

It is a mandatory step to applying tag for image that will be pushed to the curb or any container registry.

The image should have the tag name, which tag name should be matched.

The GitHub repository name.

So before we start, we should have the latest tag of the image.

So we are going to take from the latest latest one, not the development one.

This means ready for the production deployment text.

So let's check the existing image and if there is any latest tag.

Yes.

So as you can see that we already run the release mode in the last time and it is selecting it is indicating

the creating the image with the latest tag.

So this image is now ready for the production deployment and pushing any container registry.

So if there is no any latest tech shopping client application you can create with running with the release

mode on Docker run profile.

Otherwise you can also make it manually with the Docker build command.

Let me change it debugged in.

Don't forget about that.

Okay, so once we have the latest client image and we should take the latest one with the Docker repository

name.

So what I'm meaning that we have created the Docker hub image name.

So we also take this.

Latest image with the GitHub repository name Docker take and we should provide the image name.

Nine C7.

I'm only giving the first three keyword.

And also let me paste.

My reporter surname from the crop.

So this will be create a new take from this shopping client image.

Let hit the enter and see what's happened.

Okay.

It is performed.

So as you can see that we give the exact name with the Docker hub repository profile, it cannot match.

Then it should be match when we are pushing the Docker image.

So it's very important.

Let me check now Docker image.

Okay.

Yes.

We have another image name, which is the metal square slash shopping app and exactly same name with

our Docker hub.

So it is successful ticket.

Now it is time to push our image to the Docker hub.

In order to push the Docker image, we will use the Docker Push Command Docker.

Push.

Let me.

Right again.

The curb push.

And we need to provide the image name.

Image name is.

Shopping app.

And of course, we should provide the.

Take name.

It is not necessary, but it is good to use the tag name.

Also when you are pushing your image.

So basically I'm directly providing the image name and running the Docker push command with hit enter.

Yes, the pushing is starting.

It takes some time because we are uploading our image to the Docker hub environment.

When we push the image, the Docker is searching for the tag name and the image name is existing or

not.

If the Docker hub find this image name after that, push the image to the Docker hub.

If not find it is giving an error.

So let's wait for a while for uploading and pushing these local remains to the Docker hub.

And after that, we will check.

This image to the from the docker up.

It takes some time and.

We should check from the Docker hub once the operation is finished.

Since that time.

Let me recap what we have done so far.

We basically adding the Docker file with right click add Docker support in our ASP.Net project.

After that, we are running with the Docker run profile, so it is building and creating Docker image

for us with using this Docker file.

And of course when we run the Docker profile, it is also running image with creating a new container

for us.

So after that we are taking our latest.

A image.

Of course, when we get the latest tech, we are changing the release mode and run the Docker profile.

After that, we have got the latest tech of image and and we also creating a Docker hub repository,

new repository.

And of course we have to take our existing image with the Grub Repository name.

Once we have the Docker hub repository name image, we can able to push our image to the Docker up using

the Docker push command.

And when we push to our image, it is taking some time and after that we will check the Docker hub.

It is successfully pushing or not.

So it is almost done.

Yes, it is pushed.

Let's check our application and refresh our page.

Let me refresh again.

Yes, As you can see, that last pushed a few seconds ago.

And we can see the our latest tech inside of the Docker hub registry.

As you can see that we have pushed the our shopping light image to the Docker hub successfully.

All these steps could be automated with the pipeline development.

We will see it later.
# 第3節：Deploy Shopping.Client Microservice to Azure App Services - Web App for Container
**Total: 6 lectures | 22 minutes**

## 3-1 Introduction
**Duration: 1 minute**
In this section we are going to Deploy Shopping.Client Microservice to Azure App Services

which is the Web App for Containers.

Let's check our big picture and see what we are going to build one by one.

So as you can see that, we are in here and finished to development of Shopping MVC Web Application project.

Project.

And we added container support with DockerFile, push docker images to Docker hub and

now its time to deploy "Azure Web App for Container" resources for 1 web application.

You can see the image that explain our steps, we had Dockerfile and this time we will push docker images

to Docker hub not Azure Container Registry (ACR)

after that we deploy to "Azure Web App for Container".

So after we have learned the basics, we can start with the coding part.
## 3-2 Start with an Azure Free Trial Subscription and Check Azure Portal
**Duration: 3 minutes**
In this video.

We are going to start with an Azure free trial subscription and check the Azure portal.

So what is the Azure account to create or work with the Azure subscription?

You must have an Azure account for Azure account.

You can create a subscription by using the Microsoft account, which is trusted with the Azure Active

Directory.

You can start with the Azure Free Trial Services.

It is giving 200 is a startup package.

So you should follow the steps with creating Azure free account and click the start free.

I have already finished these steps and created the free account for me and linked in the my Microsoft

Mail.

And please follow the these steps and create for your Azure free account.

We will follow other course other videos with using your free account.

Also, I'm going to use my free account.

So let's see the Azure portal.

As you can see that I have logged and when I click the portal portal.azure.com.

You will see the Azure dashboard in here.

You will see Dashboard and you can see the all the resources.

We can check the what options we have for deploying containers.

So let's click the create a resources.

And I am going to click the containers because we have the image container now.

So once I click the containers, it is providing me to list of the available available deploying objects

in here.

So let me close this one.

And.

As you can see that we have container instance container registries, Kubernetes services and webapp

for containers and so on.

Right.

So we will follow one by one.

And let me explain this Kubernetes service.

We will see it in the next videos and container registries.

This is the Azure Container Registry.

Also, we will see and use this Azure container registries with the with the next videos.

And this is what we are going to do.

Web application for the containers.

This is the application service for web containers.

So we are going to follow these web app for containers.

And as you can see that.

Let me start.

Okay.

As you can see, that Azure has great resources that we can use the free trial subscription.

I have created this subscription for testing purpose.

So once you watch these courses, this subscription will be gone.

That's why I can share the portal and other stuff.

So in the next video we are going to create web application for containers.
## 3-3 Create Azure Web App for Containers - App Services for Web apps container
**Duration: 6 minutes**
In this video, we are going to create Azure Web app for containers for deploying our shopping client

application.

So let's take an action first.

Check what options we have for deploying containers.

Click the Create a resources and select the containers.

See the Our options.

We are going to follow web application for containers.

You can see the documentation in here.

The whole documentation explaining how we can use these application services for deploying our applications.

Let me create a web app for containers.

So our resource group is not exist.

So we are going to create Neve which name is.

Pink up.

Okay.

So the name could be the.

Hoping.

EP also.

Or we can say that shopping web this time Resource group is the name.

It is better to use different app name, so not available.

Okay, so let's say shopping app web.

Okay.

So let me continue our configuration.

We are using the Docker container and the region is the nearest one for me.

West Europe.

You can arrange for your location.

And I'm not going to change other stuff for now.

Could the next.

So we are going to using the single container.

And the image source should be the Docker hub because we pushed our image to the Docker hub.

And yes, it is a public image and the image and the tag name is.

Let me write.

Shopping app.

Okay.

Let me verify that.

Yes, this is our application.

You can write the latest tech or not, it is not mandatory.

That's why I'm not providing the tech name now.

And also you will continue the next.

And I'm not going to change anything with the other configurations.

So just hit review and create button.

Okay, let me create the.

My application.

Deployment is progress.

So basically what I have expected that.

The Azure Azure Web app will be retrieve my image from the Docker hub and deploying my image into the

container with the Azure resources.

And this is the basic deployment process for single container.

First time.

It takes some time.

Yes, the deployment is complete.

Very good.

Go to resources.

So now I am inside of the our application deploying application.

And as you can see, the URL once you click the URL.

Let's see what happened.

The first time in the deployment and retrieving the Web application.

It takes some time.

So it is very easy to deploying single container with using other resources.

Let's wait for the light.

So as you can see that you can.

Examined these deployment tops and other settings.

This is the application service.

Resources in the Azure.

So we basically have a one container and pushing to the docker up and we set the other creating the

app service and please, uh, deploying a container and retrieve container from the Docker hub.

And it is creating successfully.

So that means Azure deploying our container into the app service.

And yes, finally, as you can see that, uh, we can see the, our application, this is now on production

live.

And uh, once we commit the code from the Visual Studio, it is basically.

Push the R codes into GitHub repository and Docker up, identify with the webhook and start building

with the continuous integration.

Once the image is built in here and Azure captured the this Docker hub image and deploying the application.

So this is the one single container deployment process.

What we have done so far.

We can manage to deploying container to Azure successfully.

Next time we will automate this process with the multi container application microservices.
## 3-4 Examine Azure Web App for Containers - App Services for Web apps container
**Duration: 3 minutes**
In this video, we are going to examine Azure Web application from the Azure Portal, see the Azure

portal in here.

So we have created the app service plan in the last video.

So let me open this app service plan.

So as you can see that we have a URL information.

Once we click the URL information, you can see the our application is live on production and see other

configurations in here.

These are the some configuration tops we can identify one by one, for example, uh, activity logs,

access controls, tags and so on.

So there is also deployment options for our we are using the deploying container from the Docker hub.

So for that purpose you can check the container settings.

So please see the container settings and identify that image source from the Docker hub.

So in the other way we can pull the image sources from the other container registries we will see in

the next videos.

And also if you have private registry, you can use the private registries and repository access is

public for us because we are publishing into the group with public repository.

But if you choose the Azure Container Registry, you can set the private repository access.

Also you identify that full name of image and take these places.

We are exact the same name with the crop and it should be very important with the crop because Azure

container images retrieving pulling the image from Docker hub with this name information.

This is the global unique name.

And also you can see the continuous deployment is off now, but we can set the on for the CI CD pipelines.

We will do it later and you can see the logs in here, as you can see that pulling from the Docker hub.

Images and the other information locks you can see in here.

So basically, Azure have a great dashboard and there's lots of options in here.

For example, scale up and scale out functions if you would like to.

Scale your application in one container.

You can use these configurations.

So let's have a look for the Azure dashboard settings.

And in the next video we are going to try to make CI CD operations for a single container.
## 3-5 CI/CD for Single Container - Continuous Deployment on Azure Web App for Container
**Duration: 8 minutes**
In this video, we are going to see how to make CI CD pipeline for single container continuous deployment

on Azure Web app for containers.

So let's take an action.

First of all, we should check for the CI continuous integration, build part of our application.

This is the CI part.

When we push to code on GitHub, it should trigger the Docker build and push the image to the crop.

So let's check the Docker up.

As you can see that we have a width sections.

And in the U.S., we have already configured our build operation with the latest tech and giving the

main main branch ID and build operation.

And we set the auto build operations right.

But in the last build, it has some error.

I'm not going to details, but I would explain later.

But it is important to arrange all these automated builds with Docker up.

Okay.

So this is continuous integration part, right?

So once we check in the our codes to the GitHub, it will trigger the Docker up and Docker up with Docker

image again and set the latest image with the new version.

And after that we can say that we should set the CD part.

That means continue this deployment on Azure web app for containers.

So for that purpose, go to Azure portal and open open the our web app for containers.

And go to our container settings.

So maybe you remember we were talking about the continuous deployment.

So set on these parameters.

And when we set on this parameter, we have a webhook URL.

Let me say first.

Okay.

I'm going to show you as well.

As you can see that we have a webhook URL that provide to hook the Azure for deployment the image.

So I'm copy this webhook URL and we are going to paste these webhook URL to the crop.

So in the crop, open the repository again and see that you we have a webhook section in the repository.

Right?

So open the webhook section and give the name.

As.

And paste this webhook in here.

So basically what we have done with the webhooks, if there is a webhook in the Docker hub repository,

once the repository changes the image it is triggered.

These webhooks and these webhooks indicate the Azure web app for containers and it is automatically

start with the pulling image from scratch and deploying our single container into the Azure Web application

resources.

So let me test our application.

Now let me change some code and push to GitHub.

We assume that the group identified the GitHub and built the image with the latest from the latest code

and add the docker up with a new tag.

After that trigger the Azure webhook that will retrieve the image from Docker hub from scratch and deploy

the image into the Azure web app for container services.

These all operations will be automated, so that means including the CI and CD part for single container.

So let me open our code.

Close this one.

Go to the my index page views.

Index page.

And in the.

Product section.

Yes.

Let me put any comments for the single containers.

Let's save the application.

So I'm going to push this code single.

Containers.

Edit.

I'm going to commit and sync with the GitHub.

Once GitHub pushed the codes, the Docker up, identified the new code chains and try to build the Docker

image with the newly added code.

Yes, single container.

Edit.

Now we will see in here.

Yes, as you can see that another build is started from the new code changes.

So this build is trying to create a new image from the Docker file and with the new code changes.

And after the build finished it is triggering the webhooks and call this address.

And once this address called the Azure Web app resources pulling image from the crop from the beginning

and deploying our image into the production.

And we will see the description into the here.

Okay.

This is the single container CI CD pipeline.

So let me show you now in the build section.

So these are second field.

It is going to preparing now.

So let me show another bullet, which is the failed because this will also is going to be failed.

So as you can see that even I provide the Docker file path is correct.

So somehow in copy commands it cannot be identified.

Copy failed from the docker image.

No such file or directory.

I'm fixing this problem from the Azure pipelines with giving the world context, which is a Docker file

and most probably we are going to change our docker file with the copy copy patch according to this

patch, because normally the Docker file should be in the root application root folder structure.

But ASP.Net put this Docker file under the project structure.

So that's why it's getting some confusing.

But we would solve this problem with giving the build context into other pipelines we will solve in

the last section.

So that's why I'm not going to proceed now.

But you got the idea we can able to perform CI CD operation with a single container with using GitHub.

When pushing to GitHub it is tricking to the GitHub and the corrupt trying to build and create a new

image with the latest tech.

And once we provide the webhook to Docker, Docker up performing the operations after the build, it

is triggering this URL and once other triggering with this URL, Azure will be pulling the image with

the latest tech and from the secret from the beginning and deploying our application from scratch.

And we can able to access new codes with changing the index page.

So this is the single container CI CD pipeline.

In the next section we are going to develop multi container microservice and we are going to deploy

these services with using the Kubernetes and automation.
## 3-6 Delete Azure Resource - Azure Web App for Containers
**Duration: 1 minute**
In this video, we are going to delete Azure Resources Azure Web app for containers.

It is important to delete, delete and release unused resources on your subscription in order to avoid

high paying bills.

So let's take an action.

Go to Azure portal and this time I'm going to delete directly Resource Group.

This is the route object.

So once I delete this resource group, this will affect all of the items.

Delete the resource group.

So it is expecting the writing resource group name.

So I'm clicking the delete.

Yes, it is deleting.

So basically it is takes a few minutes to reflect in our portal, but make sure that you delete your

resources when you don't use into Azure.

In the next section.

We are going to developing the shopping API microservices and composing microservices with Docker Compose

and moving to Kubernetes.
# 第4節：Developing Shopping.API Microservice with MongoDb and Compose All Docker Container
**Total: 6 lectures | 22 minutes**

## 4-1 Introduction
**Duration: 1 minute**
In this section, we are going to develop Shopping.API Microservice with MongoDb and compose

Compose All Docker Containers. At the end of the section, we will have 3 microservices whichs are

Shopping.Client - Shopping.API - MongoDb microservices.

Let's take our big picture and see what we are going to build one by one, as you can see, that we have

developed Shopping.Client and now we are going to start with developing the Shopping.API project.

This API project will have Products data and performs CRUD operations with exposing api methods for consuming from clients.

from the client application.

Our API project will manage product records stored in a no-sql mongodb database as described in the picture.

in the picture.

First we will develop API project which using Mongo providers.

After that, we will pull mongodb docker image from docker hub and create connection with our API project.

And of course, we will refactor Shopping.Client MVC application, instead of using hard value Product list,

lists which connected with the shopping client, Shopping Client will consume Products from API project.
## 4-2 Create Asp Net Core Web API Project For Shopping.API Microservice
**Duration: 2 minutes**
In this video, we are going to create a new web API project for shopping API microservices.

Let's take an action.

First, we are going to create a ASP.Net Core Web API project.

For that purpose, I'm going to select Solution.

Right click Add new project.

And using the ASP.Net Core Web application and the name should be copying that API.

And of course it is under the R shopping folder.

Let me create.

This will be the API project.

So that's why I'm going to use web API template and I don't need the Https.

Now don't forget to select the API template and click the create button.

Okay.

Very good.

Our API project is in here.

And after that let's modify the launch settings in order to set the port number correctly from our big

picture of the architecture.

For that purpose, I'm going to right click properties.

Go to debug section.

And change the Rom profile.

Opening API launch would be the project and our application should be the 5000.

Okay.

It should be stay.

And.

We will change launch browser later, but it seems okay for now.

Let's save application.

So let's set a startup project right click.

Set a startup project and select a running profile is Shopping API.

And let me run the application in 5000 port.

Okay.

As you can see that we have running API project in 5000 port with the swagger implementation.

In next video we are going to implement our custom controller class.
## 4-3 Create Product Controller Class for Shopping.API Microservice
**Duration: 4 minutes**
In this video, we are going to create product control classes for shopping API, Microsoft's Polizei

connection that McLaws shopping client, we are going to developing shopping API, Microsoft.

First of all, we are going to remove default items, which is the control class and weather forecast

controller.

Let me direct this item.

And also, I am going to debate this one with the forest.

Don, we are going to let our model folders so right click.

Any involves which nameless models?

In the models followed, we are going to create boardercross, but we did before for shopping APJ.

I am going to copy this for the class of Pea and paste in here.

Right.

Click first.

OK.

Or let me change this for API namespace.

Yes, we will remove this product class in the next few days, I mean, in order to integrate with the

shopping API, but let me close for client application now.

We are going to focus shopping API and the model class is product ready.

For now.

I am modifying the namespace for shopping API project and now I can add a new control class.

Which name is product controller.

Right click.

It's a new item, let me it costs.

Which name is of the controller?

And this will be the Apple comptrollers and.

Rooting for the controller colossus is the common steps for making API, creating API in a core.

So let me add, the little girl object in here is important to namespace.

And now our control, the class is really.

For now, let me modify the product, control the get method, we are going to add at least one gate

method, but for this one I am just creating a gate method for returning Moqtada.

Let me add the product.

And generating a random product and returning this product data for now, we will refactoring this method.

But now let me understand, our API is working fine with our nucleases or not.

So after we reading this gets a little bit more data, let me run our application and see what segment.

You should run in the Shopping Appia project to provide.

And see what serpent?

OK, the slugger's user interface is open and we have a one product API one, we need to execute this

API.

We can get the response for five product with randomly generated from our gate method.

OK, very good.

As you can see that the project is running in 5000 port and learning products more data.

We will develop a product list in application that people listing products on the index page for the

next days.
## 4-4 Add Data Folder and Refactor ProductContext for Shopping.API Microservice
**Duration: 3 minutes**
In this video, we are going to add data folder and refactor product context class for shopping API

microservices.

Let's take an action.

First of all, we are going to copy whole data folder from the client application and paste into the

API project.

Because we are moving the data ownership in API project one by one and let me copy whole folder right

click.

Paste in here?

Yes, we have data folder now.

Of course we should refactor the namespace.

This namespace should be the API.

And it is required also API models namespace.

Yes, we have now product context class, but it is still in a hard, hard coded product data.

So let me refactor one by one our application.

The idea is that the data ownership should be shopping API, but it will be connected with MongoDB.

But now let's leave as it is this hard coded product information and in the next time we will get products

from the MongoDB.

But now let me verify the API and client project will work correctly.

So of course we should also update the product controller classes.

Now I am not returning the mock data.

I'm removing all the items and directly now we can return product.

Context.

Klaus.

Let me import this one.

And products.

Yes.

Now we can directly return the product context that products when we are get operation Http operation

consumed.

So let me run our application see.

All products returning.

Okay, let me directly test our API.

Yes.

The product is returned successfully.
## 4-5 Set Launch Settings and Port Numbers Defined for Shopping Microservices
**Duration: 3 minutes**
In this video, we are going to set launch settings and port numbers defined for shopping microservices.

So let me arrange.

First, we are going to launch profile settings.

Let me right click shopping properties.

We should verify the our profile settings, click the debug section and the launch settings should be

project profile shopping API and the API project should be run on the 5000 port.

It is correct.

So let's check for the client application.

Client application is now using Docker.

Let me change this shopping client profile and these popping client application should be 5001 port.

It is also correct.

So both application are using the popping client project launch settings, let's say application.

So now we are going to run both two application.

Let me close all windows and we can right click the solution properties.

Click the Multiple Startup project.

Both of them make starting apply and.

Okay, so we have one breakpoint in here.

I'm going to remove that breakpoint.

Let me remove this breakpoint.

Okay.

So now we are going to run both of the application is the same time.

Let me run the application to microservice will be run in the same time.

Both application has own data.

Now, let me see.

What is the problem with the container required Docker be running?

I was stopped the docker container, but I'm right click the project.

And it still remaining to the curb profile.

Let me change again and save the application.

I think this application should be changed to running profile with right click.

Set a startup project.

And if I change in here?

Okay, I should change even also in this place.

First, I should set a startup project and change from the running profile in here.

Okay, let me save now.

So if now I'm going to set multiple projects.

I think it will run both of them in local profiles.

Yes.

Coping client and shopping project is running.

In my local computer.

5001 is the client application and 5000 is the API application.

Okay, now both application is running on our local and correct port numbers.

In the next video we are going to consume API project from the client application.
## 4-6 Consume Shopping.API from Shopping.Client Microservices with using HttpClientFactory
**Duration: 8 minutes**
In this video we are going to consume shopping API from shopping client microservices with using Http

client factory object.

Let's take an action now.

First we should go to the shopping client class project and locate the startup.cs.

Let me close something now.

We are now shopping client application and in the startup class we will use Http client factory In order

to use Http client factory, we should add registering Http client into the ASP.Net Core dependency

injection.

Why?

We are using the Http client factory.

Because in client project we should consume API project an API project exposing the http api API methods.

So in order to consume that we should use the http client factory.

Okay.

Let me open the startup.cs and configure services method is where we have defined the ASP.Net Core dependency

injection.

So I'm going to register a Http client into the dependency injection.

For that purpose.

Let me copy and paste this line of codes.

Yes, I'm using the services and add http client.

This is the for registering the Http client.

Give the name shopping API client and inside of that I'm configuring the parameters.

The main parameters is the base address.

Base address indicates that the shopping API URL in the last video we have verified the port numbers

5000 is the shopping API URL.

So by this way, I'm I'm creating a shopping client object in ASP.Net Core dependency injection.

So in any class of the client application can be inject Http client and consume the API project with

using the shopping API client.

Okay.

And as you can see that we have defined shopping API URL when registering the Http client for this definition,

we set the URL.

URL on this place.

This is the best practice.

We set the URL on this place, so every Http client object configuration created from this dependency

injection configurations.

After that we are going to inject Http client into the home controller.

So let me go to the home controller classes.

In order to consume API project.

When we retrieving the products, we should inject http client.

For that purpose I am creating the Http client with read only object.

Okay, let me import the namespace.

So we should inject with the constructor.

That's why I am generating constructor.

Select both of them and it is basically generate constructor for us.

But this time we will use different.

A CTP client.

Let me change this HTP client to Http client factory because this comes from the factory.

We would like to create a Http client from a factory.

Let me import this namespace also.

Okay.

This is also imported.

So what we are going to do with the Http client factory, we would like to get a Http client from the

http client factory with create client method, but we only give the name of the shopping client definition.

We will put the name in the startup class.

So I am taking from the name in here.

By this way we get a Http client from Http client factory.

It is also one of the best practice when using the Http clients, not directly creating for us.

It is generating and creating from the Http client factory.

So we have injected the Http client by this constructor and after that we are going to implement index

get method with calling products API from the shopping API project.

So let me come in the index method.

As you can see that we have directly give the hardcoded values, but now we are going to consume from

the shopping API.

For that purpose we should use the Http client.

Let me copy paste the codes.

Yes.

As you can see that we have a client get async method when we are using the async method.

The method should be also async and.

We are returning to a weight of logic.

That's why I am putting the async and task keywords into the method definition.

Okay, so in the first line of code we are consuming the products API from the Http client, which is

the localhost 5000 port.

So when I get the response, I'm reading the content and of course I should deserialize the object for

that purpose.

We should install the latest news Neve console software.

Okay.

When I hit the alt enter.

As you can see that installed package Newtonsoft.json find and installed latest version.

Okay.

It is.

Find and installing the Json convert package.

Now I can remove this node.

Now we are ready for the returning product list.

So let me copy the product list and returning with the operation.

It is also a enumerable product objects.

So basically we are consuming the API project from the client application with using the Http client

and giving the product suffix.

And now we don't need to use any data class data folder inside of the class client because data ownership

is in API project.

Now let me delete this data folder and rebuild the solution and see what's happened.

Okay.

It is getting error.

Let me see.

Of course, we should also remove these using keyword.

Let me reboot again.

Okay.

Now it is rebuilt.

Succeeded.

Let me test our application.

Just hit the start and see what's happened in the client project.

Okay.

These are client application which consuming products from the API project?

Yes.

The product is comes successfully.

As you can see that we have finished the consume operation from client application.

So now our client and API project work it together very well.
# 第5節：Setup Mongo Docker Database
**Total: 6 lectures | 30 minutes**

## 5-1 Introduction
**Duration: 1 minute**
In this section we are going to Setup Mongo Docker Database.

Let's check out the big picture and see what we are going to build one by one.

As you can see that, we are in here and finished to development of Shopping MVC

and Shopping.API project.

So now its time to create a real database which is no-sql mongo db.

After that we will create connection from API project.

After we’ve learned the basics, we can start with the coding part.
## 5-2 Setup Mongo Docker Database for Shopping.API Microservices
**Duration: 9 minutes**
In this video, we are going to set up the Mongo Docker database for Shopping API microservices.

Let's take an action.

First, we should go to the club and find MongoDB official image.

So let me open the chrome window and open the app.com.

In here we can search the official image with mongo.

So as you can see that we are going to use these mongo official image.

This is the MongoDB document database and you can see the how we can get pull these mongo image in our

local computer.

Just writing Docker pull, pull mongo command.

So let me see the documentation.

You can find the detailed documentation in here.

For example, how we can run these mongo image.

This is the running mongo image command.

And also you can see the how we can start interactive terminal.

Also it is including the some yaml examples.

We can we can create an interactive terminal.

Also see the logs.

And I'm looking for the environment variables.

Yes, there is a username and password.

We are going to use this username password in the upcoming videos.

And also there is a Docker secrets authentication and so on.

You can examine this documentation when you are trying to use MongoDB for example.

Now I am looking for the port number.

Is there any port number?

Yes.

As you can see that the port number is 27017.

So these are the very good information when you are playing with the some of the images from the Docker.

It is good to see documentation.

So let's see one by one.

So we are going to pull the Docker image in our local let me see the command window.

Let me see.

The code is not running for me.

Now, let me start with the Docker.

How could they stop?

Okay.

I think.

Let me write the curve and see what's happened.

Okay.

The kid is starting.

Let me wait for a while.

When the kid is starting.

And after that we will pull mongo image in our local computer.

Let me check again.

Okay.

It is not responding.

It takes some time.

I should start before recording, but let's see.

Together.

Okay.

Not running yet.

So I'm waiting for the Docker is running in my local computer.

Not running.

So after we are pulling the mongo image, we should run the mongo image in our local computer and see

the troubleshooting.

Check the logs and create an interactive terminal.

So let me see.

The car is running in my local computer.

It is take some time.

When?

The first time running.

I'm waiting for that.

Let me check.

Okay.

Still trying to running.

Okay.

We are going to using the Mongo database which showing some of the commands in the Docker and after

that we will integrate with the API project and the API project consumed from the client project.

So we will have three microservice and we are going to compose three microservices in Docker compose

files.

So we are on track now.

Let me see the corpse.

The car somehow is not starting.

This.

So slowly this time.

I don't understand.

Maybe.

Okay.

I think it is working now.

Let me.

Okay.

Let me check now.

Yes.

So our docker is running on our local computer.

Let me see the Docker image.

So as you can see that we have some of the image, which is the client application and we are taking

the client application and push the Docker hub.

So these image comes from the Docker file from the ASP.Net and ASP.Net.

These are the base image, what we are using in our shopping client image.

So now we are going to retrieve from the MongoDB and I'm copy this command and right click and paste

in here.

Why I can paste, but I'm using the docker pull.

Mongo.

Yes.

So in the first time it is again.

Take some time to downloading the MongoDB image in your local computer.

After downloaded image.

We are going to run the image in our local and create a MongoDB container with exposing the same port.

And lastly, in this video, we are going to troubleshooting with how we can see the Docker logs for

MongoDB container and how we can execute the interactive terminal.

Let me wait for a while.

Yes.

MongoDB is downloaded for my local Docker image.

Let me check the image now.

As you can see that MongoDB with latest tech is including my image list.

For now, I'm going to run the MongoDB.

So you can also see these run commands.

So I'm going to use one of them.

Let me run Docker run command and this will be the.

The.

I mean background.

I would like to work in the background and giving the port number is the same way.

This is the default port number from MongoDB.

So I'm exposing the same number for my local computer and give the name as a.

Pink mongo And of course in the last of the last command should be the image name.

The image name is the mongo.

So basically I'm trying to run existing image from my local and create a container and exposing the

same port number hit enter and see what's happened.

Yes, it is running now.

Let me check Running containers with Docker PS.

As you can see, that Mongo image is running on my local now and exposing the same port number.

We can reach this port number with the MongoDB in my local now.

So for troubleshooting we have some commands.

Let me show some basic commands, Docker logs and shopping.

Mongo We can see the Docker logs with using this command.

As you can see that this is the container logs when the MongoDB starting these logs generated from the

MongoDB.

Let's control C and exit and the other command is Docker exec, which is create a interactive terminal.

I'm putting the IT interactive terminal and let me shopping Mongo which is the name of my container

and give the bin bash we are starting the bash command right So this is basically starting the interactive

command.

As you can see that we have pulled the mongo image in our local and run and starting the Docker container

in our local computer, exposing the same port number.

In the next video, we are going to see how we are using the interactive terminal.
## 5-3 Interactive Terminal For MongoDb Connection
**Duration: 5 minutes**
In this video, we are going to connect interactive terminal for MongoDB connection and we will see

how to run mongo commands inside the mongo container.

Let's say connection.

First we should connect to interactive terminal of mongo docker container, but mongo command is working

on PowerShell.

That's why I should open my interactive terminal in PowerShell command window for that purpose.

Let me open PowerShell terminal in the Visual Studio, select the solution folder right click and open

the terminal.

It is starting developer PowerShell for me, so we will use these window.

With the next commands.

But before we start, let me check my running containers on my local computer.

As you can see that we have already started shopping mongo container in our local.

So now we can create an interactive terminal for MongoDB.

We would use the Docker exec command.

And this it means interactive terminal, give the name of the running container which is popping mongo.

You can see the verify the names in here.

And we would like to start with the bash commands hit the enter.

And see we are in container now and in the container we are starting the batch command line.

So after that we are able to run mongo commands.

So let me try with create database, create collection, add items into collection and the list connection

for that purpose.

Let me right now to see what folders inside of the container.

You can check container folders with the command.

And in order to run Mongo commands, we should write the mongo first.

Write mongo and hit enter.

So now Mongo is starting.

That means we can run Mongo commands in here.

For example, let me see the database with the show databases.

As you can see that there is only configuration admin databases including inside of the mongo.

So let me create a new database in this window.

In order to create a new database, we can use the use keyword and let me say that catalog DB.

Now we are going to create a new database in our Mongo container.

Yes, it is creating and switching database catalog.

DB And inside of this DB, I'm going to create a collection.

As you know that this is NoSQL database.

So that means instead of tables, we are creating the collections.

So I'm going to create products collection inside of the catalog.

DB.

Okay.

Also collections are created and I would like to insert many items inside of the product collection.

For that purpose, I'm using this command.

Basically, it is accepting Json objects, so I'm providing the some of the information into the our

product class.

As you know that we are setting this pre-configured data.

Yes, it is inserted when it is command.

We are using the db dot products dot insert mini command.

So let me find the these.

Product table, product collection.

You can see the product collection and get data with using the products that find method command.

So as you can see that we have two item and it is generating the ID value, which is the ID for the

MongoDB database.

And you can remove the product collection with using DB dot products, remove method.

Okay.

So now let me see the database again.

Show databases.

As you can see, that we have created the catalog and you can see the show collections.

And we have now products collection.

We are creating the product collection.

And let me check the data.

We have no data because we are removing the product collection.

As you can see that we have run several commands into Mongo container terminal.

So now we have MongoDB is running on our local computer and we are ready for the connecting MongoDB

from our shopping cart.

Shopping dot API microservices.
## 5-4 Connect Mongo Docker Container from Shopping.API Microservice
**Duration: 4 minutes**
In this video, we are going to connect MongoDB container from our shopping API microservice project.

Let's say connection.

First of all, let me close this PowerShell window and focus on the shopping API project.

Let me close these files.

And now we are going to create a connection with the shopping API and MongoDB database.

First we should go to API project and install MongoDB Driver NuGet packages.

And let me right click the dependencies and click the Manage NuGet packages.

And let me browse and.

Proves that.

MongoDB, that driver.

Okay, we should download this package.

This will provide to create connection with the MongoDB.

This is the MongoDB driver NuGet package.

Now we are going to install this package into our shopping API project.

Let's execute this message.

And installation is finished.

Okay, so after that, we should modify the product class as a entity of the MongoDB.

So let me save this project.

And we have installed our NuGet package.

You can verify that double click with the shopping API project file and you can see in here the package

reference MongoDB Driver is including in our project.

So after that, we should modify the product class, which is the our entity class, right?

So open the models folder and go to product class.

So we should add the product class with the bson ID attribute.

So as I explained to you, when we are inserting any data with the Json format in the MongoDB, it is

required as a bson ID for understanding the primary key ID you can think about the primary key of for

the NoSQL databases.

So for that purpose I'm going to add for on the ID column and.

Indicate with the attributes that we need.

And also I'm saying that this will be the object ID.

We already saw that the object ID when we are going to the interactive terminal.

But now I am indicating that this ID column will be the ID for us.

That's enough for storing the MongoDB database, only indicating for the idea for ID column.

After that we should store connection string.

So for that reason.

I am going to add the database settings into application settings Json file.

So open the application settings Json file and I'm going to add Neve configurations in here.

Let me copy paste in here or.

Let me go top of that.

Okay, so we have a database settings and the important thing is the connection string.

As you can see that the connection string is starting with the MongoDB and looking for the localhost

because we are downloading the image of the MongoDB and exposing this image, the same port of the MongoDB.

So that's why we can connect with this connection string with the MongoDB.

And I am also providing the product, the database name and the collection name.

It will be required for the when we are creating a collection and the databases.

So that's why I'm collecting all of the settings under the database settings in the application settings,

and we will use this configuration file when we are connecting with the MongoDB database in our data

class product context class.

So in the next video we will use these configurations and connect the MongoDB in product context class.
## 5-5 Connect Mongo Docker Container from Shopping.API Microservice Part 2
**Duration: 8 minutes**
In this video, we are going to refactor product context class in order to connect Mongo Docker container

from shopping API microservice project.

Let's take an action.

First we should go to product context class.

So I'm going to open product context class.

And we are going to modify this class with connecting to MongoDB.

We will arrange our codes according to Connect MongoDB database and create database and also see database.

When connecting connecting to MongoDB.

So let me start with removing this static class because we are not using the hardcode value anymore.

So I am going to add the constructor.

So in the constructor I'm going to inject the I configuration because we are retrieving the configuration

information from the application settings file.

It is requiring these namespaces.

Let me import.

So now I can create a client from the MongoDB.

We will use the mongo client class.

Let me show you mongo client.

This will comes from the mongodb driver nuget packages.

So first of all, we are going to create a client.

This line of code provides to open connection and open a client from the MongoDB database.

We are providing the connection string into the mongo client class.

After that, we would like to create a database.

For that purpose, I'm using the client object and using the get database method and providing the database

name.

In our case, the database name is product DB.

So this is looking for the database.

If not exists, it is creating a new one.

If exists, it is not doing anything.

So that's why we are using the gate databases.

After that, I would like to create a collection as the same way.

So I'm going to create a collection.

We are using the database object this time and using the get collection method and passing the product

model class.

This is anymore the MongoDB entity class and provided the collection name collection name is products.

Okay, so I'm storing into the products I mongo collection and for that purpose let me create a property

in here.

We are storing these products into in here.

So is the last step is seeding the products.

And I would like to see the products collection with creating a new method.

I'm going to create a new method.

So let me write the Setdata method.

After the.

Or property definition.

These are Setdata method.

In this method, basically we are looking for the is there any item into the products collection?

If not, we are going to insert many command as we did before in the interactive terminal and I am pasting

with the Pre-configured product data.

So I'm going to create a new method for the Pre-configured products.

And basically we are using the new products in here.

I'm copy the new products and pasting new products into this line of code and removing the hardcoded

static method.

So let me check my class from scratch and explain better.

Okay.

We set the MongoDB connection in the constructor of the product context class.

So basically, once the product context is created, this will create connection with the MongoDB.

The first usage of the product context class.

This will create mongo connection and see the database.

After that, the whole application uses the existing configurations.

And after that we should register this context object into the ASP.Net injection method.

For that purpose, let me go to the Startup.cs.

And go to the configure services method.

In this line of method, I'm going to add the product context class.

This is a mandatory step for generating creating objects from the ASP.Net Core.

So that's why I'm adding the scoped as a product context class in here.

So we have registered our product context class into ASP.Net Core dependency injection framework.

So lastly, we should refactor our product controller class, which is the our API method, right?

So we are going to change this product controller class according to consuming MongoDB database.

So now as you can see that it is looking for the.

The hardcoded value, but we are changing these product context as a MongoDB connection.

So for that purpose, I would like to inject.

Or product context class in the constructor of the this product controller, let me modify the generate

constructor and inject the context class into the constructor of the product controller.

So now we can use the context class into our get Http method.

And I'm basically returning the context dot products.

With available.

That's why.

Let me make this function async.

And return the.

Tusk.

Okay.

The last thing will be the.

Using MongoDB driver.

Okay.

So basically I'm calling the context products property, which is the generating when we are creating

the context class in the constructor.

So I'm using the find method and the find method comes from the MongoDB NuGet package and it is set

that basically get all of the product collection and change the list object when get retrieving the

all data and using the async and the task methods because we are using the available context class.

So basically we have finished our development of the shopping API.

Let me right click and we will the shopping API microservices.

We succeed.

We have finished the coding part.

In the next video, we will test our whole application.
## 5-6 Test All Microservices - Shopping.Client - Shopping.API - MongoDb
**Duration: 3 minutes**

# 第6節：Containerize Microservices with Creating Multi-Container App using Docker Compose
**Total: 6 lectures | 45 minutes**

## 6-1 Introduction
**Duration: 2 minutes**
In this section we are going to Containerize all Microservices with Creating Multi-Container App

using Docker Compose, the Docker Compose is a Docker tool that enables complex applications to be defined and run.

With Docker Compose,

you can make multiple container definitions in a single file, and run the application by raising

all the requirements

your application needs with a single command.

Let's check our big picture and see what we are going to build one by one, as you can see, that we

we have finished to developments of our microservices on our local environment,

Also, we have tested So before we deploy these microservices, we are going to create docker images

and test on docker container environment.

You will learn how to manage more than one container and communicate between them when using

Container Tools in Visual Studio. Managing multiple containers requires container orchestration and requires an orchestrator

such as Docker Compose, Kubernetes, or Service Fabric.

Here, we will use Docker Compose.

Docker Compose is great for local debugging and testing in the course of the development cycle.

So After we’ve learned the basics, we can start with the coding part.
## 6-2 Adding Docker-Compose File for Shopping Microservices Solution
**Duration: 5 minutes**
In this video we are going to add Docker compose file for shopping microservices solution.

Let's take an action.

First we should go to shopping API project.

This project.

As you know that we have already created Docker file for shipping client application before.

In shipping client we have a Docker file, but shopping API we have not Docker file yet.

So now we need to add the file, the support for shopping API project.

But this time we also need orchestration with client API and also MongoDB.

So that's why we need to create Docker.

Compose with Docker file.

For that purpose.

In the shopping API project.

Let me right click.

It.

This time we are not using the Docker support.

This time we are using the container orchestration support because we have now more than one microservices

and we need two orchestrator.

And we would like to use orchestrator Docker compose.

Okay.

Target Operating system Linux.

Okay.

And Docker file and Docker compose file is created.

Let's wait for a while.

The file is created and now it is generating the Docker compose file.

As you can see that it is loading.

Yes.

Visual Studio creates a docker compose Yaml file and the override file in the docker compose node in

the solution.

This is the solution level and that project shows in a boldface font which showing the boldface font.

That means it is now the startup project.

Like right click set startup project.

It is automatically set as startup project with Docker Compose.

And as you can see, that running profile comes with the Docker compose.

So see the files.

This is Docker file.

Docker file is almost exactly same with the shopping client application.

This is the traditional ASP.Net Core Docker file.

Only pets are different.

So let me show you Docker compose file.

The could compose yaml file as you can see that it is generating from the Visual Studio and naming is

shopping API image is the pet and the shopping API and basically it is referring to Docker file location

shopping api slash docker file.

This is the basic definition of the Docker compose file of one of the microservices.

And also, if you expand this, you see that Docker compose overwrite file.

And you see that more detailed configurations in here.

Docker compose file defines all the services to be deployed in the environment and these services rely

on the either a Docker file or an existing container.

In our case, we have also shopping client application in here.

We already created the file, but also we need to add into the Docker compose file.

For that purpose, you can add manually in this file like API project, you can copy and paste in the

Docker file for the shopping client with addressing the shopping client.

But I would like to generate from the Visual Studio.

For that purpose it is almost the same.

Select the shopping client and right click add container orchestration orchestrator support.

This will look at the Docker file if exist not generated but extra.

It is also adding my microservices into the docker compose file.

So I'm following the same steps.

Okay.

Now, let me check the compose file.

So which studio makes changes to your docker compose yaml file now both to microservice are including

in my docker compose file and also including the docker compose overwrite file.

So as you can see that we have created Docker compose and adding to microservice configuration in here.

But one more left, we will add MongoDB database for the next video.
## 6-3 Adding MongoDb into Docker-Compose File
**Duration: 4 minutes**
In this video we are going to add MongoDB into Docker compose file.

Let's take an action.

First of all, we need to extend our Docker compose file with adding MongoDB database.

Now currently it has shopping API and shopping client microservice image.

So let me add new item.

This will be the shopping tab and the image will be the mongo.

So make sure that these indent indentations are correctly because yaml files are.

Where is the trick with the indentations?

So we have added the shopping with Image Mongo database.

The image name comes from the our Docker hub.

We have installed and download the mongo image from the Docker hub so we can use these image name by

giving the full name in here.

And the other microservices using Docker files because they are using their own Docker images.

That's why we are giving the pet in here.

But this time we are using existing MongoDB database and we give the image name in here.

Okay.

The second configuration should be in the docker.

Compose overwrite Yml file in the docker compose file.

We only give the definitions of the images, but in the docker compose overwrite file, we are performing

additional configurations.

So let me add shopping part.

In this part, I am giving two spaces.

And let me remove this one.

Okay.

Two spaces.

One, two.

Also two spaces.

Two spaces.

Okay.

Container name restart port number and volumes.

Yes.

We have also give additional definition for the Mongo database.

As you can see that we are giving the container name.

This is up to us.

We are giving the.

Shopping dbname and set that restart always that means if any problem it is restart since it is working

fine and this part is important, as you know that when we are running the MongoDB image in a container

at that time we specify the port numbers for forwarding the ports.

So this is basically set that you can forward this port into my local port.

I'm using the same port number and exposing the port number to my local computer.

So by this way I can connect with the Mongo database with local shopping API project and the volume

definition is the common common practice.

I'm using the volume definition.

Yes.

As you can see that we have defined our mongo image in our compose file and exposing port 27017 port

number and configure this MongoDB image in our Docker compose override file.

Save these files.

Let me save and make sure that your indentation is correct.

In the next video we are going to run our Multi-container application.
## 6-4 Run multi-container application with Docker Compose
**Duration: 10 minutes**
In this video, we are going to run Multi-container application with Docker Compose.

Let's take an action Before we start.

I should have arranged the port numbers, as you know, that we have give the port numbers for the MongoDB,

but we didn't arrange the port numbers for shopping API and client application.

For that purpose, I'm going to give the port numbers in here.

Let me put two 8000 won for the client application and the 8000 for the API application.

Normally in our local environment, we are choosing the 5000, one, four and 5001 for the client application.

So it's the same way in the Docker environment.

I am giving the 8000 and 8001.

So by this way, when we are running the multi container application, we expect that they should have

these port numbers when we are reaching these applications.

Okay.

So again, before we start, let me check our Docker image and the cur running containers right click

open in terminal.

So let's check the current version of the Docker image.

As you can see, that we have this Docker image now we have on shopping client, but we didn't have

the shopping API.

And let me check the running containers from this image we have.

Only mongo image is running as a container in my local computer, so we are going to run Multi-container

application and we have expected that API client and Mongo project will be run a container in this list.

But we have two options in order to use Docker compose command.

One we can close all Docker and run the manually Docker compose command.

And the second option is to run with Visual Studio.

Docker compose running profile.

Actually, it is also run the same command.

Underlining Operation Visualstudio.

Running Docker Compose Command.

That's why I would like to choose option two.

So let's make sure that Docker Compose is a startup project and make sure that your running profile

is Docker.

Compose and hit the Docker, compose run profile and see what happens.

I'm going to click the Docker Compose Run.

So if we follow the output window.

We will see the Docker compose command in here.

Basically we are expected that building the.

And running in the container.

So as you can see that this is Docker compose command.

First of all, giving the file name as a Docker compose and also giving the override file name.

We also run this command by manually, but it is good to use when you are developing in Visual Studio.

As you can see, that API and the client project Docker file is running again.

It is getting error.

Let's see.

What is the error?

Shopping database cannot start the service.

Because port is already allocated.

Allocated.

Okay.

So this is basically set that we have already running container for the mongo.

So let me stop before running again.

I'm going to stop this running container.

And make sure that our docker is running.

Container is stopping.

Let me close stop.

As you can see that it is starting stopping client and API, but let me run from scratch.

Let me.

Stop all Docker containers.

Basically even it's giving an error.

It is already running these containers in our local.

But I would like to start from the beginning and don't get any error.

So it is good to check the corpse and.

Make sure that you have not running any containers.

Okay, let me run Docker.

Compose again.

So this time I think it will be faster.

These are Docker Compose Command, which Visual Studio are using when we run Docker Compose.

It is building the Docker files and.

Now it is ready for debugging.

This is one of the another good feature in Visual Studio.

As you can see that we have containers and it is starting the whole multi container application with

Docker compose in a debug mode.

It is a very good feature.

So we have our applications now.

The first one is the.

Our project.

And the second one will be the client application.

So.

Before that.

Let me check the containers.

As you can see, that shopping API and shopping client containers in here.

And we have also shopping DB containers.

And we have stopped shopping.

Shopping mongo image we already created before with Manuela.

But these images, these containers created from the Visual Studio when we are clicking the Docker compose

command.

So in this stage if we again check for the.

Terminal.

Let me.

Check now.

If we set that.

Local.

Pierce Now we can see the running containers.

Yes.

As you can see that these shopping client and API image created from the Visual Studio and also it is

running in my Docker containers.

If I check the Docker image, I also see the shopping client and shopping API.

There is also one more shopping client and as you can see that these tag number is tag name is different

one for the latest.

And maybe you remember in the last section we are pushing these image to the docker up because it is

ready for the production.

But this time Visual Studio creates these images as a dev development or environment.

Tag it with the dev information.

So by this way we are in debug mode and we are on development purpose.

It is very good to use these visual tooling.

If you are changing these debug mode to release then visual Studio directly change these text and create

new image as a latest tech.

Just for your information.

Okay.

We have checked our containers and also this is the visual way in the container image with the containers

tab, we can check the port numbers and as you can see, it is redirecting to 8008.

You can check the log numbers.

Our API project port number should be the 8000.

Yes.

Let me click again.

And that's in order to use swagger, we put a swagger index.html.

We are testing the shopping API is working fine.

Let me test with our client application.

And this time we got the exception, which is about to address information, request address.

So that means we had a problem with the connection API inside of the client code.

The client code is cannot connect with the API project.

Even the API project is running fine.

And let me try with the products get from the.

MongoDB.

It takes some time.

Most probably it is also get exertion.

It is trying to connect the Mongo database now and retrieve the product information.

Okay.

As you can see that we are in debug mode and we saw that a timeout exception happened when trying to

connect Mongo database.

So let me paste this exception.

We have two exception now.

One for the timeout exception.

That means API project cannot connect to mongo database.

Also one exception for the client application.

That means our client application cannot find our API project.

Both exception seems to network problems, so we are going to solve these problems in the docker.

Compose overwrite file with giving the correct configurations in the next video.
## 6-5 Exception Fixes on Running Docker Compose
**Duration: 21 minutes**
In this video.

We are going to fix timeout and socket exception.

We got these exceptions when we running Multi-container applications with Docker compose command in

the last video.

So let me fix these errors.

First of all, let me focus on timeout error.

This timeout error happened between API and MongoDB containers, so somehow API cannot connect to the

MongoDB.

Even we run the mongo container in our local computer.

Let me examine the API project and see what is required.

Let's see the examine this shopping API and see the product context class.

You can see that.

For connecting mongo database.

It is using the connection string.

And this connection string is looking for the localhost.

Let me go to the application settings and see that the MongoDB connection string.

This is the server name is localhost.

So in the current environment, these mongo server name should be the shopping db, which is indicating

in the overwrite file docker compose or write file.

Let's see the docker compose overwrite and see that container name.

The server name for the mongo is shopping DB.

So we should overwrite these settings in the docker compose environment, right?

In order to overwrite this connection string settings, we should add these into the environment variables.

So let me add a Neve environment for this for shopping API.

Okay.

So in order to overwrite application settings, we should use these.

Writing.

Let me show you in here.

Application Settings.

As you can see that we have the database settings, and in order to override connection settings, we

put the double point in here and we are providing the connection string as a shopping db.

The difference with the localhost and the shopping because our mongo server name.

Now shopping in the Docker compose environment.

Okay.

We have injecting these connection string or the environment.

So let me add.

Some additional configuration in here.

Does the depends on configuration, we can provide the depends on configuration in here.

And we basically set that it should be dependent shopping database because we thought the Mongo database

API project is not running properly.

Right.

We are putting the depends on by this way, when we are composing these containers, the first is start

with the shopping DB and after that starting the API project and is the same way I can put these information

under the client project.

And it is depends on.

Of course, the indentation is very important.

Basically it is.

Depended on shopping and also shopping API because client is consumes data.

Shopping API and shopping API.

Retrieve data shopping TV.

So client image should be run in the last place of the docker compose.

Okay, we had this configuration and we can now run Docker compose.

But before that I would like to set all right in the application settings Json file because when we

are running this application with the Docker compose, we should still getting configurations from the

application settings because it is running in development mode.

That's why we should also update this shopping TV.

Okay, let me run this Docker compose and see what's happened now.

We are focusing on the timeout exception and we are now solving the shopping API and the MongoDB connection.

We will get products with the swagger and we are expecting to retrieving products MongoDB successfully.

Now it is building image from scratch.

It takes some time.

Let me wait.

Okay, now it is running.

An R API project.

We'll start.

Yes, it is started.

So now let me get products again.

It get product API over the swagger.

And as you can see that products came correctly successfully.

Basically, we had a connection with the MongoDB this time connection string get image name server name

from the Docker compose and we give the shopping and it is the correct connection server name.

So we had successfully and sold the timeout aggression.

Let me close this one.

So now we can focus on the socket exception.

As you know that we got socket exception when opening the shopping client index page.

Basically, shopping client application cannot connect to shopping API microservices because the URL

that try to connect API project was wrong.

So let me examine shopping client project first.

So let me close shopping API and examine the shopping client project and see the startup class.

And.

As you can see that when configuring the Http client address, it is giving the localhost 5000 address,

which is the local environment of the shopping API URL.

But we are in the Docker environment now.

So when we when we are running in Docker, compose these micro services, the configuration should be

the Docker name of the API project name for the server name instead of the localhost.

So let me make these parameter as a configuration.

So for that purpose I'm going to change this base address to retrieve configuration from the application

settings file.

And we will overwrite these into the Docker compose override file after that.

So.

Okay, so.

The client application consumes the API project with looking this URL.

So we are moving this URL into the settings application settings json file.

So I am pasting this line of code.

As you can see that this time we are getting from the shopping API URL from the configuration configuration

is already injected in the Startup.cs from the ASP.Net Core application.

It is coming from the out of the box.

So that's why we can easily use this shopping API URL.

After changing these base address, we would like to create shopping API URL into the application settings

Json file.

For that purpose, I'm going to add new parameter in here.

Application file and we are putting in here shopping API URL and I'm going to add local one for now

because we are going to overwrite these configuration from our docker compose overwrite file.

By this way, Docker will be retrieve configurations from the overwrite file, but let me make it parameter

way and get from the configuration.

Now we have a code change also.

So now we have changed from the shopping client Startup.cs and also application settings Json file.

After we add these changes, we should overwrite this configuration into the Docker compose overwrite

file.

So let me open the docker compose overwrite file.

And I am going to add shopping API URL into these environment variables like database settings, connection

setting for the MongoDB before.

So let me copy and paste these environment configurations.

So as you can see that I am in the shopping client microservice configuration now and into the environment

section, I'm overriding the shopping API URL with the name of the shopping API container name.

So this is important because in Docker environment we can use the container names, but it is important

that we should name the container name.

We should not forget about that.

It is very important because when we are putting shopping API in here, the will be looking at the shopping

API container name, but we didn't give the specified container name yet.

So that's why I'm going to specify container name for all my emails into the docker compose.

Let me put first for shopping.

So once we are defining the container name as a shopping API, they could easily find to our target

consume API project and the client project will be work as expected.

Also, I would like to put container name for the client application.

It is good practice.

Of course, the indentation is very important.

And the shopping has already container name in here.

So let me clarify again.

Now we can ready to run our Docker compose, but we have now code changes on the shopping client microservices,

right?

We had code changes.

We are changing the one line of code.

Even we change the one line of code.

We should recreate the shopping client image and container into the Docker environment so we will remove

these image into our Docker environment and reboot this image from the beginning in order to reflect

to these code changes into our Docker environment, it is very important.

So in order to in order to remove all Docker environment, I will remove all items and I will remove

all items in the Docker environment and start to Docker compose command with manually.

As you know that we only use from the Docker compose run profile with the Visual Studio, but this time

I would like to show you how to handle these multi-container applications and run the Docker compose

command with manual.

So I would like to show you that how we can proceed.

Docker Compose command with module.

Let me open a terminal now.

But before that, let me right click.

Close all tabs and see the docker compose overwrite file in here.

Selected Docker compose root file and right click.

And open in terminal.

It is good to be in the same pit with the Docker compose files because we are going to run Docker compose

command.

But before that, as you can see that we have running containers and images, including our application.

Let me check the images.

So we have existing shopping client.

Image, which is the old version.

Now, we had a code change, so we would like to create image from the beginning with the new code changes.

So that's why I have to remove this client application and to remove items in the Docker environment,

I will use some of the codes which provide to remove.

All items in one places.

So, for example, if you would like to stop all containers running containers, you can use this Docker

command.

It is basically a PowerShell command.

Think about that.

We have 20 or 30 microservices.

It's running.

So we should use to stop or remove all containers in one command.

These commands is provide these operations.

For now I have no any stopping containers.

Let me see.

Remove all containers.

We have now at least three images.

I think now it is my containers.

Is not in item.

That's why it is giving these errors.

Not important.

Okay.

Let me see this command Docker RMI.

As you know that we have removed the image, but this time it is removing the old image for me.

Let me hit the enter and see what's happened.

Okay.

As you can see, that this command is remove all of the images.

These are not work because we have not run the container.

If you have running containers, these commands also working fine.

So the last command I would like to use is remove all non images.

Maybe you see the non images in some place when you are looking to Docker image command.

So it is expecting to me yes or no?

I'm selecting.

Yes.

This is set of the Docker commands I will share with you in the video explanation.

Let me check for images now.

Okay.

I have no image, okay?

I have no running containers.

I have no waiting.

Stopped containers so totally free and totally empty.

My docker environment.

So now I'm ready to run.

And this docker compose application using this Docker compose and Yaml file for that purpose.

I'm using this command.

Let me copy and paste directly to docker.

Compose and give the file name docker-compose.yml file name.

And also I'm giving the docker compose overwrite file name in order to get the configuration from this

file name.

And we are basically make the docker compose up command and the mean working from the background.

I mean work these commands from the background for me.

Let's hit enter and see what's happened.

We had cleared the air, the environment and now we are starting from scratch with manually writing

Docker compose command from us.

As you can see that this is pulling the Mongo and all images from the beginning.

The idea is that once we change the Encode in our any particular microservices, we should rebuild our

image with the new code.

With this Docker file, if you check this Docker file, you will see that this is using the Dotnet restored

and Dotnet build commands which is basically build and restore with the latest codes.

So we need this one because we make parameter way these configuration and also overwrite this configuration.

In order to overwrite this configuration, we should need to give client image.

As you can see that these steps for the client and API steps API Docker image are building with these

Docker files and building again our microservices.

Let's wait.

Wait for a while and see what happened.

During this time.

I also would like to show you again our Docker compose file.

It is very important.

Make sure that you are putting the container names because Docker looking the container name when it

is searching with the server name.

As you can see that we are giving the shopping API URL directly http and give the container name is.

Network configuration for the Docker Compose in Docker environment for the Multi-container application.

We will handle these network and service configurations in Kubernetes with different way.

I will also explain the topics with the next section.

So we will remove the all items.

So that's why it is taking so much time.

We'll link image from scratch and getting MongoDB from scratch and composing all of them in one place.

Yes, as you can see that our images are running now.

Let me check Docker images.

Okay.

Image created and let me check running containers.

Yes, we have three running containers.

Client API and mongo.

So we are going to open chrome and see what's happened.

This time we are doing it manually.

So let me pose at the tops and see what's happened.

So I'm going to refresh this swagger and.

It to get products.

See that the product's coming correctly and properly, and this time we are going to check localhost

8001 So you can also check the port numbers in here.

The port numbers 8001 for the shopping client application.

So let me.

Go back and open the Locost 8001.

See that we have successfully retrieved the products from the API application.

Now our client application can able to connect the API application and APIs, retrieve products from

the Mongo database and we have successfully integrate all microservices in the Docker environment.

So all these microservices and Mongo database is running a clean environment with Docker compose command.

As you can see that we have finally the cries all microservices and running on our local machines.

It is a very good progress.

So let me.

Before we close this video, I would like to show you how we can stop all these microservices in one

shot.

So as you know that we are using this Docker compose command when we are running the all microservices

for removing and stopping all containers, we can use Docker compose down command with giving this specifying

docker-compose.yml files.

Let me.

Hit.

Enter and see what happened.

So as you can see that all containers are stopping.

And if you check the Docker PS, you see nothing, all containers stopping, but our image should be

stored as usual.

Okay, very good.

So finally we have finished that dockerize all microservices and running on our local computers.

In the next section we are going to see Kubernetes operations.
## 6-6 Recap Docker Commands
In this video we are going to see some useful Docker commands.

Let's take an action.

So the first command should be the Docker pull command.

We we have used the command.

Let me use Docker Pull Mongo.

As you remember that we use this Docker pull command from the Docker hub.

This is basically retrieving the official mongo image from the Docker hub.

So once you pull any image, you can build this image with using the Docker build command or you can

run this image directly using the Docker run command.

When you are using the Docker run command, we have some extensions.

For example, you can.

Of course.

Right.

Image name.

But the there is some configuration we can use.

For example, the dash means that it is working for the background and also you can give the name.

My mongo.

And also, of course, we should keep the port number.

The port number.

You can give that like this way.

Basically this is the exact port which related with the mongo and you are exposing this port number

into your local computer.

If there is two port number in that image, you can specify another port number like giving the additional

command, for example.

Like that.

I'm not continue.

Much.

And the other command, which is the Docker build command.

You can build your Docker image if you have existing Docker file.

In our cases we have developing API and client application and we had a Docker file.

If we had a Docker file in that location, we can run the Docker build command and we can give the name

application name like the my client application.

And also the pet is a Docker file with giving the dot that means the same location with Docker file,

or you can give the file that location Docker file.

Okay.

Once we built our image, you can also run this image in your local computer.

When we are building the image, it is creating a new image.

When we are running the image, it is creating a new container.

You can think about the image is a C sharp class and container is a object which is creating from that

C sharp class.

And also in the last videos of this section, we are using the compose commands for Multi-container

microservice application.

You can use the Docker compose commands, Docker, compose command using like that Docker compose up.

But of course we have defined some configurations like file overwrite file and also give additional

file overwrite Yaml and so on.

The best practice is using that.

The file should be the docker compose file, docker, compose yaml file and give another file which

is docker compose overwrite file.

But you can give specify these override file like this way, for example staging production and so on.

By that way you separate the configurations according to your development or staging production environments.

It is one of the good practices using Docker compose Yaml files and of course in the last step we saw

that the if you have, for example, 2030 microservices, it is good to use Docker compose down command

because it is basically stop all containers and perform all stopping operations in one shot in one command.

And of course we have some Docker commands, actually PowerShell commands in order to stop multiple

containers.

I was chased in the last video, for example, Docker Stop.

It is basic PowerShell command stopping all running commands or also we will use the.

The RMI.

That means remove images.

It is removing all of the images in your local computer.

So as you can see that we have finished the Docker part.

In the next section we are going to talk about the Kubernetes.
**Duration: 5 minutes**

# 第7節：Introduction to Kubernetes
**Total: 15 lectures | 1 hour 9 minutes**

## 7-1 Introduction
**Duration: 1 minute**
In this section we are going to give introduction about Kubernetes. We will talk about What Kubernetes

Kubernetes is and why we should use Kubernetes for deploying microservices.

Also we will talk about Kubernetes components and general architecture of operations.

general architecture of operations.

We will discuss minimum manifest definitions for deploying microservices,

we will talk about deployments, replicasets, pods, services and so on.

We will install kubernetes on our local machine. And see main kubectl commands that

can create kubernetes resources.

And of course, we will give some information about K8s yaml configuration files.

After we’ve learned the basics,

we can start with the coding part.
## 7-2 What is Kubernetes ?
**Duration: 1 minute**
In this video we are going to talk about what Kubernetes is. Kubernetes also known as k8s

or "kube" is an open source container orchestration platform that automates many of the manual processes

involved in deploying, managing, and scaling containerized applications.

Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services,

that facilitates both declarative configuration and automation.

It has a large, rapidly growing ecosystem.

Kubernetes services, support, and tools are widely available.

You can cluster together groups of hosts running Linux containers, and Kubernetes helps you easily

and efficiently manage those clusters.
## 7-3 Kubernetes Architecture
**Duration: 2 minutes**
In this video we are going to talk about Kubernetes Architecture. Kubernetes is working on clusters.

The central component of Kubernetes is the cluster. Cluster can be many virtual or physical machines

that they also includes many nodes. Each node hosts groups of one or more containers

which contain your applications,

and the master communicates with nodes about when to create or destroy containers.

You can visualize a Kubernetes cluster as two parts: the control plane and the compute machines, or nodes.

Each node is its own environment, and could be either a physical or virtual machine.

Each node runs pods, which are made up of containers.

The control plane is responsible for maintaining the desired state of the cluster, such as which applications

are running and which container images we are going to use.

Kubernetes runs on top of an operating system and interacts with pods of containers running on the nodes.

Kubernetes control plane takes the commands from an administrator and relays those instructions

to the compute machines. The desired state of a Kubernetes cluster defines

which applications or other workloads should be running,

which resources should be made available to them, and other such configuration details.

Such configurations details.

So The only thing to do is configuring Kubernetes and defining nodes, pods, and the containers within them.

Kubernetes handles orchestrating the containers.
## 7-4 Kubernetes Components
**Duration: 3 minutes**

## 7-5 Local Kubernetes Installment
**Duration: 5 minutes**
In this video, we are going to install and run Kubernetes on local environment.

There are few options to run Kubernetes on local environment like Minikube, Docker, Kubernetes and

so on.

We will follow the Docker Kubernetes installation.

So let's take an action now.

First of all, we should open the Docker dashboard and Docker settings.

We should go to the Docker settings and go to the Kubernetes section.

You will see the Kubernetes configurations in here and just click the enable Kubernetes.

We are not selecting checking these configurations.

It is enough for us to enable Kubernetes and once we click the enable Kubernetes apply and restart button

enabled and you will see that only the query is running for our local environment and click the apply

and restart button.

It is required some Kubernetes installation, install installation, and let me click the install.

Basically Kubernetes create for us.

I mean, Docker creates for us a Kubernetes cluster and we are going to run Kubernetes on the Docker

environment.

By this way, we can use Kubernetes in our local environment.

Let's wait for a while.

And after that you will see that Kubernetes also in a green window like the Docker.

So you should see the Kubernetes also green on below line like Docker is the same as Docker.

So by this way, Docker creates a Kubernetes cluster for us that we can use local Kubernetes operation.

It takes some time.

Let me wait.

Because Kubernetes, I mean, Docker downloads the Kubernetes related components.

In its own location, but that's why it takes some time.

After that, we will check the Kubernetes components with using the Kubectl commands.

Once download completed, the kubectl commands will be available for us in local environment.

Okay.

It takes for me almost five minutes.

After five minutes, I saw that the Kubernetes installed and the green image comes.

So now let me verify that the Kubernetes is running on my local computer.

For that reason, I am going to open a terminal.

Okay, we are running Kubernetes into Docker environment with Docker desktop.

It is a good feature.

So I would like to start with Docker image.

Let me show you the Kubernetes image downloaded from the Docker.

And as you can see, that this is required components for the Kubernetes.

So now we can see the Docker image, including the Kubernetes related images and also if we check the

running containers.

You saw that these are the Kubernetes related containers are running in our local computer.

So that means we can run kubectl commands.

Kubectl means that the interaction with the Kubernetes commands, we can say that kubectl get all and

you will get some results if you can.

Some get results with the kubectl get all that means the Kubernetes is working on your local computer.

As I see that the Docker I think, also deployed our Docker containers.

I'm not sure it is the same result with you, but it is.

It doesn't important.

We are going to start from the scratch for deploying.

So that's why I will clean these deployment and the pods and start from scratch for the next section.
## 7-6 kubectl Commands
**Duration: 7 minutes**
In this video, we are going to see basics of kubectl commands.

We will see the get Kubernetes resources commands like pod service deployment and so on.

Also, we will see the Crud commands.

Of course, we should mention about abstraction with deployments and pods.

So Pod should not create at all.

This will start from the deployment creation and there is also debugging troubleshooting commands.

You can find all commands from the kubectl cheatsheet in the below link of slide.

So let's take an action for now.

Let me open Visual Studio.

And run kubectl commands.

We will start with the cube.

Ctl just hit the cube Ctl and see what happens.

As you can see that Kubernetes deploy separate main commands in here with the captions.

For example, basic commands which including kubectl, create, expose, run and set.

And deploy commands.

Cluster management, troubleshooting.

Advanced commands and so on.

So if you write the kubectl, you get a great explanation about the commands which you can.

So let me close and write for the kubectl help.

Another way to understand the kubectl commands, it is the same result.

You can write directly kubectl or kubectl help in order to use some kubectl commands.

For example.

Troubleshooting and debugging is very useful commands.

We can see the describe logs and other exec commands.

The explanation is very good in here.

So now let me start from the kubectl get commands, but before that we can see the kubectl cluster info.

So cluster info is explains the which cluster Kubernetes is running now.

So as you can see that Kubernetes master is running Kubernetes, that Docker, that internal and Kubernetes

DNS is running in this URL.

You can also see the your cluster information with right click Kubernetes.

And see that your context should be the Docker desktop.

Okay.

So another version is the kubectl commands is the cube Ctl version.

You can see the version of the kubectl using the kubectl version command.

And now we can paste the cube Ctl.

Get commands.

As we explained before, kubectl.

Kubernetes has nodes so we can get nodes.

We have one node now, which is the Docker desktop.

The car is creating a node Kubernetes node for us.

So another kubectl get commands, which is the port.

As you can see that we have some pots, some of not not running, but most probably it's came from the

my examples.

You can see empty list in here.

But the pot definition is the storing our containers.

So another kubectl get command is the services.

We will explain these definitions in the next videos, but I'm showing how to get the resources in Kubernetes

with the command line so we have service definitions.

And if you would like to see all resources, you can use the kubectl get all.

And as you can see, that we have port services and deployments.

So let me see the kubectl get deployments.

And I would like to delete my old deployments.

And of course, when I remove the deployment, it is an abstraction of the replica set and pods.

So directly replica sets and pods will be delete.

So let me write kubectl delete and see empty list for me.

Mongo deployment.

Let me write from scratch.

It.

Me delete.

Mongo deployment.

Okay.

Let me see.

Cube Ctl delete deployment.

It should be delete deployment.

And Mongo deployment.

Okay.

And another one is the shopping API deployment.

We will create all these resource from scratch with together and shopping client deployment.

So once I delete these deployments, let me see.

Get all.

And see, as you can see, that there is no ports and other services is running anymore because once

we remove the deployments, the ports and replica sets directly deleted because it is an abstraction

for the deployments.

Okay, so we are only some of the service definitions and let me remove this service definition also.

Kubectl get service.

Cube Ctl delete service and give the service name.

Mongo service.

And also.

Shopping API service and shopping client service because we are going to.

VPN client service.

We are going to create all these resources with together.

Let me get again Cube kubectl get all again.

Okay.

Most probably you will see this result and we have no any replica set port deployment and services now,

but we are going to create one by one with together.

I would like to show you basic commands.

There are basic commands that we can see for the beginning, but we will use these commands later when

deploying the microservices on Kubernetes.
## 7-7 Declarative vs Imperative
**Duration: 2 minutes**
In this video, we are going to talk about declarative and imperative way.

There are two basic way to deploy to Kubernetes imperatively, which is working on CLI with the kubectl

commands.

Or declaratively by writing manifest files and using kubectl apply commands.

Kubernetes able to run both declarative and imperative way.

Let me talk about the imperative way configurations.

This is the shortest way to deploy to Kubernetes is using the kubectl run commands.

You can find the example in here.

It is said that run my application with some image and create a two replicas for me and you can perform

this command immediately now.

So when we are using the kubectl run command, it is working this command immediately at the same time.

So that means we are running this operation in imperative way because we are explaining our desired

state in the command line and Kubernetes is knowing what you have done in the same time.

Okay.

So the another way to is the declarative way.

So declarative configuration is the power of the Kubernetes because it is in declarative API.

And the controllers, you can just tell Kubernetes what you want and it will know what to do.

For example, this is the example Command cube Ctl apply and give the file name yaml file name yaml

file name is the manifest yaml file name and you will just use kubectl apply yaml manifest files.

This will save it by the Kubernetes in Etcd component inside of it and Kubernetes.

Try to reach to that desired states which specified in the manifest yaml file.

You can think the Kubernetes as an infinite wild code block.

You give your desired state and it is iterating always and try to reach your desired state.

So we will start with the imperative wait of the Kubernetes commands.

After that, I will show you how can we work with the manifest Yaml files in declarative way?
## 7-8 Creating Pods on Kubernetes
**Duration: 5 minutes**

## 7-9 Creating Deployment on Kubernetes
**Duration: 6 minutes**
In this video, we are going to see how to create deployments on Kubernetes with imperative way.

And this will triggering Kubernetes to create replica set and pods.

So let's take an action.

Let me start with creating this window.

And I would like to show you again in the same example.

So we are going to use kubectl create commands.

If you hit enter, you can see the options.

What we can do with the kubectl create command.

So these are the parameters you can provide that we will use the image parameter.

I mean, we are going to give the image which image that will be using in our Kubernetes create commands.

So let me create again and write that kubectl create deployment.

This time we are going to use deployment.

Normally the creating pod operations will not be never using because there is an abstraction from the

deployment.

So always it will be start from the deployment level.

So that's why we are going to create deployment of troops.

It'll create deployment and you can give the name again, Nginx deploy.

I'm giving the name of the deployment and of course I am providing the image name.

This is the main parameter.

We can say that the image should be the ingenix.

So when you hit the enter, it is basically creating a deployment.

Okay.

If you see the cube Ctl get deployment, you can see the deployments are creating.

But there's interesting thing, if you can see the cube Ctl get all, you will see that instead of only

deployment, there is also we saw that it is creating a replica set and also it is creating the A pod.

So once we are creating the deployment, the Kubernetes check, the deployment and according to the

deployment, Kubernetes creating one replica set and also one pod for us.

So let me show you from scratch.

Let me say that Cube Ctl get deployment deployment is an umbrella configuration and in the siblings

or.

The another level of the replica set get replica set.

So as you can see that once we are creating a deployment, it is also creating a replica set and adding

the some unique identifier and replica set.

Also creating the pods.

Kubectl get pods.

So as you can see that the same identifier in here and adding the additional identifier in here because

replica set also creating the pods.

So this is the tree structure.

Deployment creates a replica set.

Replica set creates the pods.

So you should never create a pod.

Instead of that, we should always working with the deployments because deployment is the umbrella root

level of the configuration when when you are creating the Kubernetes pods.

Okay.

So you can also follow these unique identifiers in here.

So now this time we explained that we have an abstraction from that.

Pod sets are abstracted from the replica set.

Also replica set abstracted from the deployments.

And now we are going to edit the our deployment and we are going to make our replica set to.

So that means we expected that the port number will be two.

So let me write kubectl edit deployment.

And give the deployment name a name.

Engineers.

Dipl.

It enter.

When I hit enter, the notepad is opened.

So as you can see that once we created a deployment.

And at the behind of us.

The Kubernetes created this file for us.

And this is the manifest file we will see in the next videos.

But I'm just only focusing on now replica set number.

So that should be replicas.

Yes.

As you can see that.

Now replicas count is one.

I'm changing to two and save this file.

Save and close this file.

Okay, It's edited.

So once we check the kubectl get pods.

And as you can see that the second pod creating immediately once we change the replica count of the

deployment.

So deployments are like a brain of our pod creation and replica sets.

So you should always edit the replica sets and another configurations in the deployment limit and Kubernetes

check the deployment manifest file and apply their operations according to these configurations.

So as you can see that, we can see how created pods with the kubectl create command.

In the next video, we are going to focus on the troubleshooting on Kubernetes.
## 7-10 Troubleshooting on Kubernetes
**Duration: 7 minutes**
In this video, we are going to see logging and troubleshooting of pods on Kubernetes.

Let's say connection.

First of all, we start with creating deployment and go inside the port with interactive terminal and

see the logs and explain how to troubleshooting.

But before that, in the last video, we have created these ports, right?

In order to see port logs, we can run the kubectl logs and giving the 1 or 2 port name.

Let's copy this one.

And paste in here.

Let me write again kubectl logs and paste in here.

So basically by this way.

We can.

I'm writing wrong Cube Ctl logs and provide the name of the pod and hit enter.

You will see the logs of the correct pod.

Yeah, right.

I mean, this is a basic way to checking logs inside of the pod.

You can specify the pod name and write the cube logs.

So let me clear and start from the beginning.

In this video we are going to use Mongo example.

So I'm going to start with the Create New Deployment Cube.

Ctl create deployment and deployment name will be the mongo deploy and the image should be the equal

mongo.

Okay.

Hit enter.

And so it is creating for us.

Let me see cube Ctl get port.

And as you can see that Mongo deployment is running and the pod is created from us, from the Kubernetes.

So if we would like to see the details and see the history of the plot creation process, we can have

another command, which is the kubectl describe command, kubectl describe pod, and we should provide

the pod name.

Again, let me copy first and.

You.

What's happened?

I'm copy these things.

And Cube Ctl.

Describe port and paste your port number in here, but it is not copy for me.

Let me copy again Ctrl C.

No.

Why?

Can't copy this line of code?

Okay, now it's coming.

Let me write again.

Cube Ctl, Describe port and paste in here.

Hit, enter.

So as you can see that we have set of information about the port, the name namespace, node, start

time and so on.

But the important part is the events.

So once the we create a deployment on behind of that Kubernetes created pods according to this deployment

manifest file.

So every step of the events it is adding the log in here so we can see that the started the container

Mongo created container mongo and successfully pulled image Mongo from the Docker hub and pulling the

image and successfully assigned.

As you can see that there is a event log from the Kubernetes and it is very good feature.

So we will use these describe command when we are troubleshooting the pods.

So for example, if this pod is not running, in some cases the status will be getting error.

So in that cases we are going to check these event list.

So most probably there should be a problem with the pulling when pulling the image or something else.

So this is one of the good practice when troubleshooting the pod events.

Kubectl describe pod and give the pod name.

So let me go one step further.

Kubectl get pods and now we would like to go inside of that pod and create an interactive terminal.

For that purpose.

We are going to use Kubectl Exec Command.

It is similar with the Docker exec command.

So I'm going to again provide the pod name and dash it.

It means interactive terminal.

And let me start with the Shell script.

Okay.

Now I am inside of the pod and if I hit the LS command, I saw the folder structure.

And if I run the mongo command now I'm in the Mongo CLI and I can perform the mongo commands in here,

for example, show databases or use databases.

We already saw these mongo commands, but this time we are going inside with the Kubernetes pod with

the interactive terminal using the Kubectl exec command.

Okay, so let me exit this command lines and close this one and let me delete our resources.

If you run the kubectl, get all you will see the all the items, all the deployments and pods and replica

sets.

So we would like to remove all the resources for that purpose.

We are not going to remove any pod or the replica set.

We only working on the deployment level.

If we remove the one of the deployment, the related objects, replica sets and pods will be removed

by the Kubernetes.

So let me focus on the kubectl get deployment and now I'm going to cube Ctl, delete deployment and

give the deployment name Mongo, deploy Mongo deployment, delete it.

And also now I'm going to delete Enginex deployed.

Yes.

If we check now kubectl get all.

As you can see, that this is a terminating status.

Pods are terminating.

I mean, deleting from the Kubernetes if you check again.

The pods and replica sets removed by the Kubernetes show in kubectl get all we have now any resource

on Kubernetes Now as you can see that pods and replica sets removing we are finished the imperative

way of kubectl CLI commands.

In the next video we are going to working declarative way of the Kubernetes commands.
## 7-11 Declarative Way Kubernetes - Running yaml Files
**Duration: 6 minutes**
In this video we are going to talk about declarative Kubernetes running Yaml files.

Basically what we have done so far.

Running kubectl commands on the command line.

That means the imperative way and that commands working immediately.

But there is also a more powerful way to define Kubernetes resources.

We can create Yaml manifest files and apply these files into Kubernetes with Kubectl apply command.

So let's take an action.

First of all, let me remind you some kubectl commands for declarative operation.

We are going to use kubectl create command.

As you can see that we have lots of parameters, but we mostly using the kubectl create and give the

file name file equal blah blah yaml file.

Okay.

And another command is the kubectl apply command.

Mostly we are using the we are going to use the kubectl apply command because it is basically provide

that if the file name exists updating if not exist it is creating.

So that's why it is.

Again, we can provide the file name and using the kubectl apply command when we are working the declarative

way and the another command is we are going to use kubectl describe.

We are going to use this, describe command this.

Ripe.

We are going to use this describe command for troubleshooting.

See the history of the port creation.

And also we have seen that kubectl exec command.

It is creating interactive terminal.

And the last one is the kubectl delete command.

Delete means the same.

Normally we provide the port replica set or deployment, but this time we are not providing any resources.

We are directly referring the file name.

We can give the manifest file name and it is deleting the all the resource related these manifest file

name, this file name mostly deployment file names and by this way, Kubernetes looking for the deployments

and deleting the replicas and and pods according to deployment Yaml files.

Okay.

We have seen the declarative way of the Kubernetes commands.

We mostly using the kubectl apply and kubectl delete commands with giving the file name.

Okay.

Let me open the naive Visual Studio code window.

I'm going to use the Visual Studio code.

Visual Studio code.

Okay.

Let me open, which is the code.

Let me.

Close folders.

Okay, I'm going to create a new empty page.

I mean, empty text in the Visual Studio code, and I would like to create a one deployment manifest

Kubernetes file.

So in the last video, we are going to create a enginex deployment, right?

So this time I am going to use the declarative way and I am going to write the Kubernetes manifest files.

So let me copy and paste and I will explain one by one.

So this is the minimum way of creating the deployments on the Kubernetes.

This will be the Yaml file and these are the Kubernetes commands and Kubernetes specifications templates

and specifications.

Again, I will explain detailed.

And as you can see that it is starting with the kind kind is type of the Kubernetes sources.

This time we are going to create deployment, but you can create a replica set services and so on.

The kind represent the Kubernetes type and every Yaml file has two parts.

One is the metadata and second is the specifications.

As you can see that after the API version and kind definition, we have the metadata definition and

also we have a spec definition for deployment definitions.

We have additional metadata and spec.

As you can see that under the spec we have again metadata and again spec.

So this is for the image.

And in the deployment we are providing details for the image creation.

That's why there is another metadata and spec definition inside of the template inside of the deployment

definition.

So you can enough for knowing the Yaml files separating by metadata and spec and for the deployments.

We have additional metadata and spec definition because we are also defining the pod definitions.

You can see that we are giving the image name and the other details in here.

So lastly, we are going to define the service definition service ports.

As you can see that we are providing the container port in here.

So this is the minimum manifest deployment file for the Kubernetes.

As you can see that we have developed this Yaml file for declarative operations.

In the next video we are going to test this yaml file.
## 7-12 Testing Declarative Way Kubernetes - testing yaml Files
**Duration: 6 minutes**
In this video we are going to testing declarative Kubernetes with Yaml file.

Let's take an action.

As you know that last video we have creating this Yaml file.

Let me save this file.

And I am going to save the specific location C users, my user and I'm going to save enginex deploy.

And.

Yaml file name.

Let me save.

Okay.

As you can see that the Yaml is defined from the Visual Studio.

And after that I am going to test my Yaml file for that purpose.

Let me open a new terminal.

So terminal and the file location is the same.

So that's why I'm going to writing directly.

Kubectl apply and giving the file name.

As you can set, you can reach the file names giving the top name, but my file name is enginex deployment

yaml file.

Okay.

So once I hit enter deployment is created from the Kubernetes and if you check the kubectl get all.

You can see that the container is now creating status.

And also deployment is created.

And according to this deployment, Kubernetes created a one replica set and also one container port

for me.

Let me say again kubectl get all and see that.

So this is running.

My pod is ready now.

Okay, let me kubectl get pod and see my pod name.

And you can also check that kubectl deployment.

Deployment Cube Ctl replica set.

Okay.

All this seems good and working fine now.

So basically we performed running the nginx deployment yaml file with the declarative way, creating

a manifest file and we directly use the kubectl apply command and give the file name.

So what we have done for now and we can also change the replica set as you remember.

So if I change this replica set with two and save the Yaml file, I need to run another command again.

Kubectl apply and give the file name.

So exactly same command I'm going to use because the apply command is looking for the a file and if

not exist creating if exist it's updating the yaml file.

So as you can see that we are changing the replica set and we would like to expect that Kubernetes identify

these changes and also perform the creating a new pod for us.

So let's hit the enter and let's say Cube Ctl get pod and let me see in the watch mode.

I can't catch the sodas.

It is directly run Kubernetes very fast for me.

Uh, let me exit.

And as you can see, that two ports created from the Kubernetes after we applied the changed.

So once we created the first time the apply command said that Nginx deployment created and once we updated

the file, it is identified the update operation and say that it is configured.

So this is another trick when you are using the cube apply.

So we have done the example application which using the declarative commands.

Let me recap our kubectl commands again.

In the declarative way we are using the kubectl create, apply and delete command.

You can create the file and you can create with the apply command, giving the file name, or you can

delete the file name with giving the file name kubectl delete command.

Also we can edit command, but this is the imperative way not related with the declarative way.

So if you look at the status, we have a kubectl get command.

You can say that kubectl get nodes, get pot, get service, get replica set and also get deployment.

You can get all with the all resources.

And another good command is the debugging port performing debug operations to see how we can check the

troubleshooting.

We can check the logs, kubectl logs and give the pod name in here.

We see we saw that this command is working fine.

And also, if you would like to open an interactive terminal, we can use the kubectl exec command at

the same with the Docker and give the interactive terminal.

And another side that we we are working describe, command, write, describe.

This is provide to a historic information about the port creation process.

It is one of the good commands for troubleshooting.

And when we are working with the Yaml, we are using the kubectl apply and the delete command.

As you can see that we have we have, we have seen that how to work with Yaml files and kubectl commands.

In the next video we are going to working on the creating services on Kubernetes file.
## 7-13 Creating Service on Kubernetes and More About yamls
**Duration: 14 minutes**
In this video, we are going to create service on Kubernetes and also give more details about the Yaml

files.

Let's take an action.

First of all, let me talk about the Yaml files.

Most common practice that when we are deploying microservices to Kubernetes to develop yaml files with

two parts.

One is the deployment and another should be the service.

Yaml file.

So we are going to create service yaml file in this video deployment for the creating pods service for

the pod communication.

So without the service definition, pods cannot be reachable and it is mandatory that if you are deploying

any microservice to Kubernetes once that should be a deployment yaml file and also that should be yaml

service yaml file.

Also these definitions could be the one file we will follow the separate file file for today and we

will see the the same file in the next videos.

But before that let me give some useful information about the Kubernetes Yaml files.

Yaml files has three part in the last videos.

I said that Yaml file has two parts.

One part is the metadata.

In here.

As you can see, the metadata which including the name, label and basic informations.

And the second part is the specifications, which is basically replicas and selectors and so on.

These definition of the specifications, these specifications is belong to the kind of the Kubernetes

resources.

For example, deployment has replicas specification, but service has different specifications.

And also there is one more part in the email files we can see in here.

Kubernetes generate for us to track the status of these files.

So after we define these deployment.

Kubernetes deployment Yaml file into Kubernetes, Kubernetes, extend this file and also adding some

new parts like the status part.

So when we develop two parts in the metadata and spec specs are special for the client deployments and

also Kubernetes adding a new main part of the document, which is the status.

Let me check for example, let me show you kubectl get deployment.

And we have already defined and created Nginx deploy Kubernetes manifest file.

So if we check the Kubernetes Yaml file, let me show you a new command kubectl get deployment and give

the deployment name nginx.

The pull and dash or that mean output and yaml file.

So we are requesting to Kubernetes to yaml file format and the deployment configuration hit enter and

see that there is a little bit more from our original file, right?

So that means once we created this deployment file.

Kubernetes adding the additional configurations, for example, annotations and also these metadata

fields and so on.

So the idea is that the main part is metadata is in here.

As you can see, that metadata is too long.

And also second part is the specifications.

Newly added specifications from the Kubernetes we didn't.

Interesting now and the last part is, as you can see, that status part.

So this part provide that Kubernetes tracking these documents and if any changes its understand with

the status parts.

So basically the Yaml files of the Kubernetes has three parts.

The first two parts is defined by us and the last part is the status generated from the Kubernetes.

It is one of the good knowledge when you are working with the Kubernetes.

And I would like to paste another useful information that is the desired state when we are creating

Yaml files on Kubernetes.

Kubernetes check that.

What is the desired state and what is the actual state?

So for example our desired state in replicas with the two pods and if the actual state is one pod Kubernetes

suddenly create a new pod.

And once we change this file and update the file into the Kubernetes Kubernetes, checking the updated

area and goes to the desired state, Kubernetes has scheduler and scheduler checks.

Check these differences and reach the desired status.

It has self-healing feature.

We can call that self-healing feature, one of the good features in the Kubernetes.

So if you change the replica count in here, Kubernetes scheduler, identify the changes and perform

the self-healing feature and create a new pod for us.

Okay.

And another useful information about the Yaml files is Yaml syntax really important trick with the indentation

so you can check for the how many empty spaces in this area.

It is really string.

If you are doing this kind of change and forget about it.

The Yaml file broken is not working.

So for that reason I would recommend to you some of the extensions for the Visual Studio code.

I will say explain these extensions in the next videos, but it's good to know Yaml file are very citric

about the syntax and the another good information is deployment replica set replica sets and the pods

deployments, pods and services talking each other over the labels.

So we should.

Define the labels is very important and the label information should be same with the service and deployment

files.

So Kubernetes is looking for these label information and matching the resources or the label information.

It is one of the good knowledge when working the Kubernetes.

You should really understand the labels and make sure to put the importance about the label definitions

because the name should be the same and according the same names.

Kubernetes match the resources.

In our case, we are going to match the deployment and service definition.

By this way, Kubernetes identifies and understand test.

These index deployment will be match with the newly created Nginx service definition and exposing forward

forwarding the ports according to these deployment pods.

Okay.

And now we can ready for the perform our demo application.

So we had written nginx deployment Yaml before and this time we are going to.

Create a new Yaml file.

But before that, as you remember, we have changed the container port.

Right?

We are exposing the container port when we are running this nginx deployment.

So that's why I would like to also change in here.

We are going to use container port 80,080.

So our deployment definition is ready.

We are using two replicas and we have container nginx and we have container port 80 to 80.

So now let me create a new file in the same location.

The file will be the service definition.

Let me copy and paste this service definition.

Okay.

This is the minimum requirement of the service definition.

API version should be version one.

It is different with the deployment because some of the Kubernetes resources belongs to different API

versions.

So that's why service should be the version one, and the metadata and specs is the same metadata definition,

including the name and spec definition, the service latest specs, including under the spec definition.

Okay.

So basically we are forwarding the port, see the port 80 and target port making the 8 to 80.

So let me save this file.

Go to C users.

And say that this will be the.

Enginex.

But this time it should be the service.

Save this file.

And this is the Yaml file.

So we have defined the deployment and service definitions.

Now we are going to apply these services onto Kubernetes.

That's why we are going to use Kubectl apply and give the file name.

First of all, I am applying the deployment because we have changed on that file.

And I'm writing wrongly apply file.

Let me.

Apply kubectl apply and the file name.

Okay, It's configured and this time the service definition will be the new file.

Hit enter and see that it is created.

So now we have created deployment and service Yaml file and as you can see that the label information

selector application nginx so the label definition is the Nginx.

So that means in the service definition we set that basically please look at the Nginx label name and

find the label name with the deployment and apply these services into this deployment replica set and

pod definitions.

Okay, now let me check.

Kubectl.

Get all.

So as you can see that we have now additional service definition.

So we have also created the deployment pods and the replica sets.

Let me see one by one.

Get pot.

We can see the pot definitions are two replicas to count.

And also, let me see the git deployment.

We have a enginex deployment and we can see the git replica sets.

We have replica set definitions in here and also we have a service definition for now.

So in the service definition, we can see the service type cluster IP and give some information in here.

But now we are going to check the pod IPS.

The important thing is we would like to see the port IPS.

For that purpose I'm going to kubectl describe service Nginx.

Service by this line of code command.

I would like to see additional details about the Enginex service information it enter.

And I think I'm writing wrongly something.

Describe service.

And the engineering service.

We should see the not want engine service.

I'm writing wrong.

The service.

Okay.

This time I'm writing.

Correct, I think, in Phoenix.

Source.

Okay.

Hit enter.

As you can see that we have get some additional details in here, but the important detail is the end

points.

And as you can see, that Kubernetes matching the service definition with our deployment and assign

the new IP addresses into the pod definitions.

So these end points related with the belonging with the pods which is creating from the deployment Yaml

file.

Please see that.

And 117 and 118.

If you check the kubectl get pods with the output and say that white definition, that means it is also

getting new columns which is the IP columns.

So in IP column we can verify that the service definition and the IPS are the same with the endpoint

and the pod definitions.

Okay.

So this is another way to getting detailed information about the pod or anything in the Kubernetes sources

you can show and the white definition in the Kubernetes command.

Okay.

So we have a good understanding and definition of the service and the deployment and the service.

In the next video, we will check the these resources onto the Kubernetes dashboard.
## 7-14 Deploying Kubernetes WebUI Dashboard
**Duration: 7 minutes**
In this video, we are going to set up the Kubernetes dashboard.

Kubernetes Dashboard is a Web based UI for Kubernetes clusters.

It allows users to manage applications running in the cluster and troubleshooting them, as well as

to manage the cluster itself.

We are going to follow official documentation on GitHub.

You can find in the in this URL.

And in the Readme file you can find the step by step explanation.

We are going to follow the same steps.

So let's take an action of first of all, we are going to deploying the dashboard UI.

So for that purpose.

We should kubectl apply command with a given file name.

The file name is existing in the GitHub address, so the dashboard UI is not deployed by default.

In our application we can see the kubectl get deployments.

Now there is no Kubernetes deployment yet, but now we are going to deploy some of the Kubernetes operations

in order to see the dashboard UI.

So to deploy it, run the following command kubectl apply and the file the address.

I will also share in the in this video attachments and hit enter.

Okay.

As you can see that lots of Kubernetes resources defined in our cluster Kubernetes cluster.

After that, we can access the dashboard UI to protect your cluster data.

Dashboard deploys with a minimal configuration by default.

So currently dashboard only supports for logging in the using the beta token.

So in order to create a token for this demo, we will get the admin user token from the Kube system

secret definitions.

So we should get the bearer token when logging the Kubernetes dashboard dashboard to in order to get

the token, we have a additional command kubectl describe secret and we are looking for the cube Ctl

kube system user hit enter and as you can see that we have a token response.

We are going to use this token when we are when we will log into Kubernetes dashboard.

So now we can use this command line proxy.

In order to open access to Kubernetes dashboard, we should create a proxy so we can access dashboard

using the Kubectl command line tool by running the Kubectl proxy command.

Let me right in here.

Kubectl proxy and hit enter.

Okay.

It is starting to serving this address.

So kubectl now available dashboard in our local computer.

For that purpose we are going to open a link.

This also sharing in here.

I'm hitting this link.

Let me open in here also.

Yes.

So as you can see that we are now Kubernetes dashboard, but it is requiring for the token.

For the token.

We have a token information.

Let me copy it from the.

Or we used to do quote.

Right click.

Me control C.

Okay.

We'll go back to our chrome.

Put the token.

Click the sign.

Okay.

Normally we should create an admin admin user credentials and request token for admin user, but Tucker

cover for us.

And as you can see that we are in the dashboard now.

See the dashboard and you can see the namespace.

We are using the default namespace in here and you can scroll the different information Namespaces notes.

We are using Docker desktop Node, Persistent volume storage class and Cronjobs.

These are the Kubernetes sources we are working mostly for definition replica sets.

As you can see, we have two pod definitions and also we have deployments and next deployment we can

click the deployment and see additional details.

Also you can see the.

Cycling by this dashboard.

And also you can click the edit button and see the Yaml files.

You can update in here if you are changing the replica set in here, the replicas will be updated.

So in the port definition it is the same.

If you are select a port definition, you can also see the log information or execute Interlochen for

example.

This is the log inside of the port.

And also if you go back to port, you can open an interactive terminal inside of the port.

So it is very useful when you are using the dashboard, very visualize and we can see that how we are

using the Kubernetes dashboard with in our local computer.

We are following the some of the steps, but it is not so hard.

So now I'm going to delete resources.

From the beginning of the application for the next section.

So let me go back to the Visual Studio code and let me say Ctrl C and exit the Quit proxy.

Let me refresh the window and I'm going to delete existing resources in order to be more clear for the

next section.

Let me see that kubectl get all and I have service definition and I have deployment definition.

I'm going to remove the deployment definition first.

Kubectl delete and give the file name.

Enginex deployment.

Yaml.

Okay.

Kubectl remove enginex service yaml.

Okay, let's check now.

Kubectl get all.

The ports are terminating status.

Let me check again.

Kubectl get all.

Yes.

As you can see, that all is clear.

So now we have finished to introduction of the Kubernetes.

In the next section we are going to create our own Yaml files for the big picture.


# 第8節：Deploying Microservices on Kubernetes
**Total: 17 lectures | 1 hour 24 minutes**

## 8-1 Introduction
**Duration: 1 minute**
In this section we are going to Deploy our Shopping Microservices on Kubernetes.

Let's check our big picture and see what we are going to build one by one, as you can see, that we

we have created docker images for our microservices.

Also we compose docker containers and tested them.

So now we are going to deploy.

these docker container images on kubernetes clusters.

We are going to deploy local kubernetes so for that reason we will use docker kubernetes cluster

in order to use kubernetes features in docker.

And It is the easyist way to run kubernetes on local environment.

We will develop kubenernetes manifest yamls files for Shopping Client - API and also mongoDb.

We will create deployment and service yamls and also configMap and secret definitions for storing

database related parameters.

So after we have learned the basics, we can start with the coding part.
## 8-2 Planning to Shopping Microservices yaml files
**Duration: 2 minutes**
In this video we are talking about how to plan our shopping microservice yaml files in order to deploy

in Kubernetes.

We have three microservices mongo database shopping API and shopping client microservices, so we need

to write these Yaml files into the Kubernetes deployments.

For example.

For MongoDB we are going to create deployment yaml file service definition and config map in order to

store database URLs.

Also, we will create a secret for storing database username and password for shopping API microservices.

We are going to create deployment Yaml file service Yaml file and of course we are going to inject environmental

environment variables in order to store database URL and connect with the Mongo database.

And lastly, we have a shopping client microservice.

We have also create a deployment Yaml file.

So this Yaml file and we will have a definition in the service yaml file which is the load balancer

for the external costs.

So we will call the shopping client from our local browser and shopping client will be reached.

The shopping API in order to get the products shopping API will be communicate with the Mongo database

and get the products from the database.

So this is the our plan of the manifest files.

So let's see how we can create these manifest files in the next videos.
## 8-3 Visual Studio Code Setup
**Duration: 5 minutes**

## 8-4 Create Mongo Db Deployment yaml File
**Duration: 5 minutes**
In this video we are going to create mongo database deployment yaml file.

So let's take an action.

Before we start, let me check mongo docker up page.

So please open the hub.docker.com and open the official mongo docker image and we are going to see environment

variables.

Let's scroll down.

And see that this is the one of the example yaml file.

So as you can see that there is two environment variables mongo database username and the password.

So we will use these environment variables for activate mongo database.

It is good to know these environment variables in here.

We can copy and paste and one for this.

Also you can copy all Yaml file, but this is for docker configuration.

We can see how we can how we are handling these username password parameters in the Docker in the Kubernetes

way.

So let's go back to the our Visual Studio code and under the Kubernetes folder I am going to add new

file which is the mongo dot Yaml.

I'm not specify the deployment or service because we are storing in the same place deployment and services

in one Yaml file.

It could be capable for the we can store one more definition in the one yaml file.

That's why I'm naming the on the mongo.

So inside of this mongo yaml file we are going to follow the same definitions.

So that's why I'm going to copy and paste.

Okay, let me explain.

First of all, we are defining the kind of resource this is deployment definition.

And the name will be the Mongo deployment.

We are defining the label MongoDB, and after that we will when we are creating the service definition,

we are also give the selector with the same name in order to communicate each other, to identify from

the Kubernetes.

And after the metadata definition, we have a specification.

For now I am setting this replicas one and selector matching labels is a MongoDB is the same thing and

under the template definition we have also metadata and spec because in here we are defining for the

pod definitions and we should verify the image name.

So as you can see that the image name is the Mongo is the same with the official image name and we have

exposing the port in the same port 27017 And of course I was mentioning about the environment variables.

We are going to store two environment variables for username and password.

Now I am not setting the values.

Let me.

Set a random list of.

We will get the values from the config map for username and password.

Environment variables.

It is good to create the secret definition, not the config that we should create a secret definition

on the Kubernetes for username and password.

But before that let me see the Visual Studio code error.

Once we installed the Yaml definition, it is very good to see this kind of errors.

For example, when we check this error definition, one more container do not don't have the resource

limits.

So we should define a resource definition in here without the resource definition.

It is not a good practice.

We should limiting the these definitions resource definitions by restricting this yaml file into this

yaml file.

So let me.

Copy and paste these resources.

Under the port definition.

I'm going to define resources.

So as you can see that the error is disappeared.

So basically, we are giving the restriction, the usage of the memory and usage of the CPU.

You can give the request and you can define the limits.

So without this information, Kubernetes cannot understand in which resource should be allocate.

So that's why it's a good practice and the Yaml giving this error.

So now we have defined two.

Deployment yaml file for the MongoDB.

But of course we should create a secret for the mongodb username and password.

In the next video, we are going to create secret definition on the Kubernetes.
## 8-5 Create Secret For Mongo Db Admin Root Username and Password
**Duration: 4 minutes**
In this video we are going to create secrets for Mongo database, admin, root username and password.

After that, we will use the secret definition from MongoDB deployment Yaml file in order to retrieve

sensitive data.

So let's take an action for database username and password.

We need to create secret, but the data should be in base64.

So in order to convert our data to base64, we are going to use any websites.

Let me open the website.

So for example, we would like to give the database username as a username.

So if you click the encode, you can get the base64 encoded one.

Copy this one.

Also our passport.

Will be the password information.

And this is the base64 encoded value.

So you can use this application, this web website for the converting the base64 because all secret

definitions should be in the Base64 format.

Okay.

Let me go back to the our view pseudo code.

So let me.

Under the Kubernetes folder.

Let me create a new Yaml file.

The Xaml file name is Mongo Secret.

Yaml.

Okay.

This is the same steps when we are creating a deployment or service yaml files.

Secret.

Secret is the one of the kind of the Kubernetes resources.

So we are following the same steps.

Let me copy and paste.

As you can see, the API version is V1 and the kind is this time the secret, not deployment or services.

And the other definitions is for the specific for the secret definition metadata definition.

We are giving the name and also the type is the opac and data we are going to store is the mongo root

username and this is the username Base64 encoded value and this is the password base64 encoded value

we are getting from the web application.

So we have defined the secret definition Kubernetes Yaml file.

Now we should create this secret before the deployment definition because we are going to give reference

to secret yaml file.

So that's why Kubernetes firstly know the secret Yaml file.

In order to do that I'm going to open a new terminal.

And.

In the folder location.

I'm going to locate the Kubernetes.

So now let me Kubectl.

Apply to file should be the mongo secret HTML.

Hit enter.

And say that kubectl get secret.

So as you can see that we have a mongo secret definition now.

And we apply this yaml file to Kubernetes, our local Kubernetes now.

So now our secret is ready.

We are going to use this secret from our mongo yaml deployment definition.

In the next video we are going to see how we can reach secret into the deployment yaml file.
## 8-6 Use K8s Secret Values in Mongo Deployment yaml file
**Duration: 3 minutes**
In this video, we are going to use Kubernetes secret values in mongodb deployment yaml file.

As you know that we didn't finish the mongo deployment yaml file.

Now we can able to set the mongo username and password with retrieving information from the secret Yaml

file.

Let's take and take an action.

So let me go to the our mongo yaml file and we are going to give reference once we define the value

of the username and password.

Or MongoDB deployment yaml file will be referenced for the secret.

For that purpose I'm going to copy and paste.

Let me paste this line of code.

Oh, of course I had to remove value.

Okay.

Value from sacred.

Kay and name.

Name and Kay.

Value also is the same way.

We are going to get the value from the secret and the password.

So let me remove and paste this one.

Let's save this file.

So as you can see that we have used value from and secret Kay ref in order to access secret data, Kubernetes

manage our data with these definitions.

It is very useful and now we are ready to create our mongo yaml file right for the deployment definitions.

For that purpose let me save the file and now write the kubectl.

Apply and this time file we are going to give mongo yaml file.

Thus hit enter and see deployment is created.

If you check the kubectl get pod with watch.

He has already created.

So.

Let me see the cube.

Ctl get all.

You can see that we have a deployment definition.

Also, deployment creates a replica set and the pod definition.

So let me come back from the secret cube kubectl get pod.

See that this pod is creating from the Kubernetes which is deployment yaml file.

And if we check the details.

Let me copy.

Code name and kubectl.

Describe port and give the port name.

Of course, the first character should be the Mongo.

Hit Enter and see the status event list of the history of the object code name.

Pulling the image as a mongo and successfully created the pod with the MongoDB.

So we will finally create mongo database pods with the username and password secret protection.

In the next video we are going to create Kubernetes service definition for Mongo database.



## 8-7 Create K8s Service Definitions for Mongo Db
**Duration: 4 minutes**
In this video, we are going to create Kubernetes service definition for Mongo database.

Let's take an action.

Before we start, we are going to create a service definition into the same Yaml file.

For that purpose let me.

For it.

The last character of the Yaml file and put two three dash, dash, dash, dash and hit enter.

Save the yaml file.

We are putting three days for document separation in Yaml file by this way.

Deployment and service will be the same.

Yaml file.

The three days means the separating the Yaml files.

This is the best practice.

Mostly it is using together in the same file.

So because the deployment and service is should be communicate each other.

That's why it is a good practice to using in the same file.

So selector and deployment labels should be the same because the connection between resources establishment

over the labels.

So as you can see that we have a label definition and also match labels.

So in the service definition, we are going to create a selector and give this label name.

By this way, Kubernetes, create a connection, understand that these files related to each other.

Okay, so let me create a service.

Copy and paste in here.

Let me see what happened.

Okay.

So this definition is similar.

As you can see that we only define the kind of the resource Kubernetes kind of the resource is a service.

Give the metadata, name Mongo service and specifications in here.

The first specification is the selector.

The MongoDB is the should be the same label name.

With the deployment, you can easily easily to see in the same document.

And in the port section we are going to expose the same port number two, in order to reach in our local

computer.

Right.

We should expose the same port number.

So this is target port provide to exposing the port number in the Kubernetes environment.

So this is the basic service definition.

Let me save this file.

In order to activate these services, we are going to kubectl apply command and give the file name.

File is the same mongo yaml.

It entered and see it's configured and service definition is created.

So after that, if you check the services Cube Ctl.

Get service.

You will see the Mongo service is created successfully and if you would like to see the end points for

that purpose, I am going to write the kubectl describe service and the service name.

Hit the enter and see that end points is in here.

We have one endpoint because one replica, one port for the Mongo database and exposing the 27017 port.

And if you want to verify this endpoint with the port number, kubectl get pod and we are going to copy

this port name.

And if we write the kubectl.

Gitpod.

And the port name.

The show and what it will include, the IP address and the IP address.

You can verify that the service and IP address the same.

So that means labels work in the Kubernetes and match the labels with the deployment and the service

definition in the Kubernetes.

So finally we finished to Mongo database deployment and service definition.

It is read for the ready for the connection.

So we will continue with the shopping API Kubernetes manifest file in the next video.
## 8-8 Build Shopping Docker Images , Tag and Push to Docker Hub
**Duration: 6 minutes**
In this video we are going to build shopping Docker image tag and push to Docker Hub.

As you know that in the last video we have finished MongoDB, Kubernetes, Yaml, definitions for MongoDB,

Kubernetes pull the official Monga Docker hub image but shopping image is not existing on the carpet.

For that reason, before writing the Kubernetes Yaml file for shopping projects, we should create the

commits and take them and push them to into the club.

By this way, Kubernetes retrieves images from the Docker hub.

So let's take an action.

Before we proceed as planned.

Project Kubernetes files.

We should add Docker image into the corrupt registry because Kubernetes pull from the registries.

So open the Visual Studio code and let me clear this window.

And in this time I'm changing the file location.

So I'm now in the root folder of the GitHub repository.

But now I'm going to locate the shopping folder because our docker compose files under this shopping

folder.

Let me go to the shopping folder.

This one.

For pink.

So as you can see that if we check that Docker compose file location is in here open folder.

So once I reach the docker compose.

Folder structure.

I'm going to run Docker compose command because in the last sections we have created Docker compose

and override file.

So in the same way I'm, I'm hitting the Docker compose and giving the file name Docker, compose and

Docker compose override file name and up and hit enter.

It will work the background and it will start for for the our docker images.

Creating images.

And as you can see that now I have a shopping database API and client.

So let's see the containers, the corpse.

So this is the running containers swapping client and the swapping API.

And let me test before proceed.

These are application.

Let's see that.

Our API is working fine.

Is our API project.

We had already run this project.

Okay.

Product data came successfully.

Also, let me check the shopping client application.

8001.

Yes.

Client application.

Also working fine.

So our docker environment is working fine.

So that means we can use this client end API image for the tagging and pushing the docker up.

So now I'm going to see the image.

But before that, I'm not going to run anymore.

The containers.

That's why I'm using Docker compose giving the file and down command.

By this way, these running containers are stopped and removed in my local.

Okay, but my image is still stored in here.

Okay.

As you can see, that Docker image comes in here.

We have shopping client and shopping API project and the tag is the latest one.

Because we are running with the manual coding manual writing Docker compose.

So it means the setting the tag name with the latest.

If you run with the Visual Studio run profile, the tag name will be the dev environment.

Then we need to change the latest one or we need to create with release mode.

Run with the release mode in the docker.

Compose in the Visual Studio 2019.

But now we have already defined shopping API and shopping client with the latest tech.

So we will take these images and push to the crop.

Let me take one by one and I am going to write darker take and give the image the 99.

And I'm going to say that.

Mementos and shopping client.

A pink client.

So I'm taking the this name because I have a username in the group and I'm going to create a repository

with the shopping client.

So by this way, this repository will identify from the Docker hub and I can push my image to the curb.

Just hit enter and take the image.

Oh.

Okay.

No such image, I think.

It can be.

Duplicate.

Let me put more information.

Okay.

Let me hit Enter.

Yes, as you can see, that we can able to take these images.

And at the same way, I am going to take another image, which is the shopping API.

And see the shopping API.

All this.

This one.

Let me copy also.

Come here.

Remove this tag name and put this one.

Okay, let me hit Enter.

And also this one ticked successfully.

So if we check the Docker images.

You can see our target image stored in here for pink light and shopping API.

So in the next video, we are going to push these latest images to the Docker hub.
## 8-9 Build Shopping Docker Images, Tag and Push to Docker Hub Part 2
**Duration: 7 minutes**
In this video.

We are going to continue our operations and push shopping image to the crop.

Let's take an action.

First of all, open the crop and create new to public repository.

So let me open the crop.

Come back to here.

Of course, we should sign on the log in the system.

Let me sign in.

And now I am going to create a new public repository.

I can remove this poster now.

We are not using anymore.

I'm going to go to settings and delete repository and give the name shopping app.

Elite.

Okay.

Let me create a new one.

And we are going to create two public repositories, one for the shopping API, and this will be the

public repository.

And let me hit the create button.

Okay, come back to repositories and create new repository and one for the shopping client application.

Right.

This is also a public one.

So basically we have created two public repositories for our two microservices.

We have take the image in our local environment.

Now it is time to push our image to do the crop.

So for that purpose, let me come back to the.

Or terminal.

Let me clean and we will start with the login docker.

So before fish, before pushing our image, let me write Docker login.

So it's authenticating.

Yes.

Login succeed after the login the system.

We are going to push our image to the docker up.

For that purpose we can use the Docker push command and put our repository name, Docker, push memento

slash shopping client.

So we are going to push this image because we have this image with Target.

So and also we have created this image name into the Docker hub with the public repository and login,

the Docker.

And now we are ready to push the docker, hit the enter and see what happened.

So as you can see that it is uploading our image to the cloud Docker hub repository.

We will follow the same step for the shopping API microservice image.

Let me wait for the client image.

It takes some time.

All these steps could be automated with the pipeline definitions, but we will see it in the next sections.

For now, we are doing these operations by manually and 111 by one we together.

But in the last sections we perform all these operations with automated.

Okay, let me for a while after finished pushing these images.

We will write the same thing with the shopping API.

The corpus.

Operation will be.

Perform for the shopping.

I'm waiting for the.

Finishing this upload operation.

Let me wait for a while.

After the pushing this image, we will verify that from the docker up.

Our images are uploaded to the crop or not.

Okay.

It is almost finished.

Let me wait for a while.

Sorry about waiting together, but I'm not waiting for the API project.

I will run one more command and.

Close the video.

Okay.

Let me see.

Yes.

The push operation finished is the same way.

Let me write the push and this time use the shopping API with your docker hub username.

Hit the enter.

And start for the uploading, pushing our image.

During this time, let me check for the shopping client.

The poster.

In the crop.

So as you can see that we have seen that our image in here with the latest tech and last, which is

a few seconds ago.

So as you can see that we can successfully push our image to the Docker hub.

And I'm not waiting for this shopping client operation.

Okay.

It is also very fast because the first time, most probably it is uploading the main items.

So we have successfully finished our pushing operations shopping client and shopping API microservices.

In the next video we are going to see how to use these images from the crop.
## 8-10 Clearing Docker Compose Containers
**Duration: 3 minutes**
In this video we are going to clear Docker, compose images and containers on our local computer.

Because from now on, we are not we are not going to work with Docker images because we pushed our image

to the crop.

So Kubernetes handle the rest of the operations.

So let's take an action.

Let me clear this window.

And if you not performing the Docker compose down, please write this command and clear the compose

image in your local computer.

Let me wait.

Okay.

I already created this image.

And after that, if we check the Docker PS.

Some operations still remaining.

The is a.

Let me check also.

So we can also remove these unnecessary.

Inmates, for example, Mongo.

We are going to use from the bulk as I saw that this Mongo already downloaded for the Kubernetes.

You can check the names if there is a on Kubernetes containers remaining.

So leave it as it is.

Don't remove these resources because we are running Kubernetes over the Docker, so we don't need to

remove anything.

So let me check the Docker image.

We have lots of image.

But for now I would like to remove these images in order to see that how Kubernetes download from the

Docker hub and use these images from the Docker hub for me.

For that reason I am following the docker RMI remove images.

And give the name of the image.

And of course, we should test if that means first, because we have two images for that purpose.

So shopping client image removed from my local.

And also I'm going to remove the shopping API.

Let me.

This one F first.

Okay, now let me check my emails.

So as you can see that I have no any shopping API or client image in my local computer.

So we are going to configure Kubernetes with the shopping API and client and we will see that Kubernetes

retrieve these images from the Docker hub.

So in the next video we are going to create start with creating Shopping API, Kubernetes deployment

and service Yaml files.
## 8-11 Create Shopping.API k8s Deployment and Service yaml File
**Duration: 7 minutes**
In this video we are going to create shopping API, Kubernetes deployment Yaml file.

So let's take an action.

Before we start, let me check our shopping images from the Docker shop, go to the crop and go to repositories

and verify that your shopping client and shopping API image in here and including the repositories.

There should be an image which we are pushed in the last videos.

So now after verifying this we are going to create our shopping api Yaml file.

For this reason, under the Kubernetes folder I'm going to create a new.

Finally, which is the shopping API that.

Yml file.

Let me new file and shopping api dot yaml.

Okay.

In this file we are going to write deployment and service in the same Yaml file.

So let me write deployment first.

I will.

It to service later.

Okay, let me close this one.

This is the hour.

Homing API.

Yaml file.

So let me see.

One by one we have a kind which is a deployment and we have a metadata and also specifications for metadata.

We are giving the name shopping API dash deployment and labels.

We are going to use shopping API in this specifications.

For now we are going to one put replicas and the labels is the shopping API under the template we have

again metadata and the specification labels giving again shopping API in the specification we have container

definitions.

So in the container container definitions, as you can see that we are giving the image name from the

docker up of the our username and our repository name is username in Docker up and these are the image

name and these are the tag name which we are pushed in the last video.

So the most important part is the giving the image name from the correct way we hit the cries.

Our application in our shopping microservice and push these image to the Docker hub.

This time in our local computer there is no any shopping API image.

We are referring this shopping image into Kubernetes.

Just go to the Docker hub and retrieve this image from the public repository.

Okay.

I put the image pull policy.

You can see the details when you highlight these image pull policy.

And also we are defining the port numbers.

And another important thing is the environment variables.

So let me check the R.

The could overwrite HTML file.

We can compare with that open docker compose.

Let me compose in here.

So as you can see that, let me come back to Shopping API.

We have some environment for this.

Set it as the same way in the environment variables.

We are pushing these environment variables with the name value pair in here.

This is the Docker compose writing way and this is the Kubernetes writing way.

So for injecting database.

Connection string.

In here we are using double.

Point in here.

And double underline score in here.

And also value is the mongo service mongo why we are giving the mongo service because we would like

to connect with the mongo database service resources.

So that's why I'm now giving the value mongo service.

But we will refactor after test this value.

And the another writing is the resource resource is the same.

We are limiting our resources for the Kubernetes, so we are separating Yaml files.

So now I'm going to create.

Service definition for the shopping API.

Microservices.

Let me copy and paste in here so I will explain one by one again.

So in the service definition, first of all, we put a client as a service.

Metadata is the name shopping, API service and specification.

That is type of the specifications for the Kubernetes.

For now, we are giving the nodeport.

We are giving the Nodeport because in order to test these application, we would like to see in our

local computer by Nodeport we can test our application with a given node, port, port number.

So we are providing the type as a node port and node port is a 31,000.

So this is the the arrange by the Kubernetes.

It should be start from the 31,000.

And it is I don't know what maybe 1000 arranging 1000 numbers.

We will start from scratch.

So by giving the node port, I'm providing that we will test these Kubernetes ports from my local computer.

For that temporarily during the process.

So Node port is mostly using for the development tests.

So for that purpose I am going to use node port.

Normally if you expose your application from the external costs, we can change this load balancer.

If not, it should be the cluster IP.

That means it is accessible only in the Kubernetes, but for development purpose.

I'm in test purpose.

We can set to use node port service type.

Okay.

As you can see that we are giving the selector as a shopping API, which is the same label name with

the deployment in order to make create relation from the Kubernetes.

So we are giving the port numbers, as you can see that the target port is 80, but the port was we

are forwarding to 8000 and we are going to test this from our local computer with the external call

with the 31,000 port number.

Okay.

In order to test our API, we set the service as a node port.

So now we can run our shopping api yaml file on the Kubernetes.

In the next video we are going to test this yaml file.
## 8-12 Testing Shopping.API yaml file on k8s
**Duration: 6 minutes**
In this video we are going to move connection string into the config map yaml file on Kubernetes.

So let's take an action.

So as you can see that now we are setting the volume information directly writing the connection string

in here.

But it is not too good using connection string into the Yaml file.

We should move this this information into the config map of the Kubernetes resources.

So let's take an action now.

Let me create a config map then.

For that purpose I am going to create new file under the Kubernetes folder and the name should be the

mongo config map yaml file because this will store mongo connection string information.

That's why I'm giving the name mongo dash config map dot yaml file.

So inside of this yaml file it is almost the same with the sacred definition.

Let me paste these quotes.

We are using the API version v1 and the client is the Kubernetes resources as a config map and metadata,

including the name and the data section.

We are giving the connection string and we are writing the Mongo database connection string in here.

So after that we should change the shopping Yaml file in order to retrieve these information from the

config map.

So open again shopping yaml file and we are going to remove this line of code and we will write this

here value from.

Okay.

So let me write this way.

Okay.

As you can see that we are putting the value from and under the value from.

We have a config map reference and the name is the mongo config map will be equal to config map name

and the key is the connection string which we are defining under the data section.

So this is the basic usage of the how you can set the config maps and how you can retrieve information

from config map into the deployment.yaml file another Yaml files.

So as you can see that we get the value from the config map like the secret in here.

Now let me save all Yaml files and we are going to run this config file.

So for that purpose I'm locating the kubernetes folder.

So first of all, we should define the secret and the config map information.

This should be created in the first place of the Kubernetes because for example, this shopping API

using the mongo config if not exists mongo config it will get error.

So that's why we should define the mongo config map first.

And also the secret definitions is the same way config maps and secret definitions should be defined

in the first place of the kubernetes.

Okay.

So that's why I'm going to kubectl apply command using and giving the fine name as a mongo config map

hit enter.

So our config map is created.

So let me check kubectl get cmd for config map.

We can use the keyword and you can find the mongo config map is comes in here.

Okay.

So now we are going to verify our codes.

We have created the mongo config map and in our shopping api Yaml file we have some changes about the

database settings and now we are getting the value from the config map.

That's why we should update this file also at the same way kubectl.

Apply file name shopping api.

Yaml file.

Hit enter.

And as you can see that this file is updated properly.

Service is not changed because the config map is in the deployment part.

Now it is ready for the test.

So let me check.

Kubectl get pods.

So our pod is running properly.

I'm going to check the shopping api is running properly or not.

So that's why.

Let me open the chrome section and.

I will use the localist.

1330 1000.

Okay.

Yes.

This is the ah, Nodeport which Kubernetes deployment.

When we perform the Kubernetes deployments, we create a nodeport in the service of the shopping API.

That's why we can reach this URL in our local computer.

Let me try to get products.

Okay.

As you can see that products comes successfully.

So the shopping API deployment, also working with the config maps, as you can see that we have finally

see our API and the Mongo project is running on Kubernetes cluster successfully.

In the next video we are going to focus on the shopping client Kubernetes Yaml file.
## 8-13 Move Connection String into Config Map yaml file on k8s
**Duration: 5 minutes**
In this video, we are going to create shopping client Kubernetes deployment and service symbols in

the shopping client yaml file.

Let's take an action.

Let me create new file under the Kubernetes folder.

So in here create new folder which is the shopping client dot yaml because we will store deployment

and service definition in the same yaml file.

That's why I have one yaml file for the shopping client and one yaml file for the shopping API project.

And also we have one yaml file for the mongo database.

Every Microsoft says one yaml file and includes at least two Kubernetes services like deployment and

the service part.

So before we start, let me check what we are going to define in the Yaml file and what we have done

for the Docker compose file for shopping client application.

In order to go to Docker compose file, let me opt open the docker compose overwrite yaml file and separate

the windows.

Let's close this for now.

Okay.

As you can see that in the docker compose overwrite yaml file.

We have some of the definitions for the shopping client application.

Right?

So by these definitions we should focus on the environment variables and we figured out that it is requiring

the shopping API URL definition.

You can check the docker compose overwrite file in order to see these environment variables.

So if we define these environment variables for shopping client application in the docker compose overwrite

file, we should also define these environment variables into the Kubernetes definition.

So when we write the deployment file, we should inject this parameter as an environment variable.

So let me show you.

And an overview of the shopping API URL.

We should inject the shopping API URL and we are going to use the value from and config map because

as you know that as you remember that these kind of informations should be stored in the config maps

in the Kubernetes.

So that's why it's good to start with creating config map for shopping api URL.

So let me remove this one and start with the creating config map for storing these environment variables.

For that purpose.

I'm going to create a new Kubernetes.

Item in here.

Shopping api dash config map.

Make sure that these all one file should be under the Kubernetes folder.

So inside of this, I'm going to define it the same way.

A new config map for storing this shopping API URL.

But as you know that the client is configmap and metadata is a shopping API.

The Configmap under the data we are storing the environment variables, we are giving the shopping a

shopping naming and look at the value of this data shopping API, this service.

So that means in here the car can understand shopping API with looking for the container name and Kubernetes

can understand the shopping API service with giving the service name.

So this is the difference from the orchestration orchestrating with the Docker compose or orchestrating

with the Kubernetes.

So for Kubernetes World, we should always communicate the deployment resources with the service names.

So that's why I'm giving the shopping API this service name.

So let me save this application.

Now we can close this docker compose overwrite HTML file because we don't have any other environment

variables.

For specific, let me close and open terminal.

And come to Kubernetes section.

So we are going to use the kubectl apply command in order to create these configmap into the Kubernetes.

This time we are using the shopping api dash configmap yaml file hit the enter.

And now we have created this config map.

Let's check the kubectl get config map.

As you can see that we have now two config maps, one for the mongo config map and the other one the

shopping api dash config map.

Okay.

We have successfully created the config maps.

In the next video we are going to use this config map into the shopping client yaml file.
## 8-14 Create Shopping.Client K8s Deployment and Service yaml File
**Duration: 5 minutes**
In this video we are going to create shopping client Kubernetes deployment and service yaml file.

Let's take an action.

First of all, let me focus on the Kubernetes folder and open the shopping client dot yaml file.

So let me write first.

I will explain.

After that.

First of all, we will.

Develop for the deployment yaml file and after that we are adding the.

Source Kubernetes sources.

So let me explain from scratch.

It is the same thing what we have done for the shopping API.

Yaml First of all, the first definition is the deployment and the second definition is the for the

service.

In the deployment we have metadata and specifications also under the template of the deployment we have

again metadata and specification for the pod definition.

The first thing is the deployment metadata, including the name, name, information and we have defined

the label in here in order to match with the service definitions, services, selector application shopping

client by this Kubernetes.

Understand there is a relation with these deployment and the service definition.

That's why when they are creating pods from these deployment, the pod IP address assigned from the

these service definition.

Okay.

And in the specification type, we have replicas we are using the one replicas, but you can assign

two or for three for scaling your application.

And we have selector definition under the template definition, we are going to use the again label

information and the specification of the template we are defining the our image name.

So as you can see that the name is the shopping client and the image name is the Docker hub image name.

What we have, we, we were pushed to our image in the last videos.

So Cubans retrieve this image from the public repository of the Docker hub, and we are assigning the

ports and we are injecting the environment variables.

These are the same environment variables with the Docker compose overwrite file.

You can compare with that.

The important event variable is the shopping API URL because this client application will be create

a connection communication with the API because consuming this API API method.

So that's why we are injecting the shopping API URL and the value comes from the shopping API.

Configmap.

What we have created in the last video.

Configmap You can see the data is in here shopping API URL and we are basically giving the shopping

API service name in here.

Now we have defined the service.

And the resource is the mandatory in the in this yaml file to define the restrictions.

So the last one is the service definition.

Service definition has metadata and the spec again and the name is the shopping client service.

We are using this service name in the config map and specification type.

This time we are using the load balancer in order to use external calls.

This client application because client application will be called from the external of the cluster Kubernetes

cluster.

That's why we can give type of the load balancer and the selector again, shopping client, make a relation

with the deployments and the pods.

And in the port section we are forwarding the Port 8001 for the client application.

Let me open the compose overwrite and you can compare with the docker compose overwrite.

As you can see that in the shopping client definition, we are forwarding these eight port eight 8001

in the same way in the Kubernetes we can forwarding the port this way Target port is 80, port is 81

and also we are defining node port because we would like to call from the external systems.

In this case the external system are local environment.

So that's why we are assigning the 30,000 node port with the load balancer.

Okay.

After this definition, save this Yaml file and open a terminal.

So locate the Kubernetes folders.

Just kubectl apply.

Give the file name shopping client yaml file.

Hit enter.

So it is created.

As you can see that kubectl get all.

You can see that the pods are created for the shopping client application deployment.

Replica set services and the container is creating.

So let me see now.

Kubectl close.

Kubectl get pod.

See that the client application also running.

Very good.

We have deployed the client application API and the MongoDB application in the Kubernetes cluster in

our local environment Docker cluster.

So let me test now in order to test this one, let me check Kubectl get service or you can say that

SVC.

So for the API application we are going to use node port and Node port is looking for the 13.

Let's see in here.

If you refresh, it is working fine.

And for the client application we are using the 30,000.

So let me see.

Hit enter and see what happened.

So as you can see that it is failed and there is no any exception in the browser.

And we can't see any exception in here because ports created successfully.

But somehow it is getting error in somewhere, right?

So in the next video we will see how to understand these kind of problems and troubleshooting on the

Kubernetes.
## 8-15 Create Shopping.Client K8s Deployment and Service yaml File Part 2
**Duration: 6 minutes**
In this video we are going to troubleshooting on Kubernetes for shopping client Kubernetes yaml file

definitions.

Let's take an action.

In the last video.

We get an exception when we are calling the client application from the browser.

First, let's run some commands for troubleshooting to identify the problem.

Kubectl Port.

Get the pod name.

This is the pod name.

Let me control Z and.

Control paste in here.

Kubectl port and this.

Oh, and white.

Cube.

Ctl get pods.

It should be get pod.

Get put.

And the poor name decor and white.

By this way, we can get extensive, extensive explanation in here.

So we have IP information, node information and the other information, but we didn't see any problem

in this command.

So let's check again.

Now we are using the kubectl describe command.

And describe pot, paste the pot name and hit enter.

So in here we are going to check the basic information about the pot.

And also I would like to check the events.

So the first is the successfully assigned pulling the image from the Docker up successfully pulled the

image, created the container and start the container.

So there is no problem when creating the pot in the Kubernetes side.

But somehow when we hit this port number for the client application, the browser not getting any HTML

page.

So let me see the logs for checking the logs kubectl logs and give the name of the pod, hit the enter

and see this.

These logs comes from the ASP.Net project, from the shopping client application.

Let me read this explanation.

Okay, that should be a problem, but not so clear.

So, uh, we have verified that the pod creation is successfully happened.

So the most probably the exception comes from the networking.

So networking means the service definitions.

Let's examine our service definitions again.

So let me see the service definition.

As you can see, that we have defined the load balancer and we have a shopping client for the shopping

client application.

And when we look at the shopping client application.

Somehow it is not using this way.

So if we check the service definitions.

As you can see that we give the type as a load balancer.

But Load Balancer is working about the external systems and creating any IP on your cluster.

But we are working in the local Docker environment and in the local Docker environment, creating for

us a Kubernetes cluster.

So for that purpose, the desktop, there is no any configuration about the load balancer because we

have on localhost for the external costs.

For that purpose we only can able to use node ports for the local Kubernetes when you are working with

the Docker Kubernetes cluster.

So problem was the Docker.

Since we worked on local computer, it can create a new IP to reach and the only option to expose the

ports with the node port definition.

But we will solve this problem when deploying to other cloud managed Kubernetes services.

But for now, let me change this load balancer to the node port.

This way we can access the port and the localhost.

Of the client application.

So after changing these node port, let's come back to create new terminal and.

Go to Kubernetes section.

Let me remove.

First, we should remove the our shopping client definition.

Remove and check.

It is removed for the pots.

Get bot.

It is still terminating.

Let's wait.

Okay.

Termination is success.

Now recreate the Kubernetes shopping client resources.

Kubectl apply file copying client yaml hit enter.

This time we are created this shopping client service as a node port so we are ensure that we can access

this port number in my local computer.

Let me check kubectl get pods dash or and white.

Okay.

My client pod also running so let me check kubectl get service and see that my client service is running

as a node port and exposing with the 30,000 port number.

So let's see.

Now refresh this page.

Okay, now let me try with the.

30,000.

And see what's happened.

Yes.

So finally, we can get meaningful exception, as you can see that the exception set that.

There is an exception while processing the request and this is the socket exception similar with the

API connection when we are running in docker compose.

So connection request shopping API service at port cannot be found.

Okay.

So we identified the problem.

The problem was our connection with the shopping client and the shopping API and we were referring to

shopping API service name only, but the port number 80 cannot find.

So let me see again our definitions.

So as you can see that in the shopping client application shopping API application, let me check the

shopping API service.

We are exposing the shopping API service with the port number 8000, not 80.

So this is the basic port forwarding operation in when we are working in the local environment, we

have only one IP that is the localhost.

So that's why we should forwarding the different port numbers.

And once we forward the 8000, we should also change the connection looking for this 8000.

So let me open the shopping client application, open client in the environment variables, we are setting

the shopping API URL in the config map of the shopping API config map.

So if we locate the shopping API config map and see the data shopping API URL, as you can see that

shopping API service we are looking only the service and default port number is the 80, but we have

forwarded this the 8000.

So that's why we should modify this shopping API URL with the 8000 in our local computer.

So now let me recreate or update this config map in order to update this config map.

We are using the kubectl apply and file name should be the shopping API config map because we updated

this data information and let me hit the enter and it is configured with the new shopping API URL.

But also I would like to delete shopping client email because it should be create pods from scratch

with getting the new value of the shopping api url.

Let me delete shopping client yaml file.

And check for the kubectl get pod.

Make sure that pod is deleted successfully.

And let me recreate from the beginning.

Swapping client yaml file this time our config map is with looking for the correct value.

Let me hit enter.

Okay.

It's created.

So kubectl get pod.

So my new shopping client Port, is running on my local computer.

Let me check my applications.

First of all, I would like to check again my API URL.

Okay.

It is working fine.

So let me refresh this page.

And this time, finally I can get the product successfully.

This shopping client microservice communicate with the shopping API microservice correctly in Kubernetes

cluster.

So we finally managed to deploy our microservices into the Kubernetes.

So let me see in the.

Dashboard, Kubernetes dashboard.

Also, I would like to show you.

Kubernetes Dashboard.

Let me close this one.

And in order to see in the dashboard we need to this token information.

And after that we have to create a proxy.

Hit.

Enter.

Now let me.

Run first or.

Dashboard.

This is the dashboard URL.

Hit enter.

We should join log into this dashboard with the token information for that purpose.

Let me copy this token information.

It's copy and paste into here and sign in.

So as you can see, that we have finally managed, deployed and deployed our microservices in the Kubernetes.

And this is the dashboard we have using the Docker default Kubernetes cluster and we are using the default

namespace.

If we check the pods, you can see that we have one pod for the Mongo database shopping API microservice

and shopping client microservice.

If you change the replica count into the deployment and update the Yaml file, it is going to scaling

with the naive pod numbers.

You can see in here, you can check with the dashboard and this is the replica sets and also we have

a config maps what we created.

Let me see shopping api, url shopping api URL.

As you can see the shopping service and 8000 port number.

So we have a secret sauce.

So this is our secret mongo secret and this is stored as a base 64 encoded values.

So basically we have done with the deployment local Kubernetes for our microservices and also we can

see our microservices in the Kubernetes sources, ports, replica sets, deployment and services in

the Kubernetes dashboard.

In the next section, we are going to deploy our shopping microservices into Azure Cloud Kubernetes

services.

But before that, we will clear our resources.



## 8-16 Troubleshooting on K8s For Shopping.Client K8s yaml File Definitions
**Duration: 12 minutes**
In this video, we are going to clear and create Kubernetes resources on your Docker cluster.

Once we have developed our Yaml files, it is very easy to remove and create from scratch all Kubernetes

resources.

This is more powerful when you run on the cloud.

In order to minimize usage of CPU disk resources.

So let's take an action of removing resources is very easy and similar with the kubectl apply command.

So let me close and start a new terminal.

So as you know that all our Kubernetes manifest configuration Yaml files including under the Kubernetes

folder.

Right.

So this time I'm not going to locate inside of the Kubernetes folder.

Instead of that I'm writing Kubectl delete, give the file name and giving the Kubernetes folder name.

By this way, Kubectl is looking for the Yaml files under this folder and remove all the resource about

all of the kubernetes Yaml files.

So the another way we can use that, if you locate the Kubernetes folder, we can say that kubectl.

Delete file and only just using the point dot.

That means the current folder.

Remove all the current folder.

Yaml files.

Let's hit enter and see what's happened.

As you can see that it is deleted all the Yaml files in here.

So by this way we can.

Check our kubectl get all.

All the resources.

So all resources gone.

We are totally free for our microservices.

It is just one command and very fast in Kubernetes.

This is very powerful feature when you are working in Kubernetes.

Also, we will deploy our microservices on the cloud.

Azure Kubernetes services.

At that time.

We also see that you can use this feature in order to optimize your CPU and resource usage.

So in this section we have finished the Kubernetes deployment in our local for our microservices so

we can manage to run all applications on local Kubernetes.

In the next section we are going to deploying our applications to the cloud.

Azure Kubernetes Services.

**Duration: 3 minutes**

# 第9節：Deploy Shopping Microservices into Cloud Azure Kubernetes Service with using ACR
**Total: 20 lectures | 2 hours 12 minutes**

## 9-1 Introduction
**Duration: 1 minute**
In this section we are going to Deploy Shopping Microservices into Cloud Azure Kubernetes Service (AKS)

with using Azure Container Registry (ACR).

Let's check our big picture and see what we are going to build one by one.

As you can see, that we have finished to development and deployments of our microservices on local environment

and also we tested them.

Both docker and kubernetes deployments were run in our local environment. So now it is time to

move Kubernetes to the cloud one which is Azure Kubernetes Service (AKS).

But before that we will talk about what is Azure Container Registry (ACR) and

Azure Kubernetes Service (AKS) and why we are using them.

So let's get some information about ACR and AKS.
## 9-2 Azure Container Registry (ACR)
**Duration: 2 minutes**
In this video we are going to talk about Azure Container Registry (ACR).

Azure Container Registry is a managed private Docker registry service, that is based on the open-source Docker Registry.

It Create and maintain Azure container registries in order to store and manage your private Docker container

Docker container images and related artifacts.

If you look at the slide, we have already finished our local development and dockerize all images.

Also we push the docker hub,

but this time we will start using ACR for pushing our docker images,

from Dockerhub to ACR private registry.

And ACR has organic communication with AKS,

so that's why we move our images ACR and when we deploy to AKS,

Our main target is deploy our microservices on Cloud kubernetes which is AKS.

AKS will get the images from ACR effectively.

You can use Azure container registries with your existing container development and deployment pipelines,

or use Azure Container Registry Tasks to build container images in Azure. It Build on demand,

or fully automate builds, with triggers such as source code commits and base image updates.

In this slide, you can find an example usage of ACR in a Devops pipeline,

we will see this topic in the next sections.
## 9-3 Azure Kubernetes Service (AKS)
**Duration: 2 minutes**
In this video we are going to talk about Azure Kubernetes Service (AKS) and the steps of deployment

on AKS for our Shopping microservices.

Let me start with AKS definitions. Azure Kubernetes Service (AKS) makes it simple to deploy a managed

Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes

by offloading much of that responsibility to Azure.

As a managed Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you.

The Kubernetes masters are managed by Azure.

You only manage and maintain the agent nodes. As a managed Kubernetes service,

AKS has a free plan

Also, I am using the free plan for all azure deployments and operations.

You can create an AKS cluster in the Azure portal, with the Azure CLI, or template driven deployment options

such as Resource Manager templates and Terraform.

We will use the Azure CLI for creating AKS cluster.

When you deploy an AKS cluster, the Kubernetes master and all nodes are deployed and configured for you.

Additional features such as advanced networking, Azure Active Directory integration, and monitoring can also be

configured during the deployment process.

If you look at the slide, In the left side, what we have done so far, we dockerize our images and

deploy on local Kubernetes. In the right side is our target cloud deployment items

whichs are ACR and AKS.

We will push image to ACR, and deploy our current kubernetes configurations

into the AKS with pull images from ACR.


## 9-4 Steps to the AKS Deployment
**Duration: 3 minutes**
In this video we are going to talk about Steps to the AKS Deployment.

These are the main steps what we are going to follow one by one. In the first steps we have already done

for dockerize our images and deploy on local kubernetes.

SO now our target cloud deployment items whichs are ACR and AKS.

We will push image to ACR, and deploy our current kubernetes configurations into

the AKS with pull images from ACR.

If we expand the details of these steps, we can see this image. As you can see that

there are several steps to deploy AKS cluster, 1- Build your docker images

2- Tag these images according to ACR login name

3- Create your ACR in to Azure subscription and protect with admin user and

get the loginname , username and password. 4- Connect to ACR and push your images

to ACR

5- Create your AKS service into Azure subscription,

it will take some time but we will create an AKS with using our free azure subscription

free plan.

6- Build communication with AKS and ACR

7- Update your k8s manifest files as per AKS deployment, for example

we will change image names docker hub to acr loginname address.

8- Run your k8s config files into AKS cluster.

And the last one, we are going to create load balancer service and test our application from cloud AKS deployment.

So as you can see that we have lots of steps that we have to do, but no worries, we will do all steps

steps together and we will verify that our application is running on AKS.

In this slide you can see one of the advance real-world example. In the advance senarios, we can combine

ci/cd azure pipelines for pushing images to ACR and deploying to AKS.

I'm not going to give some other details of this picture, but we will see the details on the Azure Devops section.
## 9-5 Prepare Shopping Microservices for Azure Kubernetes Service (AKS)
**Duration: 11 minutes**
In this video, we are going to prepare shopping microservices for Azure Kubernetes services.

We will prepare multi container shopping microservices for using in AKs.

Existing development tools such as Docker Compose are used to locally building and testing an image

for your local computer.

So we will start building images from the beginning with Docker Compose.

Let me recap what we are going to do.

We are going to create a container image from the Docker compose file.

Test multi container application in our local Docker environment.

Once completed, we are going to push this image to the ECR.

In the next videos.

So let's take an action.

First of all, make sure that your docker is running properly in your local computer.

Let me see that.

The CPUs and.

Okay.

Our docker is running in my local computer and when started the docker check the Kubernetes config and

not required for now, but make sure that you are in the Kubernetes for you.

Let me open the Docker settings file so you can check that Kubernetes and this should be enabled Kubernetes

for your local computer.

And you can also verify from the this bar of the application Docker desktop.

So another way to check in which Kubernetes configuration we are using to prepare.

Let me use a kubectl commands kubectl and there is a config command if you get context.

So basically it is returning to all of the configurations in your computer and the current one will

be indicated in like in this way putting the star.

So basically we are now actively using Kubernetes cluster in the Docker desktop, which name is Docker

desktop.

But if you are already using different Kubernetes cluster, for example, or another private Kubernetes

cluster, these will be listed in here.

So in our case, I was created a AKs cluster before.

That's why I can see the cluster in my computer, but most probably you will see only one record in

here.

So in order to see the current context, there is another kubectl config command.

Kubectl config.

Current context.

So this will be returned to the current context of your Kubernetes cluster.

We are going to use the desktop now.

So this is the very good knowledge about when you are using the Kubernetes clusters because in real

world mostly you are using the different clusters with different environments.

So that's why it's good to check in which cluster you are going to work right now.

So in order to switch these context, there is also another command you can use, for example kubectl

config and you set that.

Let me kubectl config and use context command.

If you provide the name of the context in here, this will be switched to the context.

But we are not going to switch for now.

You understand my point?

For example, if my AKs cluster will be available now, you can copy this cluster and paste in here

and hit enter.

This will be changed to the context and you will continue these operations.

Kubectl get apply and so on and the new context.

Okay, so we I'm showing to you how to get the context and how to switch the context.

Another way that you can see the context.

In here, right click to Kubernetes.

I'm in Docker, right click Docker and see the Kubernetes, and you can see the context in here.

Basically, if you click to another context, you will see switch context by using the Docker environment.

Okay, So also you can see the you can change by clicking the image and settings and change context.

And another way to using the kubectl config commands.

Okay.

After we have learned the context objects, let's start the development of our manifest files for Azure

Kubernetes services.

In order to do that, let me close all tabs.

And you remember in the last sections we have developed the Kubernetes folder.

This time I'm going to create a new folder which name is AKs?

So create a case folder at the same pit with the Kubernetes.

And I'm going to start with adding the commands.

Text file because we are going to use lots of Azure.

Create or Azure Resources commands.

We will follow the commands with in this text box.

But of course this folder is creating for the manifest files.

I mean is the same way with the Kubernetes folder.

We are going to write manifest file with the Azure Kubernetes services.

So most probably you are asking that why we are using the again, the folder when we have an existing

Kubernetes folder.

So mostly you are right, we are using the same concepts in the folder, but when you are shifting to

the cloud, deploying your Microsoft microservices to cloud Kubernetes, that will be a new features

and good benefits we can provide.

So that's why I would like to separate these folder structure and now.

We are going to focus on the creating images.

So think about that.

We have no local Kubernetes.

We are directly deploying our microservices to the cloud.

Azure, Kubernetes services.

And we are now going to start from the beginning.

So the first step should be the create container image, right?

Right.

So make sure that your docker is running on your computer.

So let me focus on the Docker compose or write file.

Again, this will be the shopping.

So I am the same folder structure with the Docker compose files.

So that's why now.

I'm going to write Docker, Docker compose command, but let me check the Docker PS Now.

Okay.

This is only related for the Kubernetes related images.

Look at PSA.

Yes, I have on Kubernetes related images in my computer now.

I haven't any container regarding to my shopping application, so it is good to start from the beginning.

So once I am located the Docker compose.

Yaml file folder structure.

So I'm running the docker compose, giving the file name Docker compose and giving the file name Docker

compose or writing file app up and the Dash D command, which means run from the background, hit,

enter and compose Docker from scratch again.

Okay.

As you can see, that it is building our image from the beginning with looking to Docker files.

Yes, we have created the shopping DB shopping API and shopping client containers.

Now you can see the details with checking Docker PS.

As you can see that these Docker image containers running on my local computer now.

So let me also check the Docker images.

My image created with the latest text.

Very good.

So my image and containers are running on my computer and ready for the usage.

For that purpose, let me test my application in my local computer in order to verify our application.

So.

Let me first start with the testing shopping API project.

Try and execute.

Okay.

The products came successfully from the Mongo database.

And also we have a client application which consumes this API project in 8001 port.

Okay.

Also, my products came successfully from the client application.

So we have tested our Docker containers and now let me stop my containers on local computer.

Not required to run the containers because we have successfully created our images with the latest tech.

I'm now using the Docker compose down command in order to stop containers with giving the specific Docker

file names in here.

Hit enter.

And it will stop containers.

Yes.

Stopping is finished.

So now we are finished our local operations.

We created the commits.

We tested the containers on our local computer.

And in the next video, we are going to take these images in order to push other container registry.

And we will see how we are going to push Azure container registry with local images.
## 9-6 Deploy and use Azure Container Registry
**Duration: 15 minutes**

Azure Container Registry is a private registry for container images.

A private container registry lets you securely build and deploy your applications and custom code.

In this video, we are going to deploy an Azure container registry instance and push a container image

into it.

So we are going to create an Azure container registry instance.

Take a container image for Azure Container Registry.

Upload the image to the Azure and view images in the ECR Registry.

After these operations, these ECR Instance is integrated with a Kubernetes cluster in Azure Kubernetes

Services and an application is deployed from the image.

So let's take an action.

First of all, make sure that your local docker is running and your image is created properly.

But before we begin, we should understand how to create resources on Azure so you can use Portal or

you can use the Azure commands.

So we have two options.

When you are using the Azure commands, using the Azure cloud shell, using the bash environment or

using the Azure CLI.

So we are going to follow the local installation of the Azure CLI command line tool and the version

should be greater than 2.1.

So let me show you how you are going to install Azure CLI.

Open the browser and using the this URL, I will also share this URL into the this video resources.

So basically this is the official Microsoft documentation installing the Azure CLI command line tool

and you will select for the target operating system and download to Azure CLI.

I have already downloaded for the Windows one and after installation completed.

We will check the Azure.

Command is working on your local computer for that purpose.

Check Azure CLI version.

Let me run as the version command.

Hit enter.

And see, as Ursula is working correctly, properly in my local computer and my version is 2.16.

Okay, Very good.

And now we will use these Azure commands.

Azure CLI for creating Azure Container Instance.

So let me start from the create window.

Now we can create the Azure resources, but first we need to login to Azure.

So in order to log in as Azure, we should use the Azure Login Command because we are going to create

resource in the Azure.

So that's why we should look into Azure.

These are the steps is, is the same way when you are logging the portal Azure dashboard and creating

any resources so you can choose the portal in the browser or you can follow the commands with Azure

CLI, I would like to use mostly Azure commands, but also I will show some of the results into the

dashboard from the Azure portal.

Let's hit, Enter and login to Azure Server.

So it is returning to direct to us.

Azure environment.

And once you click the your outlook address, you will see the you have logged into Microsoft Azure.

Okay.

So after logging successfully, you can see the this information.

So we have successfully logged into Azure now.

So now we can create a resource group in order to create an Azure container registry.

First of all, we need a resource group.

So an Azure resource group is a logical container into which Azure Resources are deployed and managed.

So create a resource group with a Z group.

Create command.

Location could be West Europe for us, but you can choose your nearest data center, for example, East

us.

So let me create a resource group for that purpose.

We are going to use a Z group, create command.

A group create and I'm giving the name of the resource group.

My resource group and location is the West Group.

This is the nearest data center for me.

You can change for your location.

Okay, let's hit the enter and.

We are going to create first Azure resources with using AZ group Create Command.

So hit enter.

And wait for a while.

So as you can see that the response came in a Json format and our resource group created successfully.

You can also verify this resource group from the Azure portal.

So in order to do that, log in to Azure Portal with your subscription and click the resource groups.

Also, my resource group came in here.

Let me refresh.

It is take a little bit of time to refreshing the portal because it is working background and we have

created the resource group.

But it will some takes time to see in the dashboard.

But let me continue with few commands.

We have created the resource group.

You can check resource group from the portal and when we are created the resource group, that means

the other Azure Resources will be included inside of the our newly created resource group, my resource

group.

So we will adding the Azure Container Registry into this resource group.

Okay.

So we are going to create now a Azure container registry.

In order to create an Azure container registry instance, we are using the Azure API, create command

and provide our own registry name.

The container registry name must be unique within the Azure.

The name should be global unique.

So let me show you how we can create.

We will use the Azure ACR.

This time.

We are going to create a Azure container registry in order to push our image.

So let me provide to the stage resource group.

And this is my.

Resource group, which we have created recently.

My resource group and the name is the ECR should be the global global unique name shopping ECR.

And of course we are using the basic.

Consumption model.

That's why we are writing these as Q.

So this is the minimum commands, what when we are defining the Azure container registry?

Basically, we are going to create Azure container Registry into my resource group and that's why we

provide the resource group information.

And the name should be the global unique in the Azure environment.

So I'm providing the shopping ACR but not sure about Global Unicorn.

Not, not.

We will check after running this command.

So let me hit enter and see what's happened.

Okay.

I am writing wrongly.

My resource group, I think.

Let me.

See that?

Is let me run this command again, I think.

I have some problem with.

Writing Resource Group is a create resource group.

My resource group shopping easier and giving the basic.

Okay.

We forgot the create command.

Sorry about that.

As a create command, just hit enter and see what's happened.

Again, it takes some time and we are awaiting response from the other side.

Yes.

In a Json format we have a succeed response.

So our API created into the my resource group successfully.

So we have also finished the create operation.

You can also verify this operation into the Azure Resource Group.

So as you can see that in the dashboard now, we can see the my resource group under the resource group,

we can verify that we are created ECR correctly, successfully.

This is our Azure Container Registry and it is ready for the pushing the containers.

So you can verify with examine this information in the dashboard.

It is very good, but I would like to choose using the commands one by one.

First, we have created the My resource group and after that we have created the ACR into the resource

group as a create command with giving these specific parameters.

So now I am going to enable admin account.

So we have created the ACR.

But before we are pushing the image, I would like to enable admin account.

Admin account each each container registry, including an admin user account which is disabled disabled

by default, but you can enable the admin user and manage its credentials in the Azure portal or by

using the Azure CLI or Azure Tools.

So the admin URL has full permission to the registry.

The admin account is currently required for some scenarios to deploying an image from the container

registry to certain Azure resources.

For example, the admin account is needed when you deploy a container image in the portal from a registry

directly to Azure Container Store or Azure web app for containers.

Also, what was our scenario?

We are going to deploy our images into the Azure Kubernetes services.

So that's why it is good to using the admin account in the Azure Container Registry.

I would like to enable admin account because when we are deploying microservice to Kubernetes and Kubernetes,

try to pull images from the Azure Container Registry, it can be required ECR admin user account.

So it is best practice to protect ECR with enabling admin account.

Also, it is very easy to enable.

You can enable from the portal or you can run as ECR enable commands.

I will follow the command window.

Let me.

Right to command.

Now.

We are going to use a ACR update command this time.

Let me make this the clear window.

As you know, that we have made a secret.

But this time we are going to update this ACR with the enabling admin account.

So this is also can be performed in the dashboard.

Let me show the dashboard.

Open the shopping app here.

And if you go to the access keys.

Let me show you in here.

In some cases, we have some delay in the dashboard.

So I think these enable admin account option should be in the access case.

We are going to enable these admin account because of the deploying the Kubernetes Azure Kubernetes

services.

When pulling the images from the Azure Container Registry, it is looking for the admin user account.

So let me open from the scratch.

And go to the ACR now.

Click to access case.

Now it is canned response.

Yes.

Let me open the access case.

You can enable admin user by clicking this option.

But I'm going to use.

Azure commands as Azure update.

The name of the ACR is Shopping API and I'm going to enable this option hit Enter.

Okay.

After a few minutes later, finally I can get the success response.

So we have updated the ACR with enabling admin account.

You can see the response in here.

So also let me verify with the dashboard.

Once you come to the shopping ACR and settings access case, you can see the admin user is enabled and

we saw the username and the password information in here.

So we have successfully created Azure Container Registry and enabled admin users.

In the next video we are going to take our image and push our image to the Azure.
## 9-7 Deploy and use Azure Container Registry Part 2
**Duration: 11 minutes**

講座縮圖
0:08 / 10:54
In this video, we are going to deploy an Azure Container registry instance and push a container image

into the ECR.

So we are going to take a container image for ECR.

Upload the image to the ECR and vive image in your registry.

After these operations, this ECR instance will be integrated with a Kubernetes cluster in Azure.

Kubernetes services and applications will be deployed from the image.

Let's take an action.

So first of all.

We should log in to the Azure Container Registry in order to use the Azure instance.

You must first log into the ECR.

So we will use the Azure ECR Logging Command and provide the unique name, which is the given name for

the container registry in the previous step.

So let me write as ECR login.

And the name was the.

Topping ECR global unique name for the Azure it to enter and login the system.

Okay.

Yes.

Login successfully.

So after logging the system we should take a container image.

Now we have a we have the container registry, Azure container registry, and we can push our image

to the Azure Container Registry.

But first we need to take our image.

So in order to do that, let me check my image Docker image.

And see we have created shopping client and shopping API images.

So in the last sections we had pushing this image to the docker up, but we have a new register now.

We have going, we are going to register these images to the Azure Container Registry.

In order to use shopping container images.

With Azure Container Registry, the image should be tagged with the login server address of our Azure.

This tag is used for routing when pushing the container image to the to the Azure Container Registry.

So let me get the login server address when we are starting our tagging operation.

So in order to get login name or.

Container registry.

We have this command information.

This is the long command as a list command.

Basically, it is listing some of the information we provide, the query, we are creating the login

name and we would like to see in a table format.

So let's hit enter.

No worries.

I'm copying these commands into the command text and you can use these commands by looking the command

text step by step.

So when I hit enter, as you can see that we saw that login server name shopping dot Azure azure.io.

So this is very important when we are taking the image because the server name should be this name.

And the tag name of course should be start with this login name.

You can also query these information from the dashboard.

Let me check in here.

As you can see, that login server is.

Located in here in the access case, you can see all of the information.

We will come back in here.

But it is good to learn easy commands with Azure CLI.

So that's why I'm using this command.

So after we can get these information, login server name, now we can able to take our images with

this Azure Container Registry name.

So in order to do that, we are using the Docker take command.

Let me write command in here.

As you can see that the take and I am going to take shopping API latest version.

This is our local image and the name should be the.

My name.

And also this will be the.

Image name.

So in our case, more Asian name is.

Shopping.

ECR dot Azure, cr.io/and providing the image name.

And I'm giving the tag name as a version one because we keep continue to our developments so we can

upgrade our versions version to version three and so on.

So that's why I would like to put the version, version one as a version one.

So let me take our shopping API, hit the enter.

See it is ticked.

Let me see the image again.

So as you can see that our image take it as a version one and the correct name of our ECR shopping ECR.

So now I am going to take another image, which is the client microservices, shopping client, microservice,

latest image, and this will be tagging with the shopping ECR, Dot Azure, CR and so on.

It's the same way we set as a version one.

So it is also ticked Docker image.

Let me see.

Now I have to take images with according to Azure Container Registry.

Now it is ready for the pushing.

We set the version one because after update our code we will change the version two.

So now it is time to push our image to the Azure Container Registry.

In order to do that, it is almost same operation with the Docker Push command.

So basically.

I'm using the Docker push and the name of my image as you know that we have already logged to Azure

and also we have already log log in to Azure Container Registry.

So now it is time to push our image with using the Docker push command.

Just hit the enter and start with the pushing operation.

Okay, it is take some time again.

The first image, including all of the objects with the ASP.Net Core And.net core base image.

So that's why it takes some time.

And also after that, we are going to push.

Or shopping client image to the Azure Container Registry.

Let's wait for a while and I will push to the shopping client after that.

After this, I am going to take my another image, which is a shopping client version one.

We are using the same Docker push command, but this time we are choosing the shopping Shopping client

image just hit enter and push this image also into the other container registry.

What we have created in the last videos.

So this time it is a little bit faster because in the first time it is downloaded the base image.

So that's all okay.

We have successfully pushed our two image to microservice image into the other container registry now.

So let me listing our images under the other container registry.

In order to do that, I'm going to use as ECR repository list for that purpose.

Let me write this command as ECR repository list.

The name is the Shopping ECR and we would like to see as the output it entered.

And this will return the results.

Yes.

As you can see that under the our Azure container registry, we have two images, one for the shopping

API, one for the shopping client image.

So if we look at the tag names of these images, we have also different command for that purpose.

I'm writing this as a repository show text, give the name and give the repository.

Would you like to see tag name and hit enter is the same output.

And we will see that version one.

For now, we have only one tag name, version one.

But when we are working on our microservices, when we are updating our code, this will be the version

two and so on.

Also, we are going to optimize all these versions and with continuous integration, continuous delivery

with the next section.

But now we have finished to create the ECR enabled the admin account and also we are pushing our local

image to the ECR successfully.

By the way, I am adding all these commands into the command text under the folder.

Just see the command text.

So as you can see in here, I am writing all these commands step by step according to video flow.

You can also use these commands on the GitHub repository.

Don't worry about that.

I am always located, always saving all these commands into the commands text.

Okay.

So as you can see that we have finished to creating Azure Container Registry and pushing our shopping

image to the ECR.

In the next step, we will create AKs Azure, Kubernetes Services and provide to get our image from

the Azure Container Registry.
## 9-8 Deploy an Azure Kubernetes Service (AKS) cluster
**Duration: 5 minutes**
In this video, we are going to deploy an Azure Kubernetes services cluster for our shopping microservices.

Kubernetes provides a distributed platform for containerized applications with Azure Kubernetes Services,

you can quickly create a production ready Kubernetes cluster.

So we are going to deploy a Kubernetes cluster that can authenticate to an Azure container registry,

install the Kubernetes CLI for kubectl usage and configure Kubectl to connect to your Cloud Azure Kubernetes

Services cluster.

After these operations, our shopping application is deployed to the cluster scale and updated.

Let's take an action.

Before we began.

In the previous videos, shipping container image was created and uploaded to the Azure Container Registry

instance.

So you can check to the dashboard and make sure that your shopping image.

Or stored in your container registry.

So we have created the shopping app here.

And if you go to the repositories under the services option, you should see the shopping API and shopping

client images.

If you click once, you can see the version one tag name.

Okay.

We have verified that.

So now we are going to create an Azure Kubernetes cluster using Ajax, create commands.

We will create a cluster named my cluster in the resource group of my resource group.

This resource group was created in the previous videos in the West Europe region.

For now, we don't specify a region because once we configured our root resource group location, this

will apply the child resources.

So for the cluster, it is also created in the West Europe region.

We don't need to specify that.

In order to allow our AKs cluster to interact with our other Azure resources and Azure Active Directory

service principle is automatically created.

Since you didn't specify one.

This service principal is granted the right for the pudding image from the Azure Container Registry.

And you?

We created these container registry in the last videos, so we didn't specify any other options.

When we are creating the Azure Kubernetes services automatically, Azure Active Directory Service Principal

is created.

So by this way, the Azure resources can be connected to each other, just giving some configurations.

We only need to specify a name with the ECR Command in order to give access to pulling images from the

AKs.

So now let me focus on the Azure Create Command.

I'm going to copy and paste in here so you can see that we start in the common approach as a CS create.

We are going to create Azure Kubernetes Services and please use this resource group.

This is our root umbrella resource group and we are giving the name as a my AKs cluster node count is

one.

It is the configuration of the Kubernetes and generate a SSH key.

And the important part is the attach ECR which name is shopping ECR.

We are adding this command in order to interact with shopping easier, which basically this Kubernetes

will be will be getting image from the shopping ECR.

That's why it is good to use attaching this ECR If you don't specify in the first start.

You can also update this ECR with attaching this information.

So hit enter.

And see what's happened.

It will take for a while.

As you can see it, start running.

After a few minutes, the command completes and return as a Json format.

Information about the cluster.

So I'm not going to wait for that Json success Json for now.

You can also run this command and you will get the response.

So in the next video we are continuing our operations with.

Installing the Kubernetes CLI and so on.

So for now.

I'm not waiting for a while.

Let's see in the next video.
## 9-9 Deploy an Azure Kubernetes Service (AKS) cluster Part 2
**Duration: 7 minutes**
In this video.

We are going to continue to deploying an Azure Kubernetes service cluster for our shopping microservices.

We have created a case now on the Azure cloud.

These are the logging formations.

You can see the Json response is successfully created.

Azure Kubernetes Services.

So for now, we should connect to Azure Cloud services from our local computer.

Let's take an action in order to connect to the Kubernetes cluster from your local computer.

We use the Kubectl commands, the Kubernetes command line client.

We should install kubectl CLI locally using for the Azure Instance command.

So that means you have a local computer and in the cloud you have a Kubernetes cluster, managed Kubernetes

cluster, Azure, Kubernetes cluster.

You would like to see this Kubernetes CLI and would like to run Kubectl commands.

For that purpose, we need to install Azure Arc CLI.

Let me write as AKs install CLI.

It is mandatory step in order to connect to the cloud access.

Just hit enter.

This will downloading the client for us.

And it is downloaded.

A connection with the Azure Kubernetes services and the local.

Kubernetes Kubectl commands.

We have some several steps.

The first step is the installing the Kubernetes CLI.

Yes, it is installed.

It takes.

For first not waiting so much.

So after that we should connect to the cluster using kubectl.

In order to configure kubectl to connect your Kubernetes cluster, we are going to use as AKs get credentials

command.

We are going to get credentials for the cluster, which name is my AKs cluster in the my resource group.

So let me.

Right to command the second command.

This is the AKs get credentials and providing the resource group and providing to my AKs cluster name.

Okay, so let's hit enter and this will merge our context object.

I have already created before, so that's why it's asking for me the overriding.

Yes I would like to overwrite and also I would like to overwrite object name.

So this is merged my AKs cluster.

So now you can also see the kubectl context.

We have checked before kubectl.

Get contexts.

Context.

Okay, I forgot the command, but you got the idea.

Let me see in the here.

When you check your context, you should see the my cluster for now.

After merging this command, this is the creating a context in your local computer as a get credentials.

For now we are ready to use our application.

Now we can verify our connection to your cluster.

Run your kubectl, get nodes, commands, and see what happened.

Let me see.

Kubectl.

Get old.

So as you can see that we have returning this data, but now I'm using most probably the current yes,

Docker desktop, Kubernetes.

Let me change my context to do my cluster.

We can also change this cluster name with using the kubectl context commands.

We have seen before, but let me get the notes now.

So I as you can see, that I got the exception, but most probably you are not getting the exception.

I got this exception.

Uh.

If you have no any exception, you can continue the next video.

But let me fix this error.

I will show how to fix this error if you are facing this kind of problem.

Before this project, I had another corporate Kubernetes connection, so I have to check environment

variables.

We have defined the environment variables to reach kubectl config.

As you can see that I arrange my kubeconfig with the cube location in the admin config folder, but

it is generated for me, also generated for me the connection into the my user location.

So I'm going to check first my user location and see how we can get these information under the cube

folder.

We have a config information.

Right click and edit in notepad plus plus.

You can see that this is the basic configuration information able to connect cloud server.

So this is my Kubernetes location in admin config.

Let me edit also this one.

So I'm going to copy this.

Configuration and paste into my location and save my config.

So now my kubectl commands should be looking for the latest ECS cluster.

I'm again using the kubectl get nodes and this time I can able to get nodes.

As you can see that the name comes from the AKs node pool and giving to some unique identifier.

So finally we have able to connect Cloud Azure, Kubernetes services from our local computer.

So that means we can deploy some of the deployment service and the other Kubernetes resources.

As you can see that we have created the Azure Kubernetes services and connect to the Kubernetes from

our local computer with using Kubectl commands.
## 9-10 Run applications in Azure Kubernetes Service (AKS)
**Duration: 5 minutes**
In this video, we are going to see how to run applications in Azure Kubernetes services.

We have created the Azure Kubernetes services and now it is ready to run our shopping microservices

on Cloud Azure, Kubernetes services.

We are going to build and deploy shopping applications and services into Azure Kubernetes cluster.

But before that, we need to update existing Kubernetes manifest Yaml files.

So let me recap.

We are going to update the Kubernetes manifest files according to a case.

Run these applications into AKs and test the applications on the AKs.

After these tasks, we will scale out and update to shopping microservices on the AKs.

So let's take an action.

So before we begin, make sure that your shopping applications was packaged into the container image.

And this image was uploaded to Azure Container Registry and Azure Kubernetes cluster was created successfully.

So in the last video we had an Azure Container registry instance stores the container image for the

shopping application.

In order to deploy these applications, we must update the image name in the Kubernetes manifest file

to include the Azure Container Registry login server name.

So in order to get our login server name, we have these commands.

Maybe you remember as ACR list and we are requesting the login server name or swapping ACR server name

is this one.

And we are going to update our Kubernetes manifest file with this login server name.

So you remember that when Kubernetes retrieving the containers from the Docker hub, there was a Docker

up image address.

So this time we had pushed our image to the Azure Container Registry.

So this address should be replaced with the shopping ECR address name login server name.

So let me show you which files we should change.

Let me open the Kubernetes folder and see that for example, shopping api Yaml file.

If you close this one, you can see that when we are configuring Shopping API deployment resources,

Kubernetes manifest file in the container section, we have defined the image name.

Image name was the Docker hub name, but this time we are going to change the server as a login name.

And also this will be the shopping API and the tag name will be the version one.

You can also verify this tag name from the.

Portal.

As you can see, that shopping API has version one tag name.

So this is what we have changed in the folder.

The image name.

Let me continue our operation.

In order to listing repositories into the ACR with the command.

You can also write as ACR repository list with giving the ACR name.

This will return to us shopping API and shopping light image.

We can also tag names with the command command.

Azure Command.

We will see already in the last videos.

So this is the our login server name and this will store our shopping image.

So we need to update Kubernetes image name into the.

A folder with this login name and image names.

Of course we should also provide the.

Take name.

So we have pushed this other image.

So we need to replace existing image files into the ECR ones.

But when Azure Kubernetes Services retrieves images from the ECR, it is requiring a secret to login

ECR and pull the image from the ECR.

So before we change the existing Kubernetes Yaml files, we have to create image pull secret for container

in order to pull image from the ECS.

So in the next video we are going to create image pull secret for the ECR container.
## 9-11 Create Image Pull Secret for ACR Container
**Duration: 5 minutes**
In this video, we are going to create image pull secret for ECR container.

This step required for pulling images from the ECR.

Kubernetes uses an image pull secret to store information needed to authenticate to your registry.

In order to create the pull secret for Azure Container Registry, you provide the service, principal

ID password and register URL.

When a case retrieves images from the ECR, it requires ECR, secret login, the ECR and pull image

from the ECR.

So is it somebody?

We need to create secrets for pulling containers from other Kubernetes services.

Let's take an action.

So we are going to create an image pull secret with kubectl, create secret command.

Let me give you this command Kubectl create secret.

We are giving the namespace and Docker server.

Name the username and the password.

This will be the image Pull Secret and Kubernetes Azure.

Kubernetes Services will be pulling our image with using this image pull secret.

But before that, let's collect the parameter information in our.

Azure subscription.

So first of all, the importing information is ah, of course the login name login server name.

This is the shopping ECR that associate IO and we will provide these information when we are creating

the image pull secret.

And the another informations will be collect from the dashboard because we need also password information.

So that's why.

Let me go to the portal.

Go to the Axis case.

And in his.

I'm going to use this password.

Uh, let me change password in my commands.

I'm sharing all of the things in the portal because once when you are watching this course, I'm going

to cancel this subscription at all.

So that's why I'm not avoiding hiding some information.

Okay, So let me come back in here and.

See my command.

My command is kubectl create secret.

And we are going to create secrets for the Docker registry.

And this name of the secret will be the secret.

And the parameter first parameter is a Docker server.

The server name is my login server name.

Right.

This is the login server name.

The second one is the username.

Username is a shopping ECR.

You can compare from the dashboard ECR information and the password.

I just copy and paste in here and the email is my subscription.

Okay, let's hit enter.

So a secret is created in the kubectl command.

As you can see that the kubectl is also looking to Azure Kubernetes services.

So this secret is created in the cloud.

Azure, Kubernetes Services.

You can check secret information in Kubectl, get Secret and see it is created in here for the Docker

registry.

So in order to use this image pull secret, we should add the configuration.

Once you have created the image pull secret, you can use it.

Create Kubernetes pods and deployments.

So we need to provide them provide the name of the secret under the image pull secret section in the

deployment files.

So in our example.

Let me go in here.

Let me copy this file and.

Create a random file for now.

So in our example, we are going to give the specification name.

I mean, when we are defining our image in here, we should also provide the image pull secret with

giving the name of the image pull secret.

Okay, so we will update the existing Kubernetes file for Azure Kubernetes Services in the next video.
## 9-12 Edit K8s Manifest Yaml Files For Deploying AKS
**Duration: 10 minutes**
In this video we are going to edit existing Kubernetes manifest Yaml files for deploying Azure Kubernetes

services.

We have created other Kubernetes services now on Azure Cloud Environment and we have created pull secret

from the pulling image from Azure Container Registry.

So for now, we modify our Kubernetes manifest files according to cloud access requirements.

Let's take an action for access deployment.

We have several updates on Kubernetes manifest files.

In order to do that, I am going to copy all Yaml files into the Kubernetes and paste into the access

folder.

For that purpose.

Let me.

Right click.

Copy.

All these files and into in here right click and paste.

So as you can see that I am copy all Yaml files into the folder.

You can think about that.

We have no any local Kubernetes and we are directly deploying microservices into the Azure Kubernetes

service on the cloud.

So of course we should write from scratch all of the things, but we already seen in the last sections.

So now we are going to perform updates according to Azure Kubernetes service.

We are going to update our Yaml files as per the deploying the Azure Kubernetes services.

So we can edit manifest files.

First of all, we should replace the existing Yaml file image names to the ECR ones.

For that purpose I'm going to open the.

Let me.

Open this file and I'm going to open shopping api.

Yaml file.

Let me close terminal.

And in here shopping API.

Yml file.

Go to the container specifications and the image name should be the ECR one.

Let me replace the new one.

Okay.

This is a surname.

ACR.

Azure Container Registry.

This is the image name.

This is the tag name.

This is already pushed in our ACR.

So that's why.

Azure Kubernetes Services will be connect and pull this image.

But when pulling this image, it is also required the image pull secret information.

So for that purpose, I am going to add.

Image pool secrets.

In order to.

Pool images.

From the.

Asia.

And the name of the image pull secret is a secret.

You can also verify that kubectl get secret.

We have created in the last video.

So we thought this information cannot be pulling these image from the ECR image pull secrets which is

the name is the secret.

Okay.

We have to changes in the shopping api deployment yaml file.

So is the same changes will be applying shopping client yaml file, open the shopping client yaml file

and go to the container image name and replace this one for the ECR ECR one shopping client and version

one.

And of course we should also define image pull secret as the same way.

Let me come back in here and paste this pull secret with a secret name.

So as you can see that we have replaced image names as it the ECR names.

Also, we added the image pull secret configurations into the deployment yaml container configurations

in order to allow to pulling image from the AKs.

So let's go next next configuration.

We are going to update the services.

So now so far we only updating the image names and the image pull secrets.

So after the getting the image to AKs.

Of course there should be a service definition, right?

We had performed the service definitions for the local Kubernetes environments, but now we are on the

cloud.

Kubernetes services manage Kubernetes services, so we should also update the service definitions.

Go to the shopping Yaml again and open the service definition.

So for service definition upgrade operations, go to the shopping API and locate the service definition.

As you can see that we have defined the service definition when we are deploying the local Kubernetes

environment.

For that purpose we are selecting the type as a node port in order to reach from the local environment.

But this time we are deploying on the Kubernetes AKs Cloud cloud environment.

For that purpose, we don't need to open API project from the external calls, so that's why I'm removing

deleting this type information.

The default type will be the cluster IP and also as you can see, that there is no port.

Let me delete this one.

So the second change in the service definition will be the port forwarding.

We have forwarding the ports in order to reach from the local environment with local list server name.

But this time we are deploying the cloud Azure environment.

So that means we don't need to forwarding any port.

All ports will be get the new IP address from the cloud.

So that's why I'm removing also this target port and give the port number as a default one.

That means this is a summarized service definition for the shopping API.

We don't specify any kind type of the specification with node port or load balancer because this will

be the inside of the cluster and communicate only the ports into the cluster.

Don't communicate with the external cause.

Okay.

So we are going to deploy a case on live flight so we don't need the port forwarding every port.

Take new IP from the cloud Azure, Kubernetes services.

We did it before in order to see in our localhost.

That's why I have removed the target port and set the port to 80.

Okay, let's save shopping API and the next operation will be the update the shopping API configmap

address.

Once we change this port number, we should also update the config map URL.

So go to the shopping api config map.

If you open the shopping API config map, you can see that shopping API URL is shopping API dash service

8000.

This time we don't need to specify port number because this will be enough for us.

We are not port forwarding anymore.

So this is the make.

This seems more clear way in our Kubernetes definition because once we are in the cloud, the IP address

can be generate one more IP address.

It is more clear now.

These parameters use from the client project shopping API URL parameter using from the shopping client

microservice project so we don't need to set any ports anymore.

That's why I'm cleaning this port number also.

So let's go to the client change shopping client chains.

Go to the shopping client and locate the service definition again.

This time we can able to set load balancer.

Maybe you remember we have to make port in order to reach from the local Kubernetes with Docker Docker

cluster.

So now when we are deploying the Azure Kubernetes services, we can basically set the load balancer.

So we have changed the type load balancer because this will be the client application can be open,

the consumes from the external costs.

And also we have.

Northport Port information.

We don't need to know port information.

Once you change the type of the load balancer, this will be generated from the Kubernetes automatically.

This will create an IP address for us for external calls and I'm not going to forwarding ports again.

So that's why I'm changing the target port and leave as port as a default one.

So basically we set the load balancer first.

So a case creates a ingress IP for us and not need to port forwarding again.

And I think the refactorings of the manifest files are finished.

As you can see that we have created the case manifest file definitions.

Next video.

We are going to run these commands on the AKs.



## 9-13 Run K8s Manifest Yaml Files For Deploying AKS
**Duration: 9 minutes**
In this video, we are going to run configured Kubernetes manifest files for deploying Azure Kubernetes

services.

Is you that we have configured Kubernetes manifest files.

Now it is time to run Kube Yaml files on Azure Kubernetes services and deploy our application on the

case.

So let's take an action now.

First of all, let me open a new terminal.

And we should check the our current context to verify in which context we are working for the Kubernetes.

We should have kubectl config, get contexts command hit enter and see that.

Now I have only one.

Kubernetes configuration, which is my AKs cluster.

And this is representing to me my Azure Azure, Kubernetes services.

Okay.

So you can also check the get current context, my AKs cluster.

You can also see which these contexts, if you have one more cluster information, kubectl config use

context commands.

Okay.

We have made sure that we are in the correct context so now we can create all other resources.

Before that, let me check.

Kubectl get all and see what including resources in my case.

As you can see that I have only cluster IP info for the Kubernetes and I have no any resources defined

on the AKs.

So let's perform the deployment now in order to deploy our microservice application to the AKs, we

should run the Kubectl apply command.

Kubectl apply G The file name and the file name should be the case.

This will be handled all of the operation.

This will be run all of the Yaml files and configured for the AKs Kubernetes cluster and we expect that

our microservices will be run on the AKs.

Just hit the enter and see what's happened.

So we have one error definition on the shopping API.

Yml file.

And I think.

We have right wrongly image pull secret and also shopping client.

Let's check shopping API and shopping client version.

Image pool secrets.

We are giving providing the image pull secret and put to two empty places.

Image pool secrets.

Okay, I understand the problem.

The problem is the definition of the image.

Pull secret.

We are providing image, pull secret information under the container section.

But these will be the same step.

I mean, same level should be the containers.

So let me remove this one.

And in the container section, let me come back to container section.

Paste in here.

So image pull secret.

Also one of the specification that should be defined under the specification, not under the image name.

So that's why it was.

Getting error.

So let me see again.

So this will be the.

That will empty place.

So the array start with the name.

Okay, so this is important.

When you are configuring your Yaml files, you should know in which place it will be stored your configurations.

So we are the is the same level in the containers.

We have a image pull secret specification for the deployment and we provide in here.

It's the same way.

Let me come back to client application and copy this line of code.

Come in here.

And.

Come back to the container section.

Yes.

Now it is in the container level.

Okay, we have fixed our image.

Pull secret error with moving image, pull secret information to the same level with the containers

under the specifications both API and the client Yaml files.

After you save these Yaml files we can continue our operations.

Open the command line and again I am going to kubectl apply with giving the folder.

Hit the enter.

So this time shopping API and shopping client created successfully.

After that, we can check all the resources on the ECS with the kubectl, get all command hit enter.

And see all resources created from the Azure Kubernetes services.

We have port definitions and service definitions.

Deployments and replica sets.

So as you can see that all Kubernetes resources created successfully on the AKs.

So let me check the service definition.

In order to see the correct IP address.

Git svc kubectl get SVC.

For the service information.

Okay.

You should wait for the external IP.

These IP will be generated from the Azure Kubernetes services when we are giving the service type as

a load balancer, in my case the external IP already created.

So I'm just going to hit this IP from my Chrome browser.

So let me.

Open your browser and hit these IP.

The okay.

It was very fast.

I think when we are first attempt it can be take some time, but as you can see that Azure Kubernetes

service open an API to me for my client application and retrieve products from consuming API container

in the port and this API project looking for the Mongo database.

So these product information comes from the Mongo database on the Cloud Azure Kubernetes services.

So our application on live production of you can search from the mobile browser or any browser.

You can see that your application on live.

So now we can scale and deploy new changes with zero downtime on the production.

But before that, if you have got any exception, I would like to show how you can troubleshooting.

Your execution.

For example, if you get any problem when you are creating pots, think about that.

There is a source.

If you see that the status is error, image, pull or image pull back of this kind of error, you should

check the pot definition with the kubectl.

Describe command.

For example.

Let me check client project.

Hit the enter and you should follow the this event list.

Most probably you will see the error information into the this line of explanations.

So according to this information you can fix your config variables.

So basically our application is run on the production on the cluster.

We can successfully deploy our microservices on the case with using the ECR because our images are pushed

into the Azure Container Registry.

And of course we have used the ECR secret information image pull secret information when retrieving

these images from the ECR.

It is using the Kubernetes secret information.

Now it is time to scale and deploy new changes with zero downtime on the production.

In the next videos, we are going to see these details.
## 9-14 Scale Shopping applications in Azure Kubernetes Service (AKS)
**Duration: 11 minutes**
In this video, we are going to scale shopping applications in Azure Kubernetes services.

We have a working Kubernetes cluster in AKs and we deployed the shopping microservices.

So now we are going to scale out the pods in the shopping applications and tripod auto scaling.

So we are going to scale the Kubernetes nodes.

We are going to manually scale Kubernetes pods and run your application.

And we are going to configure auto scaling pods that can be run application on the front end.

So let's take an action before proceed to scaling operation.

I would like to remember that what we have done in the last video, we have deployed our microservices

in the Azure Kubernetes services.

So we have checked the kubectl get pods and see the our microservices is running on Kubernetes cluster

on cloud managed Kubernetes.

So I would like to show you services, get services.

So as you remember that in the shopping client microservice, we had changed the API service definition

shopping client for the.

I'm sorry.

I'm in the folder in the shopping client.

We change the type of the service as a load balancer.

Once we set the load balancer, that means we are open an IP for the external calls.

Kubernetes handle these operation for us and generate an external IP for us, and that IP can be accessible

from outside of the world.

So that means it will be published for the outside and the shopping API and other microservices will

be the cluster IP.

This is the default type of the service.

That means they have only communication inside of the cluster microservices and the pods.

So what is mean load balancer?

How we can identify the external IP.

Once we set the kubectl get service, you can see that in here.

Cluster IP.

That means these are the connecting each other with these IP address.

But for the load balancer type, there is an external IP and this external IP generating from the AKs

and it comes a little bit after that, this assigning.

So once it is assigned, you can use these IP address external IP address in order to reach our client

application.

We already seen in the last video, but in this video I would like to also show you how we can examine

these resources in the Azure dashboard.

So let's connect the Portal.azure.com and you can go to the resource group.

And you can see the our resource group, this is our resource group under the our main resource group,

we have two items, one for the Azure Container Registry, which including our dropping microservice

Docker images and one for the our Azure Kubernetes services manage Kubernetes cluster.

Once I click the Kubernetes section, I have some basic information in the overview.

You can see that.

But I would like to show you the these Kubernetes resources section.

So as you can see that it is like a Kubernetes dashboard.

You can see the namespace workloads is usually with the Kubernetes dashboard.

This is the Kubernetes resources section and the important part for US service and increase records.

So please check this type information.

It is the same as the CLI.

These are the cluster IP address.

These three item is our microservice service resources.

So the last item is the shopping client.

As you can see that there is an external IP for us and if you open the services you can get more details

about the service.

And if you click the external IP, you can directly reach the production IP address, which is the which

is the releasing for the production IP.

So, uh, I would like to show you how you can examine and see.

Kubernetes resources from the Azure managed Kubernetes services.

For example, configurations also stored in here.

We have a config map definitions.

Also we have a security information.

You can see our security information.

Of course, there is more item for in here because this comes from the Manage Kubernetes Systems Azure.

Kubernetes services handle for us some base operations.

So that's why you can see more than more items.

It is very normal.

So now we can go ahead our main.

Operation, which is the auto scaling, scaling, shopping, microservices.

So first of all, I'm going to manually scale our pods.

So for that purpose, let me see again my pods.

Kubectl get pods.

As you can see that for now we have only one pod item as per our microservices.

So we have three microservices and we have three pods, three containers running on our Kubernetes.

But we can be scaling these pod numbers with setting the replica count, right?

We already seen that items before that.

Let me check the kubectl get deployments.

See our deployment name is in here.

So now I would like to scale my client application up to three, a number.

I mean, I would like to create three pod number in here.

For that purpose, we can use the kubectl scale command.

Let me write the kubectl scale command.

This is the imperative way to setting items, the access replicas, and I'm giving the three as a replica

count.

And of course, we should provide the.

Deployment name.

Let me.

Copy.

I'm sorry.

Copy and paste in here.

So basically, as I said that in here, we would like to create three posts for this deployment item.

Let's hit enter and see what happened.

Doesn't find because we should provide the long.

Name, I guess.

Let me cube Ctl get all and.

Yes, we should provide the deployment apps for the name of the coping client because.

This is the exact name for the apps.

Let me.

Let me clear and right.

These replicas with adding the long name.

Okay, now let's hit enter and see what's happened.

Kubectl get pods.

Let me adding the watch.

So now I can sketch again normally, which means that we can watch the creating items and the rating

plots one by one.

But now it is already created three port number Kubernetes for us.

We can exit with the Ctrl C, So the idea is that we can perform the scale operation with only running

the scale command and giving the replicas count and also providing the name of the shopping client application.

So as you can see that now we have three shopping client port number and it is ready for the ready for

the consuming calls from the outside.

Okay.

So the another option, the scaling is using the Yaml files.

So let me come back to scale one again and.

Let's.

So as you can see that now, I can catch the watch operation.

Now it is terminating the unnecessary two items and it is terminating and the last status.

We will see that only three port numbers left in our application.

So.

As you can see that now we have three port number is running.

Another as the another port numbers are, uh, terminated.

So let's check the latest status.

Latest status is running three port numbers.

Okay.

So now another version is the scaling using the Yaml files.

For that purpose I am going to open the shopping client yaml file.

And go to the deployment under the container section.

We should know under the specification of the deployment, we have replicas specification.

So basically you can change to this time.

Let me make it two, two, two.

And after that, of course we should apply this change with using the kubectl apply command and also

I can give the whole folder.

It will compare the Yaml files and find the updated one and perform the update operation.

Let's hit enter and see what's happened.

So as you can see that unchanged unchanged and the shopping client yaml file configured.

So if we check the kubectl get port we would like to see the two item for the client application.

And yes, as you can see that we have two shopping client ports generating from the Yaml file.

This is the second approach to scaling our pod pods on Kubernetes.

Okay.

As you can see that we have scaled our applications both manually and using Yaml files, but we can

go one more step which is the auto scaling with the Azure Kubernetes services.
## 9-15 Autoscale Shopping Pods in Azure Kubernetes Service (AKS)
**Duration: 8 minutes**
In this video, we are going to auto scale shopping pods in Azure Kubernetes services.

Kubernetes supports horizontal pod auto scaling to adjust the number of pods in a deployment depending

on the CPU utilization.

The Matrix server is used to provide resource utilization to Kubernetes and is automatically deployed

in clusters.

Let's take an action.

First, let me check the version of the my AKs.

For that purpose, I'm going to use this command.

Let me copy paste.

The AKs version should be.

Greater than 1.1.

So yes, our version is 1.18.

So it is ready for the auto scaling.

It is good for the auto scaling feature.

So now in order to use auto scaling feature, of course we should create a new Yaml file.

Right.

For that purpose I am going to create new Yaml file under the folder.

Let's create new file which name is shopping auto scale dot yaml.

So in this file we are going to provide these definitions of the auto scale.

So let me copy and paste and I will explain after that.

Let me copy and paste.

Okay.

Let me close these empty spaces.

Yes.

This is the basic similar to creating the Kubernetes resources, but this time kind is the horizontal

pod autoscaler.

This is for the auto scaling feature, and I'm defining one for the shopping API and one for the shopping

client microservices.

And we are giving the shopping API and shopping client HPA.

So under the specification we have maximum replicas and minimum replicas.

That means we would like to minimum three pods for the shopping API microservice and maximum the number

could be the ten because it is auto scaling if the request comes, thousands of thousands.

So at that time it is auto scaling the 3 to 5, six and up to ten.

And this is the basic scale target reference reference configurations.

We are referencing to our deployment Yaml file, as you can see that kind deployment and this is our

deployment Yaml file and we are set that target CPU usage percentage is 50 because we have two microservices

on the case now.

So we are separating by two 5050 and at the same way we are defining the shopping client HPA and referencing

the shopping client deployment and giving the minimum replicas three maximum replicas ten.

So in this shopping autoscale yaml file we have defined min max replicas with target CPU utilization.

This performs auto scale command to auto scale the number of the pods in the shopping deployments.

So in order to run auto scale, we are going to use kubectl apply command.

The auto scaler defined in the Yaml file.

So we should we should define or create this yaml file into the Kubernetes environment.

For that purpose.

Let's open a terminal.

And again, again we are using the kubectl apply file folder.

So let's hit enter and see.

This will be the created.

These two items is created Autoscaler for chopping API and client.

So in order to check that we have a another kubectl command kubectl get HPA.

By this way we can get the autoscale definitions.

So you can see that our resources created successfully.

We also see the kubectl get HPA command.

So let's check the pods.

Now we are waiting expecting minimum three pod number for the API and the client application for that

purpose.

Let me check.

Kubectl get pods.

So in here, as you can see that we have three port for API microservice, three port for the client

microservice.

But somehow, as you can see, that one item is still pending.

The status is still pending.

That means it can't be created yet.

So if you saw if you see this kind of.

Still stuck in the panic status.

Check the details of the pot in order to check the status.

What we have done Cube Ctl describe port and give the port number.

Let me copy this port number.

Paste in here.

Hit, enter and we will see the error.

Yes.

As you can see that the error appears in the events section and the basically the problem is the insufficient

CPU.

So we have defined the resources.

Maybe you remember, let me check in the API project.

So basically, actually we should modify these resources because this is the request we are using the

250 CPU.

It is the quarter CPU that means so it's the same way.

It is a quarter CPU for the shopping client.

In the next video we are going to decrease these values.

But for now, let me.

Decrease the replica counts.

So in order to decrease replica count, please let me open the.

Or autoscaling again.

So it is good to two replicas for the shopping API.

Let's save and let's try now.

We should have two port for the API, three port for the our shopping client application.

In order to reflect these changes I'm going to kubectl apply the whole folder, hit the enter.

And see what is the configured.

So let's see now what's the latest source of the.

Kubectl get pods.

Okay.

It is not responding now.

Let me wait for a while.

We have changed the minimum replicas as a two for the shopping API and leave three in the shopping client.

So we have configured not responding now.

Resmi kubectl get pod.

Okay.

Now it is I think still 3 or 3 and.

Scoping client scoping API.

I think it should be tried to delete.

Let me kubectl apply again.

Maybe we didn't save.

Let me open and save to and kubectl apply.

Okay.

It is configured.

So after that kubectl get put.

Yes, it is creating from scratch, I think.

Okay.

As you can see that in the last tortoise we saw that we have two pots from the shopping API microservice

and three pots for the shopping client, shopping client microservice.

All of them running properly.

So we have auto scaling by creating auto scale resources in the Kubernetes.

As you can see that we have auto scaled our applications with kubernetes yaml definitions.
## 9-16 Update Shopping Microservices With Zero-Downtime Deployment on Live AKS
**Duration: 9 minutes**
In this video, we are going to update shopping microservices with zero downtime deployment on live

ECS.

After chopping microservices has been deployed in Azure Kubernetes services.

It can be updated by specifying any container image or image version.

So we will update a part of our application.

Update will be state.

So that's why only that portion of deployment will be updated with zero downtime.

This update enables the application to keep running during the process deployment process.

It also provides a rollback mechanism if a deployment failure occurs.

So let me explain what we are going to do.

First, we are going to update the front end shopping client microservices index page code.

We will update shopping client MVC application index page.

After that, we are going to create an updated container image.

We are going to create updated Docker container image.

And pushed to Docker Container image to Azure Container Registry.

After that, we are going to deploy updated container image to the Azure Kubernetes services.

So let's take an action of.

So now let me start with the updating the front end shopping client microservices index page code.

For that purpose, let me open the our application.

So close this one.

Go to the application shopping client application and.

Under the weaves home.

Index page.

This is the first page, so I'm going to say that products.

Now we have not single container.

Updated.

Version two from deployment.

So now we have Neve caption Neve header for our index page.

As you know that our version one application is running on a case now and we can also check these application.

So let's check in here.

So this application is running on the live, not on the production.

So now we are going to change this text, right, with the updated version to.

Okay, so we are going to deploy version two of the client application with no effect to end user.

So now we're down to our application.

Kubernetes apply rollout deployment, kill the pods one by one and create the new version application

pod one by one.

So one of the user once open this page, it will continue with the version one, but the new users,

when you refresh the page, we are going to see that you it will see the new version of the product

shopping Client Microservice Version two Container Image.

Okay.

So in the second step, we are going to update the container image.

As you can see that we have updated the index page in our local source controller index page, but this

will be reflected our image Docker image.

In order to do that, we are going to update container image for first of all, we need to remove existing

image in our local computer in order to create fresh image from our shopping client application.

For that purpose, let me create a new terminal and write here.

Docker image.

Let me check the our local image.

So as you can see that the shopping client image stored in here, this is the actual shopping client

and this is the target one for the Azure Container Registry.

So now I'm going to remove these shopping client images and create from the secrets for refreshing and

getting these information, latest index page information.

So let me come back in here in order to remove that docker RMI and use that.

This image.

And we should force the date these images in order to remove both of them.

Okay.

It is unticked and delete.

So if we check the image now we should not see the.

Hoping client.

So this is the second one.

Okay.

This time we have no shopping client, right?

Okay.

So we have deleted our images.

Now we need to create new image from new code with composing Docker files.

Again, In order to do that, I am going to locate the Docker compose location.

So this is the shopping client shopping API.

And so I should go to the shopping folder because this folder structure, including the Docker compose

level.

So by this way, I'm going to run the Docker compose command from scratch in order to recreate shopping

client microservices from the Docker file.

So let me run this command.

This command is the compose command.

I'm going to copy and paste in here.

The compose giving the file name and giving the overwrite file name up and the admin dash working background.

Okay.

Hit enter.

This time.

This will be look at Docker compose file and see that shopping client not exists.

So follow the shopping client Docker file items and creating shopping client image Docker image from

the beginning.

With this updated version to index page.

Okay.

All these created.

So let me check now the car images.

As you can see, that shopping client is created with the latest tech.

And if I check, the Docker is running containers.

I saw that shopping client API and mongo microservices running on my local computer.

Let's have a quick test.

Our application is working on local or not.

For that purpose I'm using.

This swagger file.

It is working fine.

Let me get the products from the Mongo database.

Okay.

And we have also 8001.

Our client application?

Yes.

As you can see that the new index, the image created with the naming updated version two from deployment.

But if you refresh the Azure Kubernetes services, as you can see, this is the old container image

running on the production.

So we are going to make a new deployment with the version two, and this will be replaced with the shopping

client index page and the end user will not be affect with the zero downtime deployment.

This is the more powerful feature of the Kubernetes.

So we will follow these steps.

But now we have verified that captions change in our local Docker container image version so we can

go back to the our terminal and now we can stop our Docker image because we don't need to running containers.

Now let's hit enter.

Since we created a new Docker image, we can close the Docker containers, right?

So this will stop the commands.

As you can see that we have no any microservice running on my local computer now, but if I checked

the commits, we have created the shopping client microservices with the new version of the index page.

As you can see, that we have created a new version of our shopping client microservice image.

In the next video, we will take and push the image to the Azure Container Registry.
## 9-17 Tag and Push the New Version of Shopping.Client Image to ACR
**Duration: 5 minutes**
In this video, we are going to take and push the new version of shopping client image to the Azure

Container Registry.

Let's take an action before we take and push the image.

We should identify the tag name in order to correctly use the updated image.

We should take the shopping client image with the login server name of the Azure Container Registry.

So get the login server name with ACR list command.

Lets me clear these commands.

And we have a Azure List command hit the enter.

You will see the popping Azure Azure address, which is the Azure Container login name address.

You can also see this name from the dashboard of the Portal Azure Portal.

We will see this information in the last videos.

So now we should check the newly created image.

Let me check the Docker images.

In the last video we have created naive shopping client image.

So this is our new image.

So we are going to take this image which is login name of the Azure Container Registry.

So let me close.

We will use the critic to do in order to take the image.

So we should replace the login name and we are going to use ACR login server name for the public register

host name and update the image name with version two this time.

So let me show the liquor take command and I will explain after that.

So as you know that we have already take our image before.

But now I would like to show you these version two number.

So the normal the take operation will be handled giving the our local image shopping client with the

latest tech.

And this is the login name of the our Azure container registry.

This is the image name.

And the last one is the version two tag name.

So as you know that we have already pushed the version one in the last videos.

So with the name change of the index page, we have created the new image.

Now we are pushing this image with the version two before pushing the image.

We are taking the this image with the version two tag name.

So let's hit enter.

So it is take it.

Let me check Docker images.

You can see that our naive image created with this name and with this tag name.

Version two.

Tag name.

Okay, Very good.

So it is time to push this image to the other container registry.

So in order to push this image to the Azure Container Registry, we are going to use Docker, push command

liquor, push and give the full name of the image with the tag name tag number.

Hit enter and see what's happened.

So it is pushing.

Now if you got unauthorized error in this level, when you are pushing this image to the ECR, you should

log in first with the Azure ECR Login Command.

Let me show you this command.

If you have any authorized unauthorized error, you should use this command.

But in my case I have already logged in before, so okay, now it is pushed.

So.

Let me check the image into my Azure container registry.

In order to do that, I'm using Azure ECR Repository list Command.

Hit enter and see that the image already, including with the shopping API and shopping client microservices

Docker image.

So the important thing is listing the tag numbers.

So now with this command, as the ECR repository show text, I'm going to show the shopping client tag

numbers into the Azure Container Registry.

Normally we have only version one, but this time you can see that version two also included in our

Azure Container Registry.

So you can also check these information from the Azure portal for that purpose.

Let's go to the.

My resource group, my ECR, so you can see the repositories in here.

If you check the shopping client.

You can see the version to also edit in this list.

So we have two image, two container image under the shopping client.

Now we can shifting the deployment by using version one and version two.

As you can see that we have two versions of the client application now.

So we can shift it version one to version two into the Azure container.

Azure Kubernetes services cluster in the next video.
## 9-18 Deploy v2 of Shopping.Client Microservices to AKS with zero-downtime rollout k8s
**Duration: 7 minutes**
In this video, we are going to deploy version two of shopping client microservices to Azure Kubernetes

services with zero downtime rollout deployment of Kubernetes.

Let's take an action.

Before we start to zero downtime deployment, I would like to talk about the CPU usage and uptimes.

In order to provide maximum uptime, multiple instances of the application pods must be running.

We should verify the number of running front end shopping client instance with the kubectl get pods

command.

So let's check the cube.

Ctl.

Gitpod.

And see that we have running three ports for the shipping client microservices.

All is running well for now.

And we can also verify from the production.

You will check in here?

Yes.

So now we are going to deploy updated or replication updated version of our application.

For that purpose, let's go to the folder, go to the shopping client and let me close this one for

now.

And the deployment section.

We have a containers and the image in the image specification.

We are only changing the version one to version two.

So this is basic upgrade and deployment for shopping client microservices.

We have shifting to the version one to version two.

This is enough for the changing configuration Yaml file.

We have only one change version one to version two.

After that we are going to run the same command again.

Cube Ctl.

Apply.

On the file case.

So when we are hitting these enter, these will configure our application and shopping client image

should be get from the version two.

So if you watch these spots, let's cube Ctl get spot and watch.

You will see that some of the deployments will be removed and some of the deployments will be created

from scratch.

If we check the hot reload.

It is not deployed yet.

Let me see what's happened.

As you can see that one of the pods status pending status.

If you see some pods stuck in the pending status, we should examine these pods with the troubleshooting

and logging commands of the Kubernetes.

So let's check the details for now.

Cube Ctl get.

Deployment.

Let me check deployment again.

So we have a shopping and shopping client deployment and the image name, as you can see, that version

two.

Okay.

It is correct.

There is no problem with the deployment.

You can also check the replica set with the.

This all and white character replica sets.

Hit enter and check for the shopping client application.

As you can see that we have two replica set and version two is ready.

It is okay.

So now let me describe port.

Let me see this.

Problematic.

But for that purpose we are using the kubectl describe pod and give the port number.

Why it is can't removing or creating.

We will.

Verify with the kubectl.

Describe pod command.

Hit Enter.

Go to events and see that failed for the scheduling and the problem was insufficient CPU.

Okay.

Problem is the same insufficient CPU.

As you can see that when replacing the pods it can't run enough pod as allocated CPU because one Kubernetes

shifting to the deployments.

Think about that.

We have three old version and it is going to create a new three new version.

So when it is shifting to the new versions before closing the one pod, it is trying to create an input,

but it can't be create because there is not enough CPU.

It is getting error insufficient CPU.

That's why the deployment is not performed yet.

This is the core reason we should update the CPU resources.

This error occurs when performing zero downtime deployment Kubernetes Try to create new pods without

zero downtime.

That's why old pods also running, but CPU allocation can't allow that operation.

We should decrease the CPU allocation of our pods.

Okay.

Let me close.

Let me try to deploy from scratch again or apply with the access folder.

Let's see that.

All ports.

Created from scratch.

Kubectl get pod.

Okay.

It is creating from secret.

Container is creating.

Okay.

All containers created.

Well, if we run our deploy operation from the beginning, so if you check the this.

Page.

As you can see, that deployment performed with the update version two version of the deployment.

So basically we got the error one once shifting these containers.

But if you deploy from the scratch again with kubectl apply command, we saw that the new pods creating

with the new version of the image and we performed the deployment successfully.

Now as you can see that we performed the zero downtime deployment.

The user cannot see any waiting page or any other problem problematic in the client side.

But we have seen that the deployments rollout process has problem with the insufficient CPU.

We will see in the next video, but at least we can see that the end user never affect the deployment,

but it is good to optimize our process.

We should fix the problem for the next deployment.
## 9-19 Update CPU Resources for Zero-Downtime Deployments
**Duration: 9 minutes**
In this video, we are going to update CPU resources for zero downtime deployments.

Let's take an action.

In the last video we were talking about.

The problems can happen when deploying new versions of Kubernetes shifting to pods.

But if it is not enough CPU usage, then it could be problems.

So what we can do, we should fix this this problem for the next deployments.

So the real solution should be the decreasing the CPU usage for the allocation of the pod definitions.

So let's decrease existing resource definition of the CPUs for that purpose.

Let me close this window.

Let me close.

All and open the shopping.

I would like to start with the shopping API.

We have resource definitions in the resource definition.

I would like to decrease the CPU usage to the

0.1.

So this is another way to write in CPU.

You can write this way.

Five.

Under it.

But this is this is equal to 0.5.

So that's why I'm changing the CPU usage.

10% 0.1.

Okay.

It's the same way.

I'm going to update the limits with the.

30%.

Okay, this is for the shopping API.

And also I should update for the shopping client.

For the shopping client.

We can say that this time we can say that.

It this way and at the same way.

I'm going to change this CPU usage in the resources section.

Both API and client deployment definition.

I have decreased the CPU usage and the maximum CPU usage and I think this time we can perform the rollout

deployment with no effect and we can watch pod shifting properly.

So now it is time to test.

Let me test deployment again.

As you know that our Azure Container Registry has also version one so we can perform the rollback deployment

now.

So in order to perform the rollback deployment, we should go to the shopping client HTML file.

And under the deployment resources, you can see that we can change the version one to version two.

This time, let's think about that.

We our application is not ready for the production and we are going to rollback our application.

So that's why I'm shifting to version one.

And after that of course we have apply command kubectl, apply command and shifting to version one with

these resource CPU allocations.

Let me open the terminal.

And this time I'm going to.

Apply.

Cube Ctl apply and give the file name X and after that I will watch the pod creation operation.

Okay.

So let me kubectl get pod and watch.

As you can see that one by one it is terminating and creating the new containers.

The operations is performing well enough.

Let's see.

One by one.

Terminating and the new pot is creating.

So if you can check now, let me refresh this page.

Okay.

As you can see that I have no any downtime.

My all whole application is working.

But when I refresh the page, I saw that the version one of the shopping client microservice Docker

image is on production now.

It is very good.

We can see the zero downtime and we can watch the port shifting operation in the Kubernetes.

Kubernetes one by one by one, terminating the old pods with the version two and creating the new pods

with the version one.

By this way, we can rollback our deployments smoothly and no downtime, no effect for the end user.

It is very good feature, very powerful feature when you are working with the Azure Kubernetes services.

So as you can see that we perform the rollback deployment with zero downtime and user never affect this

deployment and still able to use application on live Azure Kubernetes services.

So now we can also check some other details.

Let me exit with the ctrl C, let me open Kubectl get pod.

And see that cube.

Ctl.

Let me examine newly created pot and how to pull version one of our application.

Describe.

Ripe and put give the one of the client put.

It entered.

And see that in here when the pulling this image kubernete is creating the image from the version one

of the shopping client container into the other container registry.

So it is very powerful when you are describing the pots and see in which pots it is retrieving.

Also, we can see this information from the kubectl get deployment.

With dish or.

And white.

You can also check that cube deployment getting pod from the image name version one of the shopping

client Docker image of the other from Container Registry.

Azure Container Registry.

Let me see.

You can see the all resources kubectl get all and check one by one.

What we have done, we have created our pods with autoscaling and deploying zero downtime, shifting

to new versions with rollout deployment, performing from the Kubernetes.

And we have a service definition.

One service type is load balancer, which is the shopping client.

And we have that's why Azure Kubernetes Service create a external API with the ingress definition.

Once we hit this IP, we can see that our deployments on Live Azure, Kubernetes services, we have

deployment definition and we have also replica set definitions.

And of course we have auto scaling definitions.

So as you can see that we have finished the zero downtime deployment on Live Azure Kubernetes services

with updating shopping client microservices index page and shifting version one to version two was very

similarly, if you would like to change again, let me show you in here, open the shopping client and

let's change the version two.

And after that if you write kubectl apply.

File a case.

Let's deploy again.

Okay if kubectl get pod.

Watch.

Now you can see that the version one pods will be terminating and version two pods will be creating.

So let's refresh the page and see what's happened.

So as you can see that we have also performed version one to version two very fast and very similarly.

Okay.

In the next video, we will optimize all these operations with the Azure DevOps and Azure Pipelines.


# 第10節：Automate Deployments with CI/CD pipelines on Azure DevOps
**Total: 20 lectures | 2 hours 26 minutes**

## 10-1 Introduction
**Duration: 1 minute**
In this section we are going to Automate our Microservices Deployments with CI/CD pipelines

on Azure Devops.

Let's check out our big picture and see what we are going to build one by one.

As you can see, that we have created created docker images, compose docker containers and tested them,

deploy these docker container images on local kubernetes clusters,

push our image to ACR, shifting deployment to the cloud Azure kubernetes services (AKS),

and finally update microservices with zero-downtime deployments.

That means we have finished to the box.

So in this section, we are focusing on automation deployments with creating ci/cd pipelines

on azure devops tool.

Before we start this actions, we should understand what is Azure Devops and how to use pipelines on Azure Devops.

on Azure Devops.
## 10-2 Introduction to Azure DevOps
**Duration: 2 minutes**
Azure Devops

Azure DevOps provides developer services for support teams to plan work, collaborate on code development,

and build and deploy applications.

Developers can work in the cloud using Azure DevOps Services.

Azure DevOps provides integrated features that you can access through your web browser or IDE client.

IDE client.

We will follow with using web browser.

You can use one or more of the following services based on your business requirements.

We are going to use Azure pipelines when automate deployment of microservices,

But let me explain all services.

Azure Boards delivers a suite of Agile tools to support planning and tracking work, code defects,

and issues using Kanban and Scrum methods

Azure Repos provides Git repositories for source control of your code

Azure Pipelines provides build and release services to support continuous integration and continuous delivery

delivery of your application.

Azure Test Plans provides several tools to test your apps, including manual/exploratory testing and continuous testing

and continuous testing, Azure Artifacts allows teams to share packages such as Maven, npm, NuGet

and more from public and private sources and integrate package sharing into your CI/CD pipelines

Also Azure DevOps supports adding extensions and integrating with other popular services,

like Campfire, Slack, Trello, UserVoice, and more, and developing your own custom extensions.
## 10-3 Introduction to Azure Pipelines
**Duration: 2 minutes**

## 10-4 Deploy to AKS through Azure CI/CD Pipelines
**Duration: 3 minutes**

## 10-5 Sign up for Azure Pipelines
**Duration: 5 minutes**

## 10-6 Create Our first pipeline with Azure Pipelines
**Duration: 7 minutes**

## 10-7 Pipeline Docker Task Yaml File Explained in Azure Pipelines
**Duration: 11 minutes**

## 10-8 Pipeline Docker Task Error Fixed in Azure Pipelines
**Duration: 17 minutes**

## 10-9 Pipeline Tasks and How to Decide to Write Correct Task in Azure Pipelines
**Duration: 4 minutes**

## 10-10 Create Pipeline for Continues Delivery with Deploy to AKS Task
**Duration: 6 minutes**

## 10-11 Create Pipeline for Continues Delivery with Deploy to AKS Task Part 2
**Duration: 9 minutes**

## 10-12 Refactoring Our Pipeline for Continues Delivery with Deploy to AKS
**Duration: 8 minutes**

## 10-13 Running Pipeline for Continues Delivery with Deploy to AKS
**Duration: 6 minutes**

## 10-14 Manage Pipelines for Multi-Container Microservices with CI/CD flows in Azure Pipelines
**Duration: 11 minutes**

## 10-15 Create New Pipeline For ShoppingAPI Microservices with existing pipeline yaml file
**Duration: 11 minutes**

## 10-16 Create New Pipeline For Shopping.Client Microservices
**Duration: 8 minutes**

## 10-17 Create New Pipeline For Shopping.Client Microservices Part 2
**Duration: 19 minutes**

## 10-18 Rename Pipeline and Test Trigger Shopping Pipelines
**Duration: 8 minutes**

## 10-19 Put Azure Pipeline Status Badge into Your Github Project
**Duration: 4 minutes**

## 10-20 Clean All AKS and Azure Resources
**Duration: 4 minutes**

# 第11節：Automate Existing Microservices Reference Application with AKS and Azure DevOps
**Total: 1 lecture | 2 minutes**

## 11-1 Introduction
**Duration: 2 minutes**

# 第12節：Thanks
**Total: 1 lecture | 1 minute**

## 12-1 Bonus Lecture
**Duration: 1 minute**

